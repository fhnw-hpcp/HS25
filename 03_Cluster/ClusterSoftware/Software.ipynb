{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3d4ef75",
   "metadata": {},
   "source": [
    "# Cluster (Merlin6) Software\n",
    "\n",
    "All clusters need some way to be accessed, provide common software, and launching compute jobs in a coordinated fashion. We'll look at what [Merlin6](https://hpce.pages.psi.ch/merlin6/introduction.html) provides.\n",
    "\n",
    "## Cluster Access\n",
    "\n",
    "For cluster access, an account with proper credentials and authorization is needed.\n",
    "\n",
    "* **ssh** access to the login nodes. ssh is also possible to allocated nodes (see below - batch system)\n",
    "* **Remote desktop**, in this case NoMachine nxplayer, for graphical access.\n",
    "* **jupyterhub** for [interactive Python sessions](https://merlin-jupyter.psi.ch:8000).\n",
    "\n",
    "## Envrironment Modules\n",
    "\n",
    "We've seen, that especially lower level software needs to be compiled with support for system performance features. System administrators know the hardware and provide kernel drivers and specialized libraries either directly as system packages (if there is only one choice), or as environment **pmodules** (if there's a choice). Environment modules do little more than setting appropriate Linux environment variables for using installed software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aad96d",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Documentation\n",
    "module --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212b257a",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# List software\n",
    "# Only software above the current hierarchy is shown\n",
    "# Hierarchy: Compiler -> MPI -> MPI Specific\n",
    "module avail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d148c97",
   "metadata": {},
   "source": [
    "The general idea behind *pmodule* is to limit output to software that is compiled with a certain compiler and MPI version.\n",
    "\n",
    "* First compilers are shown. Selecting one.\n",
    "* Then MPI versions are shown as well. Select one.\n",
    "* MPI specific software is shown as well.\n",
    "\n",
    "Dependencies are a bit too complex to always fit into this scheme, but as a guideline this should help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52fe825",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Search for software\n",
    "module search openmpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726e8e66",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Include only lightly tested newer software\n",
    "module use unstable\n",
    "module search openmpi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c13c947",
   "metadata": {},
   "source": [
    "Sysadmins install software into a certain location on disk. To use it, environment variables like *PATH* have to be set. You could do it by hand, *pmodule* just simplifies the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21790b51",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Show environment changes\n",
    "module show gcc/14.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf62068",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Execute environment changes\n",
    "module add gcc/14.2.0\n",
    "which gcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2068ddd7",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# List added modules\n",
    "module list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7579413a",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Reset pmodule managed environment\n",
    "module purge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abcc9fb",
   "metadata": {},
   "source": [
    "For Python, the *anaconda* module is provided, together with some preinstalled environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e03769",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "module add anaconda\n",
    "\n",
    "# Show existing conda envs\n",
    "conda env list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0135e54f",
   "metadata": {},
   "source": [
    "## Batch System\n",
    "\n",
    "People already struggle to cooperate in ordinary kitchens (idiom: too many cooks spoil the broth), so cooperating remotely for orderly access to compute nodes may sound like an impossibility. To end the endless quarrels and frustration amongst overeager scientists, some smart people came up with the idea of access control software in the form of a batch system.\n",
    "\n",
    "Batch systems like **slurm** ([website](https://slurm.schedmd.com)) schedule compute jobs from a queue. Scientist add jobs to such a queue by adding *batch scripts* to it. These batch scripts define compute jobs and metadata, such as the maximum expected runtime, number of nodes and cores, for the job. The batch system assigns a rule based priority to each job, and schedules compute jobs according to the assigned priority. The rules for priority assignment are designed by system administrator to ascertain fair and efficient cluster usage.\n",
    "\n",
    "For special tasks, that need predefined resources, reservations can be created.\n",
    "\n",
    "Most MPI (used for distributed and parallel programming) implementations support slurm as a batch system. Thus, if slurm reserves resources for an MPI job, these resources are automatically used by MPI. \n",
    "\n",
    "Selection of slurm terms:\n",
    "* *cpu* - a CPU hardware thread. If hyperthreading is not used, this corresponds to a CPU core.\n",
    "* *gpu*\n",
    "* *task* - OS process, normally an MPI rank. Every task runs on a *cpu*.\n",
    "* *socket* - a CPU\n",
    "* *node* - compute node\n",
    "* *partition* - basically a queue for compute jobs with access to certain resources\n",
    "* *cluster* - collection of compute nodes\n",
    "\n",
    "Selection of slurm commands (see man pages for details):\n",
    "* *sinfo* - show slurm resources\n",
    "* *squeue* - shows job information\n",
    "* *srun* - run app on existing allocation or run app interactively\n",
    "* *salloc* - get resource allocation and run a command\n",
    "* *scancel -b job_id* - send (cancel) signal to a job\n",
    "* *sbatch* - enqueue batch script, run noninteractively\n",
    "* *sacct* - show accounting info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8cab11",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Show defined clusters and partitions\n",
    "sinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e372a46d",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Show job info on gmerlin6 cluster\n",
    "squeue --clusters=gmerlin6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e54bb2",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Show job info for a specific user\n",
    "squeue -u stadler_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d3b48b",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Run one task executing the command hostname on cluster 'merlin6' and partition 'hourly' with a timelimit of 30 seconds\n",
    "srun --cluster=merlin6 --partition=hourly --time=00:00:30 --ntasks=1 hostname\n",
    "salloc --cluster=merlin6 --partition=hourly --time=00:00:30 --ntasks=1 hostname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f80c73",
   "metadata": {},
   "source": [
    "*srun* is like a shortcut for *salloc .. srun*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fede5b3",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Run two tasks executing the command hostname on cluster 'merlin6' and partition 'hourly', label the output with the task number\n",
    "srun --cluster=merlin6 --partition=hourly --time=00:00:30 --ntasks=2 --label hostname\n",
    "salloc --cluster=merlin6 --partition=hourly --time=00:00:30 --ntasks=2 srun --label hostname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0dc921",
   "metadata": {},
   "source": [
    "**IO redirection**\n",
    "\n",
    "By default *srun* sends stdin to all tasks, and redirects stdout/stderr to itself. By specifying *--input=1*, stdin can be sent to task 1 only. There are many more possibilities like redirecting output into one or several (one per task) files, see man page. The option *--output* redirects both stdout and stderr output, unless *--error* is specified explicitly for stderr output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d43324",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "echo \"hello\" | srun --cluster=merlin6 --partition=hourly --time=00:00:30 --ntasks=2 --label cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe23b52a",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "echo \"hello\" | srun --cluster=merlin6 --partition=hourly --time=00:00:30 --ntasks=2 --label --input=1 cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c67089f",
   "metadata": {},
   "source": [
    "**Resource selection**\n",
    "\n",
    "Slurm allows a wide range of commandline arguments to select resources, and places the processes in a cgroup that restricts resources to the selected ones. We'll use the **taskset** command to show cpu masks, and **nvidia-smi** to show available gpus.\n",
    "\n",
    "* *--ntasks* specifies the number of tasks, potentially running anywhere.\n",
    "* A combination of *--nodes* and *--ntasks-per-node* selects compute nodes and tasks per selected node.\n",
    "* A combination of *--ntasks* and *--gpus-per-task* selects a number of gpus per task.\n",
    "* *--hint=nomultithread* places only one task per core\n",
    "* *--mem-per-cpu* allocates a minimum amount of memory per task\n",
    "* *--exclusive* grabs entire compute nodes only.\n",
    "* *--mem=0* grabs all compute node memory\n",
    "\n",
    "There are many more, see the man page for your slurm command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87b727d",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "srun --cluster=merlin6 --partition=hourly --ntasks=2 --time=00:00:30 -label bash -c 'echo \"$(hostname; taskset -c -p $$)\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11946f7d",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "srun --cluster=merlin6 --partition=hourly --nodes=2 --ntasks-per-node=2 --time=00:00:30 --label bash -c 'echo \"$(hostname; taskset -c -p $$)\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12021c5",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "srun --cluster=gmerlin6 --partition=gpu-short --ntasks=2 --gpus-per-task=2 --label bash -c 'echo \"$(hostname; nvidia-smi)\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5624be",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27ac67b5",
   "metadata": {},
   "source": [
    "## MPI\n",
    "\n",
    "lll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff529b4",
   "metadata": {},
   "source": [
    "## Data Catalog and Backup\n",
    "\n",
    "Scientists often produce large datasets. Sometimes these need to be transferred to or away from the cluster. Archiving for later retrieval and reexamination or verification of scientific results is also important. To facilitate retrieval, archived data is enriched with meta data thaht describes how data was produced and what it is about.\n",
    "\n",
    "* Data transfer from and to the cluster is supported at PSI via [Globus](https://www.globus.org/data-transfer).\n",
    "* Meta data enriched data archiving with [SciCat](https://github.com/SciCatProject)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78bfa9b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63b1e5eb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
