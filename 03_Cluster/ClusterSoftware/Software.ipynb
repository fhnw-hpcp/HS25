{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3d4ef75",
   "metadata": {},
   "source": [
    "# Cluster (Merlin6) Software\n",
    "\n",
    "All clusters need some way to be accessed, provide common software, and launching compute jobs in a coordinated fashion. We'll look at what [Merlin6](https://hpce.pages.psi.ch/merlin6/introduction.html) provides.\n",
    "\n",
    "## Cluster Access\n",
    "\n",
    "For cluster access, an account with proper credentials and authorization is needed.\n",
    "\n",
    "* **ssh** access to the login nodes. ssh is also possible to allocated nodes (see below - batch system)\n",
    "* **Remote desktop**, in this case NoMachine nxplayer, for graphical access.\n",
    "* **jupyterhub** for [interactive Python sessions](https://merlin-jupyter.psi.ch:8000).\n",
    "\n",
    "## Envrironment Modules\n",
    "\n",
    "We've seen, that especially lower level software needs to be compiled with support for system performance features. System administrators know the hardware and provide kernel drivers and specialized libraries either directly as system packages (if there is only one choice), or as environment **pmodules** (if there's a choice). Environment modules do little more than setting appropriate Linux environment variables for using installed software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aad96d",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Documentation\n",
    "module --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212b257a",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# List software\n",
    "# Only software above the current hierarchy is shown\n",
    "# Hierarchy: Compiler -> MPI -> MPI Specific\n",
    "module avail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d148c97",
   "metadata": {},
   "source": [
    "The general idea behind *pmodule* is to limit output to software that is compiled with a certain compiler and MPI version.\n",
    "\n",
    "* First compilers are shown. Selecting one.\n",
    "* Then MPI versions are shown as well. Select one.\n",
    "* MPI specific software is shown as well.\n",
    "\n",
    "Dependencies are a bit too complex to always fit into this scheme, but as a guideline this should help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52fe825",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Search for software\n",
    "module search openmpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726e8e66",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Include only lightly tested newer software\n",
    "module use unstable\n",
    "module search openmpi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c13c947",
   "metadata": {},
   "source": [
    "Sysadmins install software into a certain location on disk. To use it, environment variables like *PATH* have to be set. You could do it by hand, *pmodule* just simplifies the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21790b51",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Show environment changes\n",
    "module show gcc/14.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf62068",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Execute environment changes\n",
    "module add gcc/14.2.0\n",
    "which gcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2068ddd7",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# List added modules\n",
    "module list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7579413a",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Reset pmodule managed environment\n",
    "module purge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abcc9fb",
   "metadata": {},
   "source": [
    "For Python, the *anaconda* module is provided, together with some preinstalled environments. (**ATTENTION**: by itself, the PSI anaconda modue only provides the *python3* executable, not *python*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e03769",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "module add anaconda\n",
    "\n",
    "# Show existing conda envs\n",
    "conda env list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0135e54f",
   "metadata": {},
   "source": [
    "## Batch System\n",
    "\n",
    "People already struggle to cooperate in ordinary kitchens (idiom: too many cooks spoil the broth), so cooperating remotely for orderly access to compute nodes may sound like an impossibility. To end the endless quarrels and frustration amongst overeager scientists, some smart people came up with the idea of access control software in the form of a batch system.\n",
    "\n",
    "Batch systems like **slurm** ([website](https://slurm.schedmd.com)) schedule compute jobs from a queue. Scientist add jobs to such a queue by adding *batch scripts* to it. These batch scripts define compute jobs and metadata, such as the maximum expected runtime, number of nodes and cores, for the job. The batch system assigns a rule based priority to each job, and schedules compute jobs according to the assigned priority. The rules for priority assignment are designed by system administrator to ascertain fair and efficient cluster usage.\n",
    "\n",
    "For special tasks, that need predefined resources, reservations can be created.\n",
    "\n",
    "Most MPI (used for distributed and parallel programming) implementations support slurm as a batch system. Thus, if slurm reserves resources for an MPI job, these resources are automatically used by MPI. \n",
    "\n",
    "Selection of slurm terms:\n",
    "* *cpu* - a CPU hardware thread. If hyperthreading is not used, this corresponds to a CPU core.\n",
    "* *gpu*\n",
    "* *task* - OS process, normally an MPI rank. Every task runs on a *cpu*.\n",
    "* *socket* - a CPU\n",
    "* *node* - compute node\n",
    "* *partition* - basically a queue for compute jobs with access to certain resources\n",
    "* *cluster* - collection of compute nodes\n",
    "\n",
    "Selection of slurm commands (see man pages for details):\n",
    "* *sinfo* - show slurm resources\n",
    "* *squeue* - shows job information\n",
    "* *srun* - run app on existing allocation or run app interactively\n",
    "* *salloc* - get resource allocation and run a command\n",
    "* *scancel -b job_id* - send (cancel) signal to a job\n",
    "* *sbatch* - enqueue batch script, run noninteractively\n",
    "* *sacct* - show accounting info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8cab11",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Show defined clusters and partitions\n",
    "sinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e372a46d",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Show job info on gmerlin6 cluster\n",
    "squeue --clusters=gmerlin6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e54bb2",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Show job info for a specific user\n",
    "squeue -u stadler_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d3b48b",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Run one task executing the command hostname on cluster 'merlin6' and partition 'hourly' with a timelimit of 30 seconds\n",
    "srun --cluster=merlin6 --partition=hourly --time=00:00:30 --ntasks=1 hostname\n",
    "salloc --cluster=merlin6 --partition=hourly --time=00:00:30 --ntasks=1 hostname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f80c73",
   "metadata": {},
   "source": [
    "*srun* is like a shortcut for *salloc .. srun*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fede5b3",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Run two tasks executing the command hostname on cluster 'merlin6' and partition 'hourly', label the output with the task number\n",
    "srun --cluster=merlin6 --partition=hourly --time=00:00:30 --ntasks=2 --label hostname\n",
    "salloc --cluster=merlin6 --partition=hourly --time=00:00:30 --ntasks=2 srun --label hostname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0dc921",
   "metadata": {},
   "source": [
    "**IO redirection**\n",
    "\n",
    "By default *srun* sends stdin to all tasks, and redirects stdout/stderr to itself. By specifying *--input=1*, stdin can be sent to task 1 only. There are many more possibilities like redirecting output into one or several (one per task) files, see man page. The option *--output* redirects both stdout and stderr output, unless *--error* is specified explicitly for stderr output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d43324",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "echo \"hello\" | srun --cluster=merlin6 --partition=hourly --time=00:00:30 --ntasks=2 --label cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe23b52a",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "echo \"hello\" | srun --cluster=merlin6 --partition=hourly --time=00:00:30 --ntasks=2 --label --input=1 cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c67089f",
   "metadata": {},
   "source": [
    "**Resource selection**\n",
    "\n",
    "Slurm allows a wide range of commandline arguments to select resources, and places the processes in a cgroup that restricts resources to the selected ones. We'll use the **taskset** command to show cpu masks, and **nvidia-smi** to show available gpus.\n",
    "\n",
    "* *--ntasks* specifies the number of tasks, potentially running anywhere.\n",
    "* A combination of *--nodes* and *--ntasks-per-node* selects compute nodes and tasks per selected node.\n",
    "* A combination of *--ntasks* and *--gpus-per-task* selects a number of gpus per task.\n",
    "* *--hint=nomultithread* places only one task per core\n",
    "* *--mem-per-cpu* allocates a minimum amount of memory per task\n",
    "* *--exclusive* grabs entire compute nodes only.\n",
    "* *--mem=0* grabs all compute node memory\n",
    "\n",
    "There are many more, see the man page for your slurm command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87b727d",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "srun --cluster=merlin6 --partition=hourly --ntasks=2 --time=00:00:30 --label bash -c 'echo \"$(hostname; taskset -c -p $$)\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11946f7d",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "srun --cluster=merlin6 --partition=hourly --nodes=2 --ntasks-per-node=2 --time=00:00:30 --label bash -c 'echo \"$(hostname; taskset -c -p $$)\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12021c5",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "srun --cluster=gmerlin6 --partition=gpu-short --ntasks=2 --gpus-per-task=2 --time=00:00:30 --label bash -c 'echo \"$(hostname; nvidia-smi)\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a29275",
   "metadata": {},
   "source": [
    "**Batch scripts**\n",
    "\n",
    "The *salloc* command (and *srun* as used so far) actually waits for a resource allocation. If the job queues are fulll with higher priority jobs, the waiting time might seriously test your patience. The real deal with batch systems becomes apparent with the *sbatch* command, that allows to submit non-interactive batch scripts to job queues. In other words: submit a batch script, enjoy your week-end, and when your back, the results of the computation are ready.\n",
    "\n",
    "The *sbatch* command allocates resources, and runs the batch script like *salloc* would do it. By default, output is redirected into files \"slurm-%j.out\", where %j is the job id. Commandline arguments can be given directly to *sbatch*, or much better for documentation, within the batch script itself in the form\n",
    "\n",
    "> #SBATCH commandline argument\n",
    "\n",
    "*sbatch* sets a pletora of environment variables the script can react on, like *SLURM_SUBMIT_DIR*, see the manpage for more. In order to use allocated resourcces with MPI jobs, use something like\n",
    "\n",
    "> srun python my-mpi-code.py\n",
    "\n",
    "in the batch script.\n",
    "\n",
    "**Exercise**: Look at the use-srun.sbatch file and submit it with `sbatch use-srun.sbatch`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f2b25e",
   "metadata": {},
   "source": [
    "**Array jobs**\n",
    "\n",
    "The most primitive form of parallelism is just running the same program on distinct inputs in parallel. For this slurm features *array jobs*. Array jobs demand the array commandline argument\n",
    "\n",
    "> #SBATCH --array=1-10\n",
    "\n",
    "This will launch 10 jobs with the environment variable *SLURM_ARRAY_TASK_ID* set to distinct integer values from 1 to 10. Your code should map this integer value to distinct program inputs. In the *--output* argument, *%a* can be used to fill in the array index.\n",
    "\n",
    "**Excercise**: Look at arra-job.py and array-job.sbatch and submit with `sbatch arra-job.sbatch`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ece40a9",
   "metadata": {},
   "source": [
    "**Running multiple programs**\n",
    "\n",
    "Slurm allows to start several programs as a unit by separating the parts with `:`. Some options, like *--ntasks* can be given to parts separately. Task numbering continues from one part to the next. This could be used to for parallel MPI jobs to run a single rank with the debugger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524e4914",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "srun --cluster=merlin6 --partition=hourly --time=00:00:30 --label --ntasks=1 echo hello one : --ntasks=1 echo hello two : --ntasks=2 echo hello three"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c2640b",
   "metadata": {},
   "source": [
    "**X forwarding**\n",
    "\n",
    "X forwarding is done with the *--x11* commandline option. By default, it does forwarding from all tasks. The environment variable *SLURM_PROCID* will help to order the windows by task number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7936213",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "srun --cluster=merlin6 --partition=hourly --time=00:05:00 --x11 --ntasks=2 xterm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50f0fe8",
   "metadata": {},
   "source": [
    "**Terminal**\n",
    "\n",
    "The *--pty* option creates a pseudo terminal for task 0. It can be used to get a terminal on a compute node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf0d716",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "srun --cluster=merlin6 --partition=hourly --time=00:05:00 --ntasks=1 --pty bash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ac67b5",
   "metadata": {},
   "source": [
    "## MPI\n",
    "\n",
    "lll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff529b4",
   "metadata": {},
   "source": [
    "## Data Catalog and Backup\n",
    "\n",
    "Scientists often produce large datasets. Sometimes these need to be transferred to or away from the cluster. Archiving for later retrieval and reexamination or verification of scientific results is also important. To facilitate retrieval, archived data is enriched with meta data thaht describes how data was produced and what it is about.\n",
    "\n",
    "* Data transfer from and to the cluster is supported at PSI via [Globus](https://www.globus.org/data-transfer).\n",
    "* Meta data enriched data archiving with [SciCat](https://github.com/SciCatProject)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
