{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49363c59",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "# Merlin6 access\n",
    "\n",
    "All clusters require a method for access, common software availability, and a way to launch compute jobs in a coordinated manner.  \n",
    "The [Merlin6 documentation](https://hpce.pages.psi.ch/merlin6/introduction.html) provides details on the different ways to access Merlin6.\n",
    "\n",
    "> To access the cluster, you need a valid account with proper credentials and authorization.\n",
    "\n",
    "## Access to login nodes\n",
    "\n",
    "> ðŸ‘‰ Try accessing the login nodes using your preferred method.\n",
    "\n",
    "### Options\n",
    "- **SSH**  \n",
    "  - Direct SSH access to the login nodes.  \n",
    "  - SSH access is also possible to allocated nodes (see section *Batch System*).  \n",
    "  - ðŸ“– [Instructions](https://hpce.pages.psi.ch/merlin6/interactive.html)\n",
    "\n",
    "- **Remote Desktop (optional)**  \n",
    "  - Use *NoMachine NX Player* for graphical access.  \n",
    "  - [Download NoMachine](https://www.nomachine.com/)  \n",
    "  - ðŸ“– [Access Merlin6 via NoMachine](https://hpce.pages.psi.ch/merlin6/nomachine.html)\n",
    "\n",
    "## Access to JupyterHub\n",
    "\n",
    "Many users prefer working on the cluster through **JupyterHub**, as it allows running interactive Python sessions directly in a Notebook.  \n",
    "From JupyterHub, you can also open a terminal console to work on the cluster.\n",
    "\n",
    "> ðŸ‘‰ Try creating a new session on the Merlin6 JupyterHub.\n",
    "\n",
    "- **JupyterHub**: [Merlin6 JupyterHub](https://merlin-jupyter.psi.ch:8000)\n",
    "\n",
    "## Storage\n",
    "\n",
    "Merlin6 provides multiple filesystems, each designed for a specific purpose:\n",
    "* User home directories (NFS)\n",
    "* User data directories (GPFS)\n",
    "* Project directories (GPFS)\n",
    "* Other high-performance directories (GPFS)\n",
    "\n",
    "> ðŸ‘‰ Try listing the different partitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab069b0",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "df -hT -t gpfs -t nfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c8ef27",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "All storage areas are subject to **quotas**, which can be applied at both the **user** and **project** level. Users are responsible for monitoring their quota usage. Checking quota status:\n",
    "- NFS quota status: `quota -s -w -p`\n",
    "- GPFS and NFS quota status: `merlin_quota`\n",
    "\n",
    "> ðŸ‘‰ Check your current quota. You can add some files and observe how this is updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d505bb",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "quota -s -w -p\n",
    "merlin_quotas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f0781b",
   "metadata": {},
   "source": [
    "## Merlin6 Slurm clusters\n",
    "\n",
    "Merlin6 consists in a **multi-cluster** setup (multiple `slurmctld` servers): `merlin6` (CPU cluster) and `gmerlin6`. In the past, it also contained other clusters (`merlin5`).\n",
    "\n",
    "They share the same accounting server (`slurmdbd`), which contains job accounting, logging and user information.\n",
    "\n",
    "> ðŸ‘‰ List the different clusters using `sacctmgr` (contacts `slurmdbd`)\n",
    "\n",
    "> ðŸ‘‰ List user settings on each different cluster using `sacctmgr`\n",
    "\n",
    "> ðŸ‘‰ List cluster information with `sinfo` (contacts `slurmctld`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6056ecc",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    " sacctmgr show cluster format=Cluster,NodeCount,ControlHost,ControlPort,RPC\n",
    " sacctmgr show assoc tree User=$USER format=Cluster,Account%7,User%18,MaxTRES%20,MaxSubmit,QoS%12\n",
    " sinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f99e0d",
   "metadata": {},
   "source": [
    "# Merlin6 Partitions\n",
    "\n",
    "Some commands are available for listing partition information. The commonest one is `sinfo`.\n",
    "\n",
    "> ðŸ‘‰ List Merlin6 partitions with advanced `sinfo` commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe291ff",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "sinfo --clusters=all -o \"%.16P %.14F %.14C %.16L %.14l %.40G %.5D %N\"\n",
    "sinfo --clusters=merlin6 -o \"%.16P %.14F %.14C %.16L %.14l %.5D %N\"\n",
    "SINFO_FORMAT=\"%.16P %.14F %.14C %.16L %.14l %.5D %N\" sinfo --clusters=merlin6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6785c53",
   "metadata": {},
   "source": [
    "A more advanced command, `scontrol`, can be also used to list detailed information for the different partitions.\n",
    "\n",
    "> ðŸ‘‰ Use `scontrol show partition` to see the details for the different partitions. Show QoS associated to each partition with `sacctmgr`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58d0766",
   "metadata": {},
   "source": [
    "scontrol show partition\n",
    "sacctmgr show qos format=Name,MaxTRES%30,MaxTRESPU%30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4e7d66",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3d4ef75",
   "metadata": {},
   "source": [
    "# Cluster (Merlin6) Software\n",
    "\n",
    "## Envrironment Modules\n",
    "\n",
    "We've seen, that especially lower level software needs to be compiled with support for system performance features. System administrators know the hardware and provide kernel drivers and specialized libraries either directly as system packages (if there is only one choice), or as environment **pmodules** (if there's a choice). Environment modules do little more than setting appropriate Linux environment variables for using installed software."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fd8243",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "* **NOTE I**: The following should work, when logging in via `ssh -Y <username>@merlin-l-002.psi.ch`\n",
    "* **NOTE II**: On the JupyterHub console, X11 is not working, and `srun --options..` should be left away or, if possible, replaced by `salloc --options..`\n",
    "* **NOTE III**: Inside a notebook the `! command` will run on a virgin shell lacking the module system environment and such. If this is a problem, use `! . ~/.bashrc; command`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aad96d",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Documentation\n",
    "# module --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212b257a",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# List software\n",
    "# Only software above the current hierarchy is shown\n",
    "# Hierarchy: Compiler -> MPI -> MPI Specific\n",
    "# module avail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d148c97",
   "metadata": {},
   "source": [
    "The general idea behind *pmodule* is to limit output to software that is compiled with a certain compiler and MPI version.\n",
    "\n",
    "* First compilers are shown. Selecting one.\n",
    "* Then MPI versions are shown as well. Select one.\n",
    "* MPI specific software is shown as well.\n",
    "\n",
    "Dependencies are a bit too complex to always fit into this scheme, but as a guideline this should help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52fe825",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Search for software\n",
    "# module search openmpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726e8e66",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Include only lightly tested newer software\n",
    "# module use unstable\n",
    "# module search openmpi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c13c947",
   "metadata": {},
   "source": [
    "Sysadmins install software into a certain location on disk. To use it, environment variables like *PATH* have to be set. You could do it by hand, *pmodule* just simplifies the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21790b51",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Show environment changes\n",
    "# module show gcc/14.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf62068",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Execute environment changes\n",
    "# module add gcc/14.2.0\n",
    "# which gcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2068ddd7",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# List added modules\n",
    "# module list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7579413a",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Reset pmodule managed environment\n",
    "# module purge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abcc9fb",
   "metadata": {},
   "source": [
    "For Python, the *anaconda* module is provided, together with some preinstalled environments. (**ATTENTION**: by itself, the PSI anaconda modue only provides the *python3* executable, not *python*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e03769",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# module add anaconda\n",
    "\n",
    "# Show existing conda envs\n",
    "# conda env list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0135e54f",
   "metadata": {},
   "source": [
    "# Batch System\n",
    "\n",
    "People already struggle to cooperate in ordinary kitchens (idiom: too many cooks spoil the broth), so cooperating remotely for orderly access to compute nodes may sound like an impossibility. To end the endless quarrels and frustration amongst overeager scientists, some smart people came up with the idea of access control software in the form of a batch system.\n",
    "\n",
    "Batch systems like **slurm** ([website](https://slurm.schedmd.com)) schedule compute jobs from a queue. Scientist add jobs to such a queue by adding *batch scripts* to it. These batch scripts define compute jobs and metadata, such as the maximum expected runtime, number of nodes and cores, for the job. The batch system assigns a rule based priority to each job, and schedules compute jobs according to the assigned priority. The rules for priority assignment are designed by system administrator to ascertain fair and efficient cluster usage.\n",
    "\n",
    "For special tasks, that need predefined resources, reservations can be created.\n",
    "\n",
    "Most MPI (used for distributed and parallel programming) implementations support slurm as a batch system. Thus, if slurm reserves resources for an MPI job, these resources are automatically used by MPI. \n",
    "\n",
    "Selection of slurm terms:\n",
    "* *cpu* - a CPU hardware thread. If hyperthreading is not used, this corresponds to a CPU core.\n",
    "* *gpu*\n",
    "* *task* - OS process, normally an MPI rank. Every task runs on a *cpu*.\n",
    "* *socket* - a CPU\n",
    "* *node* - compute node\n",
    "* *partition* - basically a queue for compute jobs with access to certain resources\n",
    "* *cluster* - collection of compute nodes\n",
    "\n",
    "Selection of slurm commands (see man pages for details):\n",
    "* *sinfo* - show slurm resources\n",
    "* *squeue* - shows job information\n",
    "* *srun* - run app on existing allocation or run app interactively\n",
    "* *salloc* - get resource allocation and run a command\n",
    "* *scancel -b job_id* - send (cancel) signal to a job\n",
    "* *sbatch* - enqueue batch script, run noninteractively\n",
    "* *sacct* - show accounting info\n",
    "\n",
    "**NOTE**: For the course, we'll have a reservation. The name of which, at the time of writing, was unknown. Please add `--reservation=name` to commands leading to resource allocations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8cab11",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Show defined clusters and partitions\n",
    "# sinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e372a46d",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Show job info on gmerlin6 cluster\n",
    "# squeue --clusters=gmerlin6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e54bb2",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Show job info for a specific user\n",
    "# squeue -u stadler_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d3b48b",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Run one task executing the command hostname on cluster 'merlin6' and partition 'hourly' with a timelimit of 30 seconds\n",
    "# srun --cluster=merlin6 --partition=hourly --time=00:00:30 --ntasks=1 hostname\n",
    "# salloc --cluster=merlin6 --partition=hourly --time=00:00:30 --ntasks=1 hostname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f80c73",
   "metadata": {},
   "source": [
    "*srun* is like a shortcut for *salloc .. srun*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fede5b3",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Run two tasks executing the command hostname on cluster 'merlin6' and partition 'hourly', label the output with the task number\n",
    "# srun --cluster=merlin6 --partition=hourly --time=00:00:30 --ntasks=2 --label hostname\n",
    "# salloc --cluster=merlin6 --partition=hourly --time=00:00:30 --ntasks=2 srun --label hostname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0dc921",
   "metadata": {},
   "source": [
    "**IO redirection**\n",
    "\n",
    "By default *srun* sends stdin to all tasks, and redirects stdout/stderr to itself. By specifying *--input=1*, stdin can be sent to task 1 only. There are many more possibilities like redirecting output into one or several (one per task) files, see man page. The option *--output* redirects both stdout and stderr output, unless *--error* is specified explicitly for stderr output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d43324",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# echo \"hello\" | srun --cluster=merlin6 --partition=hourly --time=00:00:30 --ntasks=2 --label cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe23b52a",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# echo \"hello\" | srun --cluster=merlin6 --partition=hourly --time=00:00:30 --ntasks=2 --label --input=1 cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c67089f",
   "metadata": {},
   "source": [
    "**Resource selection**\n",
    "\n",
    "Slurm allows a wide range of commandline arguments to select resources, and places the processes in a cgroup that restricts resources to the selected ones. We'll use the **taskset** command to show cpu masks, and **nvidia-smi** to show available gpus.\n",
    "\n",
    "* *--ntasks* specifies the number of tasks, potentially running anywhere.\n",
    "* A combination of *--nodes* and *--ntasks-per-node* selects compute nodes and tasks per selected node.\n",
    "* A combination of *--ntasks* and *--gpus-per-task* selects a number of gpus per task.\n",
    "* *--hint=nomultithread* places only one task per core\n",
    "* *--mem-per-cpu* allocates a minimum amount of memory per task\n",
    "* *--exclusive* grabs entire compute nodes only.\n",
    "* *--mem=0* grabs all compute node memory\n",
    "\n",
    "There are many more, see the man page for your slurm command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87b727d",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# srun --cluster=merlin6 --partition=hourly --ntasks=2 --time=00:00:30 --label bash -c 'echo \"$(hostname; taskset -c -p $$)\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11946f7d",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# srun --cluster=merlin6 --partition=hourly --nodes=2 --ntasks-per-node=2 --time=00:00:30 --label bash -c 'echo \"$(hostname; taskset -c -p $$)\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12021c5",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# srun --cluster=gmerlin6 --partition=gpu-short --ntasks=2 --gpus-per-task=2 --time=00:00:30 --label bash -c 'echo \"$(hostname; nvidia-smi)\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a29275",
   "metadata": {},
   "source": [
    "**Batch scripts**\n",
    "\n",
    "The *salloc* command (and *srun* as used so far) actually waits for a resource allocation. If the job queues are fulll with higher priority jobs, the waiting time might seriously test your patience. The real deal with batch systems becomes apparent with the *sbatch* command, that allows to submit non-interactive batch scripts to job queues. In other words: submit a batch script, enjoy your week-end, and when your back, the results of the computation are ready.\n",
    "\n",
    "The *sbatch* command allocates resources, and runs the batch script like *salloc* would do it. By default, output is redirected into files \"slurm-%j.out\", where %j is the job id. Commandline arguments can be given directly to *sbatch*, or much better for documentation, within the batch script itself in the form\n",
    "\n",
    "> #SBATCH commandline argument\n",
    "\n",
    "*sbatch* sets a pletora of environment variables the script can react on, like *SLURM_SUBMIT_DIR*, see the manpage for more. In order to use allocated resourcces with MPI jobs, use something like\n",
    "\n",
    "> srun python my-mpi-code.py\n",
    "\n",
    "in the batch script.\n",
    "\n",
    "**Exercise**: Look at the use-srun.sbatch file and submit it with `sbatch use-srun.sbatch`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f2b25e",
   "metadata": {},
   "source": [
    "**Array jobs**\n",
    "\n",
    "The most primitive form of parallelism is just running the same program on distinct inputs in parallel. For this slurm features *array jobs*. Array jobs demand the array commandline argument\n",
    "\n",
    "> #SBATCH --array=1-10\n",
    "\n",
    "This will launch 10 jobs with the environment variable *SLURM_ARRAY_TASK_ID* set to distinct integer values from 1 to 10. Your code should map this integer value to distinct program inputs. In the *--output* argument, *%a* can be used to fill in the array index.\n",
    "\n",
    "**Excercise**: Look at arra-job.py and array-job.sbatch and submit with `sbatch arra-job.sbatch`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ece40a9",
   "metadata": {},
   "source": [
    "**Running multiple programs**\n",
    "\n",
    "Slurm allows to start several programs as a unit by separating the parts with `:`. Some options, like *--ntasks* can be given to parts separately. Task numbering continues from one part to the next. This could be used to for parallel MPI jobs to run a single rank with the debugger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524e4914",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# srun --cluster=merlin6 --partition=hourly --time=00:00:30 --label --ntasks=1 echo hello one : --ntasks=1 echo hello two : --ntasks=2 echo hello three"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c2640b",
   "metadata": {},
   "source": [
    "**X forwarding**\n",
    "\n",
    "X forwarding is done with the *--x11* commandline option. By default, it does forwarding from all tasks. The environment variable *SLURM_PROCID* will help to order the windows by task number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7936213",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# srun --cluster=merlin6 --partition=hourly --time=00:05:00 --x11 --ntasks=2 xterm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50f0fe8",
   "metadata": {},
   "source": [
    "**Terminal**\n",
    "\n",
    "The *--pty* option creates a pseudo terminal for task 0. It can be used to get a terminal on a compute node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf0d716",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# srun --cluster=merlin6 --partition=hourly --time=00:05:00 --ntasks=1 --pty bash"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
