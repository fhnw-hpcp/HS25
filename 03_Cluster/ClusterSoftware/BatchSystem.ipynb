{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49363c59",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "# 1. Merlin6 access and topology\n",
    "\n",
    "All clusters require a method for access, common software availability, and a way to launch compute jobs in a coordinated manner.  \n",
    "The [Merlin6 documentation](https://hpce.pages.psi.ch/merlin6/introduction.html) provides details on the different ways to access Merlin6.\n",
    "\n",
    "> To access the cluster, you need a valid account with proper credentials and authorization.\n",
    "\n",
    "## Access to login nodes\n",
    "\n",
    "> ðŸ‘‰ Try accessing the login nodes using your preferred method.\n",
    "\n",
    "### Options\n",
    "- **SSH**  \n",
    "  - Direct SSH access to the login nodes.  \n",
    "  - SSH access is also possible to allocated nodes (see section *Batch System*).  \n",
    "  - ðŸ“– [Instructions](https://hpce.pages.psi.ch/merlin6/interactive.html)\n",
    "\n",
    "- **Remote Desktop (optional)**  \n",
    "  - Use *NoMachine NX Player* for graphical access.  \n",
    "  - [Download NoMachine](https://www.nomachine.com/)  \n",
    "  - ðŸ“– [Access Merlin6 via NoMachine](https://hpce.pages.psi.ch/merlin6/nomachine.html)\n",
    "\n",
    "## Access to JupyterHub\n",
    "\n",
    "Many users prefer working on the cluster through **JupyterHub**, as it allows running interactive Python sessions directly in a Notebook.  \n",
    "From JupyterHub, you can also open a terminal console to work on the cluster.\n",
    "\n",
    "> ðŸ‘‰ Try creating a new session on the Merlin6 JupyterHub.\n",
    "\n",
    "- **JupyterHub**: [Merlin6 JupyterHub](https://merlin-jupyter.psi.ch:8000)\n",
    "\n",
    "## Storage\n",
    "\n",
    "Merlin6 provides multiple filesystems, each designed for a specific purpose:\n",
    "* User home directories (NFS)\n",
    "* User data directories (GPFS)\n",
    "* Project directories (GPFS)\n",
    "* Other high-performance directories (GPFS)\n",
    "\n",
    "> ðŸ‘‰ Try listing the different partitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab069b0",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "df -hT -t gpfs -t nfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c8ef27",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "All storage areas are subject to **quotas**, which can be applied at both the **user** and **project** level. Users are responsible for monitoring their quota usage. Checking quota status:\n",
    "- NFS quota status: `quota -s -w -p`\n",
    "- GPFS and NFS quota status: `merlin_quota`\n",
    "\n",
    "> ðŸ‘‰ Check your current quota. You can add some files and observe how this is updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d505bb",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "quota -s -w -p\n",
    "merlin_quotas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4f2e28",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "## Hardware topology\n",
    "\n",
    "One can see the hardware topology in multiple ways. However `lstopo`is a very useful application supporing a graphical interface which can be used for checking the topology of the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae67d1b",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "## Hardware topology\n",
    "lstopo --no-factorize\n",
    "lstopo -v              # text based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f0781b",
   "metadata": {},
   "source": [
    "## Merlin6 Slurm clusters\n",
    "\n",
    "Merlin6 consists in a **multi-cluster** setup (multiple `slurmctld` servers): `merlin6` (CPU cluster) and `gmerlin6`. In the past, it also contained other clusters (`merlin5`).\n",
    "\n",
    "They share the same accounting server (`slurmdbd`), which contains job accounting, logging and user information.\n",
    "\n",
    "> ðŸ‘‰ List the different clusters using `sacctmgr` (contacts `slurmdbd`)\n",
    "\n",
    "> ðŸ‘‰ List user settings on each different cluster using `sacctmgr`\n",
    "\n",
    "> ðŸ‘‰ List cluster information with `sinfo` (contacts `slurmctld`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6056ecc",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    " sacctmgr show cluster format=Cluster,NodeCount,ControlHost,ControlPort,RPC\n",
    " sacctmgr show assoc tree User=$USER format=Cluster,Account%7,User%18,MaxTRES%20,MaxSubmit,QoS%12\n",
    " sinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f99e0d",
   "metadata": {},
   "source": [
    "# 2. Merlin6 Partitions\n",
    "\n",
    "Some commands are available for listing partition information. The commonest one is `sinfo`.\n",
    "\n",
    "> ðŸ‘‰ List Merlin6 partitions with advanced `sinfo` commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe291ff",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "sinfo --clusters=all -o \"%.16P %.14F %.14C %.16L %.14l %.40G %.5D %N\"\n",
    "sinfo --clusters=merlin6 -o \"%.16P %.14F %.14C %.16L %.14l %.5D %N\"\n",
    "SINFO_FORMAT=\"%.16P %.14F %.14C %.16L %.14l %.5D %N\" sinfo --clusters=merlin6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6785c53",
   "metadata": {},
   "source": [
    "A more advanced command, `scontrol`, can be also used to list detailed information for the different partitions.\n",
    "\n",
    "> ðŸ‘‰ Use `scontrol show partition` to see the details for the different partitions. Show QoS associated to each partition with `sacctmgr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58d0766",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "scontrol show partition\n",
    "sacctmgr show qos format=Name,MaxTRES%30,MaxTRESPU%30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4e7d66",
   "metadata": {},
   "source": [
    "# 3. Job Submission\n",
    "\n",
    "## salloc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e42872",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d237cd5",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "cat /sys/devices/system/cpu/cpu*/topology/thread_siblings_list | sort -n | uniq -c \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7fe26a",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "## sbatch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ef802d",
   "metadata": {},
   "source": [
    "## srun\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d4ef75",
   "metadata": {},
   "source": [
    "# Cluster (Merlin6) Software\n",
    "\n",
    "## Envrironment Modules\n",
    "\n",
    "We've seen, that especially lower level software needs to be compiled with support for system performance features. System administrators know the hardware and provide kernel drivers and specialized libraries either directly as system packages (if there is only one choice), or as environment **pmodules** (if there's a choice). Environment modules do little more than setting appropriate Linux environment variables for using installed software."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fd8243",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "* **NOTE I**: The following should work, when logging in via `ssh -Y <username>@merlin-l-002.psi.ch`\n",
    "* **NOTE II**: On the JupyterHub console, X11 is not working, and `srun --options..` should be left away or, if possible, replaced by `salloc --options..`\n",
    "* **NOTE III**: Inside a notebook the `! command` will run on a virgin shell lacking the module system environment and such. If this is a problem, use `! . ~/.bashrc; command`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aad96d",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Documentation\n",
    "# module --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212b257a",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# List software\n",
    "# Only software above the current hierarchy is shown\n",
    "# Hierarchy: Compiler -> MPI -> MPI Specific\n",
    "# module avail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d148c97",
   "metadata": {},
   "source": [
    "The general idea behind *pmodule* is to limit output to software that is compiled with a certain compiler and MPI version.\n",
    "\n",
    "* First compilers are shown. Selecting one.\n",
    "* Then MPI versions are shown as well. Select one.\n",
    "* MPI specific software is shown as well.\n",
    "\n",
    "Dependencies are a bit too complex to always fit into this scheme, but as a guideline this should help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52fe825",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Search for software\n",
    "# module search openmpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726e8e66",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Include only lightly tested newer software\n",
    "# module use unstable\n",
    "# module search openmpi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c13c947",
   "metadata": {},
   "source": [
    "Sysadmins install software into a certain location on disk. To use it, environment variables like *PATH* have to be set. You could do it by hand, *pmodule* just simplifies the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21790b51",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Show environment changes\n",
    "# module show gcc/14.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf62068",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Execute environment changes\n",
    "# module add gcc/14.2.0\n",
    "# which gcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2068ddd7",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# List added modules\n",
    "# module list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7579413a",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Reset pmodule managed environment\n",
    "# module purge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abcc9fb",
   "metadata": {},
   "source": [
    "For Python, the *anaconda* module is provided, together with some preinstalled environments. (**ATTENTION**: by itself, the PSI anaconda modue only provides the *python3* executable, not *python*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e03769",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# module add anaconda\n",
    "\n",
    "# Show existing conda envs\n",
    "# conda env list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0135e54f",
   "metadata": {},
   "source": [
    "## Batch System\n",
    "\n",
    "People already struggle to cooperate in ordinary kitchens (idiom: too many cooks spoil the broth), so cooperating remotely for orderly access to compute nodes may sound like an impossibility. To end the endless quarrels and frustration amongst overeager scientists, some smart people came up with the idea of access control software in the form of a batch system.\n",
    "\n",
    "Batch systems like **slurm** ([website](https://slurm.schedmd.com)) schedule compute jobs from a queue. Scientist add jobs to such a queue by adding *batch scripts* to it. These batch scripts define compute jobs and metadata, such as the maximum expected runtime, number of nodes and cores, for the job. The batch system assigns a rule based priority to each job, and schedules compute jobs according to the assigned priority. The rules for priority assignment are designed by system administrator to ascertain fair and efficient cluster usage.\n",
    "\n",
    "For special tasks, that need predefined resources, reservations can be created.\n",
    "\n",
    "Most MPI (used for distributed and parallel programming) implementations support slurm as a batch system. Thus, if slurm reserves resources for an MPI job, these resources are automatically used by MPI. \n",
    "\n",
    "Selection of slurm terms:\n",
    "* *cpu* - a CPU hardware thread. If hyperthreading is not used, this corresponds to a CPU core.\n",
    "* *gpu*\n",
    "* *task* - OS process, normally an MPI rank. Every task runs on a *cpu*.\n",
    "* *socket* - a CPU\n",
    "* *node* - compute node\n",
    "* *partition* - basically a queue for compute jobs with access to certain resources\n",
    "* *cluster* - collection of compute nodes\n",
    "\n",
    "Selection of slurm commands (see man pages for details):\n",
    "* *sinfo* - show slurm resources\n",
    "* *squeue* - shows job information\n",
    "* *srun* - run app on existing allocation or run app interactively\n",
    "* *salloc* - get resource allocation and run a command\n",
    "* *scancel -b job_id* - send (cancel) signal to a job\n",
    "* *sbatch* - enqueue batch script, run noninteractively\n",
    "* *sacct* - show accounting info\n",
    "\n",
    "**NOTE**: For the course, we'll have a reservation. The name of which, at the time of writing, was unknown. Please add `--reservation=name` to commands leading to resource allocations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8cab11",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Show defined clusters and partitions\n",
    "# sinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e372a46d",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Show job info on gmerlin6 cluster\n",
    "# squeue --clusters=gmerlin6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e54bb2",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Show job info for a specific user\n",
    "# squeue -u $USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d3b48b",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Run one task executing the command hostname on cluster 'merlin6' and partition 'hourly' with a timelimit of 30 seconds\n",
    "# srun --cluster=merlin6 --partition=hourly --time=00:00:30 --ntasks=1 hostname\n",
    "# salloc --cluster=merlin6 --partition=hourly --time=00:00:30 --ntasks=1 hostname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f80c73",
   "metadata": {},
   "source": [
    "*srun* is like a shortcut for *salloc .. srun*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fede5b3",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Run two tasks executing the command hostname on cluster 'merlin6' and partition 'hourly', label the output with the task number\n",
    "# srun --cluster=merlin6 --partition=hourly --time=00:00:30 --ntasks=2 --label hostname\n",
    "# salloc --cluster=merlin6 --partition=hourly --time=00:00:30 --ntasks=2 srun --label hostname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0dc921",
   "metadata": {},
   "source": [
    "**IO redirection**\n",
    "\n",
    "By default *srun* sends stdin to all tasks, and redirects stdout/stderr to itself. By specifying *--input=1*, stdin can be sent to task 1 only. There are many more possibilities like redirecting output into one or several (one per task) files, see man page. The option *--output* redirects both stdout and stderr output, unless *--error* is specified explicitly for stderr output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d43324",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# echo \"hello\" | srun --cluster=merlin6 --partition=hourly --time=00:00:30 --ntasks=2 --label cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe23b52a",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# echo \"hello\" | srun --cluster=merlin6 --partition=hourly --time=00:00:30 --ntasks=2 --label --input=1 cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c67089f",
   "metadata": {},
   "source": [
    "**Resource selection**\n",
    "\n",
    "Slurm allows a wide range of commandline arguments to select resources, and places the processes in a cgroup that restricts resources to the selected ones. We'll use the **taskset** command to show cpu masks, and **nvidia-smi** to show available gpus.\n",
    "\n",
    "* *--ntasks* specifies the number of tasks, potentially running anywhere.\n",
    "* A combination of *--nodes* and *--ntasks-per-node* selects compute nodes and tasks per selected node.\n",
    "* A combination of *--ntasks* and *--gpus-per-task* selects a number of gpus per task.\n",
    "* *--hint=nomultithread* places only one task per core\n",
    "* *--mem-per-cpu* allocates a minimum amount of memory per task\n",
    "* *--exclusive* grabs entire compute nodes only.\n",
    "* *--mem=0* grabs all compute node memory\n",
    "\n",
    "There are many more, see the man page for your slurm command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87b727d",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# srun --cluster=merlin6 --partition=hourly --ntasks=2 --time=00:00:30 --label bash -c 'echo \"$(hostname; taskset -c -p $$)\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11946f7d",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# srun --cluster=merlin6 --partition=hourly --nodes=2 --ntasks-per-node=2 --time=00:00:30 --label bash -c 'echo \"$(hostname; taskset -c -p $$)\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12021c5",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# srun --cluster=gmerlin6 --partition=gpu-short --ntasks=2 --gpus-per-task=2 --time=00:00:30 --label bash -c 'echo \"$(hostname; nvidia-smi)\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a29275",
   "metadata": {},
   "source": [
    "**Batch scripts**\n",
    "\n",
    "The *salloc* command (and *srun* as used so far) actually waits for a resource allocation. If the job queues are fulll with higher priority jobs, the waiting time might seriously test your patience. The real deal with batch systems becomes apparent with the *sbatch* command, that allows to submit non-interactive batch scripts to job queues. In other words: submit a batch script, enjoy your week-end, and when your back, the results of the computation are ready.\n",
    "\n",
    "The *sbatch* command allocates resources, and runs the batch script like *salloc* would do it. By default, output is redirected into files \"slurm-%j.out\", where %j is the job id. Commandline arguments can be given directly to *sbatch*, or much better for documentation, within the batch script itself in the form\n",
    "\n",
    "> #SBATCH commandline argument\n",
    "\n",
    "*sbatch* sets a pletora of environment variables the script can react on, like *SLURM_SUBMIT_DIR*, see the manpage for more. In order to use allocated resourcces with MPI jobs, use something like\n",
    "\n",
    "> srun python my-mpi-code.py\n",
    "\n",
    "in the batch script.\n",
    "\n",
    "**Exercise**: Look at the use-srun.sbatch file and submit it with `sbatch use-srun.sbatch`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f2b25e",
   "metadata": {},
   "source": [
    "**Array jobs**\n",
    "\n",
    "The most primitive form of parallelism is just running the same program on distinct inputs in parallel. For this slurm features *array jobs*. Array jobs demand the array commandline argument\n",
    "\n",
    "> #SBATCH --array=1-10\n",
    "\n",
    "This will launch 10 jobs with the environment variable *SLURM_ARRAY_TASK_ID* set to distinct integer values from 1 to 10. Your code should map this integer value to distinct program inputs. In the *--output* argument, *%a* can be used to fill in the array index.\n",
    "\n",
    "**Excercise**: Look at arra-job.py and array-job.sbatch and submit with `sbatch arra-job.sbatch`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ece40a9",
   "metadata": {},
   "source": [
    "**Running multiple programs**\n",
    "\n",
    "Slurm allows to start several programs as a unit by separating the parts with `:`. Some options, like *--ntasks* can be given to parts separately. Task numbering continues from one part to the next. This could be used to for parallel MPI jobs to run a single rank with the debugger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524e4914",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# srun --cluster=merlin6 --partition=hourly --time=00:00:30 --label --ntasks=1 echo hello one : --ntasks=1 echo hello two : --ntasks=2 echo hello three"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c2640b",
   "metadata": {},
   "source": [
    "**X forwarding**\n",
    "\n",
    "X forwarding is done with the *--x11* commandline option. By default, it does forwarding from all tasks. The environment variable *SLURM_PROCID* will help to order the windows by task number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7936213",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# srun --cluster=merlin6 --partition=hourly --time=00:05:00 --x11 --ntasks=2 xterm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50f0fe8",
   "metadata": {},
   "source": [
    "**Terminal**\n",
    "\n",
    "The *--pty* option creates a pseudo terminal for task 0. It can be used to get a terminal on a compute node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf0d716",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# srun --cluster=merlin6 --partition=hourly --time=00:05:00 --ntasks=1 --pty bash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ac67b5",
   "metadata": {},
   "source": [
    "## MPI Overview\n",
    "\n",
    "MPI ([Message Passing Interface](https://www.mpi-forum.org/)) is the most widely used standard for distributed and parallel computing on HPC clusters. MPI standardizes collaboration through communicator objects between processes that run in parallel on one or on separate compute nodes. We'll focus on MPI 5.0 and the world model (there's also the session model) that predefines the `MPI_COMM_WORLD` communicator object. This object is comprised of all processes. Processes are numbered sequentially by *rank*, starting with 0.\n",
    "\n",
    "Several implementations of MPI exist. OpenMPI, MPICH, and MVAPICH are well known, vendor based implementations usually are specializations of these - Intel MPI, Cray MPI, ... I'll focus on [OpenMPI](https://www.open-mpi.org) here whenever implementation details become important.\n",
    "\n",
    "Communicator objects come in two flavours. The first, like *MPI_COMM_WORLD*, are intra-communicators used to communicate between the processes in a group. The second, inter-communicators, are for communicating between two distinct groups of processes. In the following, we'll focus on intra-communicators.\n",
    "\n",
    "Intra-communicator objects are associated with\n",
    "\n",
    "* *context* for communication (e.g. messages within a context are ordered according to some rules)\n",
    "   * messages sent from one rank to another one are received in the order they were sent\n",
    "* *group* of processes, ordered by sequentially increasing integer *rank* (starting with 0)\n",
    "   * groups with the same processes, but different ordering, are not exactly equal\n",
    "* *topology*, virtual neighborhood information\n",
    "   * like cartesian, or graph topologies\n",
    "   * topologies map ranks to coordinates or graph vertices\n",
    "* *attributes*, (tag, value) pairs, for additional info\n",
    "   * e.g. *MPI_COMM_WORLD* has the *MPI_TAG_UB* attribute (upper bound for tags)\n",
    "* *error handler*\n",
    "   * by default, predefined *MPI_ERRORS_ARE_FATAL* handler is set\n",
    "   * the predefined *MPI_ERRORS_RETURN* handler can be set to handle errors in the code\n",
    "\n",
    "Communicators can be duplicated or split into subcommunicators with distinct groups in various ways. It's also possible to create new communicators specifying a group of processes. Different communicators can be used independently.\n",
    "\n",
    "The predefined intra-communicator `MPI_COMM_SELF` only has the local rank in the group.\n",
    "\n",
    "**Source and destination**\n",
    "\n",
    "If source or destination is needed, they are specified by the process rank. *MPI_ANY_SOURCE* accept incoming data from any rank. *MPI_PROC_NULL* is a dummy rank, that may be valid as source or destination. This might be useful to simplify the code.\n",
    "\n",
    "**Point to point communication**\n",
    "\n",
    "Send and receive commands will be seen in sending order on the receiver within a communication context. The datatypes must match on both sides. The receiver can specify a higher element count than the sender, but not vice versa.\n",
    "\n",
    "Send operations specify a destination (or *MPI_PROC_NULL*) rank and an integer tag. Receive operations specify the source (or *MPI_ANY_SOURCE*/*MPI_PROC_NULL*) and an integer tag (or *MPI_ANY_TAG*). The source and destination ranks and the tags must match.\n",
    "\n",
    "Receive operations have a status object output argument, it is filled with info on the number of transferred elements, the source rank, the tag, or an error on failure. *MPI_STATUS_IGNORE* may be given as argument if the status is unimportant.\n",
    "\n",
    "Send and receive exist in many different versions.\n",
    "\n",
    "| Mode        | Blocking | Non-blocking | Persistent |\n",
    "| ----------- | -------- | ------------ | ---------- |\n",
    "| Normal      |          | I            |   _init    |\n",
    "| Buffered    | B        | IB           | B _init    |\n",
    "| Synchronous | S        | IS           | S _init    |\n",
    "| Ready       | R        | IR           | R _init    |\n",
    "| Partitioned | -        | -            | P _init    |\n",
    "\n",
    "*Blocking*\n",
    "\n",
    "Sender waits until the send buffer can be used again. Receiver waits until the data is received.\n",
    "\n",
    "*Non-blocking*\n",
    "\n",
    "Start the opration in the background. Return a *request* object that **MUST** be queried for completion or waited for in order to finish the operation. Great for hiding communication.\n",
    "\n",
    "*Persistent*\n",
    "\n",
    "Prepare the communication operation and return a request object. The request can then be started later and repeatedly. After starting the operation, the request is used as in *non-blocking* communication. Might safe time if the same operation is done many times.\n",
    "\n",
    "*Modes*\n",
    "\n",
    "* *Normal*: The system decides how this is done\n",
    "* *Buffered*: Data is buffered. System sends data in the background.\n",
    "* *Synchronous*: Wait until receiver starts receiving and send buffer is no longer used.\n",
    "* *Ready*: Receiver **MUST** be ready before send. No buffering is allowed.\n",
    "* *Partitioned*: Send/receive data in chunks. Chunk size must not match.\n",
    "\n",
    "**Collective communication**\n",
    "\n",
    "Collective operations must be done by all ranks in the group associated with an intra-communicator. They exist in normal, non-blocking, and persisten variants.\n",
    "\n",
    "* *Barrier*: synchronization\n",
    "* *Broadcast*: one to all\n",
    "* *Gather*: all to one\n",
    "* *Scatter*: distribute content from one to all\n",
    "* *Allgather*: like *gather*, but all receive the result\n",
    "* *Alltoall*: all distribute content to all\n",
    "* *Reduction*: reduction from all to one\n",
    "   * maximum\n",
    "   * minimum\n",
    "   * sum\n",
    "   * product\n",
    "   * logical and\n",
    "   * bit-wise and\n",
    "   * logical or\n",
    "   * bit-wise or\n",
    "   * logical exclusive or (xor)\n",
    "   * bit-wise exclusive or (xor)\n",
    "   * max value and location\n",
    "   * min value and location\n",
    "* *Allreduce*: like *reduce*, but all receive the result\n",
    "* *Reduce-Scatter*: like *reduce*, but result is distributed to all\n",
    "* *Scan*: prefix reduction, rank *i* recives reduction result from rank *0...i*\n",
    "\n",
    "![Collectives](img/Collectives.png)\n",
    "\n",
    "**Virtual topologies**\n",
    "\n",
    "Create neighbourhood links and communicate with neighbours.\n",
    "\n",
    "* Create cartesian topology\n",
    "* Create graph topology\n",
    "* Neighbourhood gather\n",
    "* Neighbourhood alltoall\n",
    "\n",
    "**One sided communication**\n",
    "\n",
    "Remote memory window access and operations, separates communication from synchronization.\n",
    "\n",
    "* Read\n",
    "* Write\n",
    "* Accumulate\n",
    "* Read and update\n",
    "* Compare and swap\n",
    "* \n",
    "\n",
    "**Datatypes**\n",
    "\n",
    "You can create your own structured and array data types.\n",
    "\n",
    "**I/O**\n",
    "\n",
    "MPI supports parallel I/O. Libraries like HDF5 support MPI I/O.\n",
    "\n",
    "A file is abstracted as a view on a sequence of elementary data type. Both collective and individual access functions are standardized, as well as blocking and non-blocking file access.\n",
    "\n",
    "Depending on the underlying file system and implementation, MPI I/O supports\n",
    "\n",
    "* collective buffering: I/O is performed by a subset of compute nodes that collect smaller chunks into bigger ones\n",
    "* striped access: file is distributed to stripes on ditinct I/O devices to increase throughput\n",
    "* data access through chunks: like subarrays\n",
    "\n",
    "**Dynamic processes**\n",
    "\n",
    "Create new processes and communators for interacting with them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff529b4",
   "metadata": {},
   "source": [
    "## Data Catalog and Backup\n",
    "\n",
    "Scientists often produce large datasets. Sometimes these need to be transferred to or away from the cluster. Archiving for later retrieval and reexamination or verification of scientific results is also important. To facilitate retrieval, archived data is enriched with meta data thaht describes how data was produced and what it is about.\n",
    "\n",
    "* Data transfer from and to the cluster is supported at PSI via [Globus](https://www.globus.org/data-transfer).\n",
    "* Meta data enriched data archiving with [SciCat](https://github.com/SciCatProject)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
