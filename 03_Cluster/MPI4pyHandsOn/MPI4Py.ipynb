{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a942da2b-436d-4bb1-ba95-5e9d30d895b1",
   "metadata": {},
   "source": [
    "# MPI4py Hands On\n",
    "\n",
    "What is MPI4py? Basically a python implementation of the MPI Standard. (See presentation of HC)\n",
    "\n",
    "It is a bit more object-oriented than the C API (e.g. `MPI_ -> MPI.` or `comm_ -> comm.`), but conceptually close.\n",
    "\n",
    "## Prerequisite for this course\n",
    "\n",
    "In the Merlin jupyter notebook, open a terminal then run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ee463d-c9d7-46db-a1ad-bcb9d6f6c437",
   "metadata": {},
   "outputs": [],
   "source": [
    "$ module load anaconda/2024.08\n",
    "$ conda create -p /data/user/$USER/conda-envs/MPI numpy scipy scikit-learn matplotlib mpi4py -c conda-forge\n",
    "$ conda activate /data/user/$USER/conda-envs/MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfebada-fe39-4e95-ac1c-a158d0901bc7",
   "metadata": {},
   "source": [
    "All examples will be run from the terminal (not within the notebook)\n",
    "\n",
    "You can also run the examples on your local computer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5863e07-aea4-47eb-b5be-d76443eed5cb",
   "metadata": {},
   "source": [
    "## Hello MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f041ddc5-7e0f-4ceb-8eae-ad37876fccd7",
   "metadata": {},
   "source": [
    "As a initial test, try to run the following python script (named `helloMPI.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03925851-3b8f-4115-af80-5aae3f23bb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "# Initialize MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "# Print a message from each process\n",
    "print(f\"Hello from process {rank} of {size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3605b5f-d4d0-4814-b63d-98a40e7d9b52",
   "metadata": {},
   "source": [
    "On a terminal, run the program as:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f80c220d-5099-42d0-affa-6bc73e3068a9",
   "metadata": {},
   "source": [
    "$ mpirun -n 2 python helloMPI.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0acb07-9120-428c-a58a-b0cb5e194e3c",
   "metadata": {},
   "source": [
    "If everything is installed properly, you should see as output: (If not, make sure you loaded the proper conda env in your terminal)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4f8e78ed-65c0-41dd-8e3b-610bc9744e16",
   "metadata": {},
   "source": [
    "Hello from process 0 of 2\n",
    "Hello from process 1 of 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903ede82-292c-44fe-8d4c-5e4979623910",
   "metadata": {},
   "source": [
    "So here we started two python processes, each process printing simply its ID (rank) and total number of processes started. No IPC so far"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef43155-1097-426b-b833-d7c617d41c97",
   "metadata": {},
   "source": [
    "## Basic Communication Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36c6504-7ed2-4a74-98d1-41fdbc641a91",
   "metadata": {},
   "source": [
    "General Note: We will only use `numpy` arrays to do Inter-Process-Communication (IPC). This is the most common use-case in HPC and DataScience. \n",
    "\n",
    "But you can in principle exchange any python data between processes. \n",
    "\n",
    "(Less efficient than numpy arrays, since data needs to be pickled/serialized under the hood, whereas the numpy array can be treated like a C-pointer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6d8237-490f-427c-a2c7-1a1c0bba702b",
   "metadata": {},
   "source": [
    "### Point-to-Point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5bb576-b235-496a-ba4f-893fea93942b",
   "metadata": {},
   "source": [
    "We send a 1D numpy array (10 items, datatype `float64`) from process 0 to process 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316ac629-8d44-4683-989c-e895cd9dec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    data = np.arange(10, dtype=np.float64)\n",
    "    comm.Send(data, dest=1, tag=13)\n",
    "    print (\"0 sent:\", data)\n",
    "elif rank == 1:\n",
    "    data = np.empty(10, dtype=np.float64)\n",
    "    comm.Recv(data, source=0, tag=13)\n",
    "    print (\"1 recv:\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf145193-d05d-47c4-b7cb-ba3b7cb367b2",
   "metadata": {},
   "source": [
    "Run with two processes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bc57148e-44fa-43a4-8d80-5c0f0983b18b",
   "metadata": {},
   "source": [
    "$ mpirun -n 2 python Point2Point_1d.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f475c9a-4f39-43f2-a5e4-39fec6913eee",
   "metadata": {},
   "source": [
    "### <span style=\"color:Blue\">_Exercise_ </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ef3f19-55a0-496c-9dbf-b1e3cbfac323",
   "metadata": {},
   "source": [
    "Send a 2d array (size 3x3, datatype `int64`) from rank 0 to rank 1 \n",
    "\n",
    "(In productive applications, these would typically be detector images)\n",
    "\n",
    "_Hint_: You can treat metadata (i.e. image size) as hard coded (global)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503a2850-65e7-4ec9-86c7-50eaf52dcaa5",
   "metadata": {},
   "source": [
    "_Proposed solution_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59daa023-91bf-4c06-ae9f-5659494a7041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "imageSize = 3\n",
    "\n",
    "if rank == 0:\n",
    "    #We emulate a (small) detector image, typically rank 0 would load it from file\n",
    "    detImage_2BeSent = np.arange(imageSize**2, dtype=np.int64).reshape(imageSize,imageSize)\n",
    "    comm.Send(detImage_2BeSent, dest=1, tag=13)\n",
    "    print (\"0 sent:\\n\", detImage_2BeSent)\n",
    "elif rank == 1:\n",
    "    #We allocate a 1d array as receive buffer which will be reshaped at the end\n",
    "    data = np.empty(imageSize**2, dtype=np.int64)\n",
    "    comm.Recv(data, source=0, tag=13)\n",
    "    detImage_Recv = data.reshape(imageSize,imageSize)\n",
    "    print (\"1 recvd:\\n\", detImage_Recv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350feae1-7747-4df9-92b6-174fcb4dd0d8",
   "metadata": {},
   "source": [
    "### <span style=\"color:Blue\">_Exercise_ </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdc4686-4bb5-4d79-bdcf-7ba1b3b8cebc",
   "metadata": {},
   "source": [
    "Now we try to improve the code in the sense that we make it more generic. \n",
    "Assume the receiver does not know the metadata (i.e. the image size)\n",
    "Hence the sender must as well send the metadata (besides data)\n",
    "\n",
    "_Hint_: Use MPI-tags to distinguish metadata and data\n",
    "\n",
    "PseudoCode:\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e4df8310-876b-4419-8322-765250c1515c",
   "metadata": {},
   "source": [
    "dataTag=..\n",
    "metaDataTag=..\n",
    "\n",
    "if rank == 0:\n",
    "  emulateData\n",
    "  sendMetaData\n",
    "  sendData\n",
    "if rank == 1:\n",
    "  receive Metadata\n",
    "  receive Data \n",
    "  Reshape received Data according received metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9406f756-7586-462e-ac9b-4536dde4cfbd",
   "metadata": {},
   "source": [
    "_Proposed solution_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb1b333-1d1c-40d3-a857-24da617e1b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "dataTag = 13\n",
    "metaDataTag = 14\n",
    "\n",
    "if rank == 0:\n",
    "    #We emulate a (small) detector image, typically rank 0 would load it from file\n",
    "    #Hence only rank 0 knows its size (might be varying). Datatype is assumed fixed\n",
    "    imageSize = 3 #this information would e.g be derived from hdf5 metadata of image directly\n",
    "    #imageSize = 50 #Deadlocks, but this is MPI-implementation dependent\n",
    "    #Send Meta data (we could also send an int here, but for a non quadratic image would be an array)\n",
    "    comm.Send(np.array(imageSize, dtype=np.int64), dest=1, tag=metaDataTag)\n",
    "    detImage_2BeSent = np.arange(imageSize**2, dtype=np.int64).reshape(imageSize,imageSize)\n",
    "    #Send data\n",
    "    comm.Send(detImage_2BeSent, dest=1, tag=dataTag)\n",
    "    print (\"0 sent:\\n\", detImage_2BeSent)\n",
    "\n",
    "elif rank == 1:\n",
    "    #Receive Metadata first (We must know how large the buffer is that we allocate\n",
    "    imageSize_asArray = np.empty(1, dtype=np.int64)\n",
    "    comm.Recv(imageSize_asArray, source=0, tag=metaDataTag)\n",
    "    imageSize = imageSize_asArray[0]\n",
    "    #Now that we know what to expect we can allocate receive buffer\n",
    "    data = np.empty(imageSize**2, dtype=np.int64)\n",
    "    comm.Recv(data, source=0, tag=dataTag)\n",
    "    detImage_Recv = data.reshape(imageSize, imageSize)\n",
    "    print (\"1 recvd:\\n\", detImage_Recv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909dda05-0435-4bdd-94e3-571d09c74d35",
   "metadata": {},
   "source": [
    "### <span style=\"color:Blue\">_Exercise_ </span> \n",
    "\n",
    "No make the sender rank send the data first, such that it is out of order with the receiver.\n",
    "\n",
    "_Question_: Why is this code working? \n",
    "\n",
    "The `Send` and `Recv` are blocking, but the sequence of `Send`s does not match the Sequence of `Recv`s (order is reversed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76f82d0-8741-4b3e-a04e-9baf8f0466e9",
   "metadata": {},
   "source": [
    "## Blocking versus Non-Blocking Communication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77c1fe4-02c9-4c61-a48d-55f39a3a5131",
   "metadata": {},
   "source": [
    "### What means blocking? Why Deadlocks depend on data size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a50d030-408e-48ec-a3cd-62ad93314835",
   "metadata": {},
   "source": [
    "When you use blocking sends (MPI.Send) and blocking receives (MPI.Recv) in MPI, the order of operations matters, especially for large data. \n",
    "\n",
    "If the order of MPI.Recv calls on one process does not match the order of MPI.Send calls on the other, a deadlock can occur. Here’s why:\n",
    "How Blocking Communication Works\n",
    "\n",
    "    Blocking Send (MPI.Send): The call does not return until the message buffer can be reused (i.e., the message is either copied out or the matching receive is ready).\n",
    "    Blocking Receive (MPI.Recv): The call does not return until the message is received.\n",
    "\n",
    "Why Small Data Works\n",
    "\n",
    "For small messages, MPI implementations often use eager protocol:\n",
    "\n",
    "    The data is copied into a temporary buffer and sent immediately.\n",
    "    The send call returns as soon as the data is buffered, even if the receive is not yet posted.\n",
    "    This can hide mismatched order for small messages.\n",
    "\n",
    "Why Large Data Fails\n",
    "\n",
    "For large messages, MPI typically uses rendezvous protocol:\n",
    "\n",
    "    The send call waits until the matching receive is posted before transferring data.\n",
    "    If the receive order is wrong, both processes can end up waiting for each other, causing a deadlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80ae06e-0f32-45e0-b17c-c682fab89108",
   "metadata": {},
   "source": [
    "### Making blocking/non-blocking more explicit explicit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423347f3-f8bd-4be6-89f6-8f80913b7228",
   "metadata": {},
   "source": [
    "Take away from above and motivation:\n",
    "\n",
    "Blocking Send (MPI.Send): The call does not return until the  <span style=\"color:red\">send message buffer can be reused</span>.\n",
    "\n",
    "Note that nothing is said about the state of the receiver. \n",
    "This is confusing or at least vague\n",
    "\n",
    "And: <span style=\"color:red\">This is also mean</span>: If you test your MPI-application with a tiny dummy dataset on the login node, all works well,\n",
    "if you then submit your large real-world job to the cluster, it deadlocks)\n",
    "\n",
    "But this is how it is defined in the MPI standard. (And the standard is always and by defintion correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3570e25d-30ae-4155-86ff-8f8417a4ab99",
   "metadata": {},
   "source": [
    "#### Synchronous Send and Receive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a12573-c9a5-4b5b-b99b-8325a3e669a6",
   "metadata": {},
   "source": [
    "You can use `comm.Ssend` to enforce synchronous send. `comm.Ssend` waits for a hand shake with the corresponding receiver, only afterwards it returns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a22ba6a-353d-47e8-98a4-9305b61ccfc5",
   "metadata": {},
   "source": [
    "### <span style=\"color:Blue\">_Exercise_ </span> \n",
    "\n",
    "Edit the above script and replace a single `comm.Send` by a `comm.Ssend` such that the application deadlocks for sure (also for small datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c043c5df-773b-433c-b561-18ec4d7f63b4",
   "metadata": {},
   "source": [
    "#### Aynchronous Send and Receive\n",
    "You can use `comm.Isend` to enforce asynchronous (or non-blocking) send\n",
    "`comm.Isend` should be used tother with `comm.Irecv`, the non-blocking version of `comm.Recv`\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a674e5-1be9-437f-9911-aace53132020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    # Non-blocking send\n",
    "    data = np.asarray([1, 2, 3])\n",
    "    req_send = comm.Isend(data, dest=1, tag=77)\n",
    "    # Do some computation while the send is in progress\n",
    "    result = 42**2\n",
    "    # Wait for the send to complete\n",
    "    req_send.Wait()\n",
    "    print (\"Sent:\", data)\n",
    "elif rank == 1:\n",
    "    #Allocate memory for the receive buffer\n",
    "    data = np.empty(3, np.int64)\n",
    "    # Non-blocking receive\n",
    "    req_recv = comm.Irecv(data, source=0, tag=77)\n",
    "    # Do some computation while the receive is in progress\n",
    "    result = 100/26\n",
    "    # Wait for the receive to complete\n",
    "    req_recv.Wait()\n",
    "    print(\"Received:\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ad0ba0-39c1-4fb2-b971-96f19ab86246",
   "metadata": {},
   "source": [
    "The above example also highlights that non-blocking calls can be used to interleave communication with calculation.\n",
    "\n",
    "This might give a performance increase as additional benefit. But this is not in the focus here. (The focus is on correct code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cec73c-650b-4403-9ae6-344eeb2941a3",
   "metadata": {},
   "source": [
    "### <span style=\"color:Blue\">_Exercise_ </span> \n",
    "\n",
    "Modify the above script such that it never deadlocks, no matter what the sequence of sends/receives is and no matter how big data is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df90c1a-7cfc-4512-b8e9-1f1049ea51bf",
   "metadata": {},
   "source": [
    "#### _Proposed Solution_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bb6361-b025-4ce2-892b-39ee0983897d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "dataTag = 13\n",
    "metaDataTag = 14\n",
    "\n",
    "if rank == 0:\n",
    "    #We emulate a (small) detector image, typically rank 0 would load it from file\n",
    "    #Hence only rank 0 knows its size (might be varying). Datatype is assumed fixed\n",
    "    imageSize = 3 #this information would e.g be derived from hdf5 metadata from image directly\n",
    "    detImage_2BeSent = np.arange(imageSize**2, dtype=np.int64).reshape(imageSize,imageSize)\n",
    "    #Send data first (*non-blocking*)\n",
    "    req_data = comm.Isend(detImage_2BeSent, dest=1, tag=dataTag)\n",
    "    print (\"0 sent:\\n\", detImage_2BeSent)\n",
    "    #Send Meta data (executes immediately since above in s non-blocking)\n",
    "    req_metadata = comm.Isend(np.array(imageSize, dtype=np.int64), dest=1, tag=metaDataTag)\n",
    "    req_data.Wait()\n",
    "    req_metadata.Wait()\n",
    "elif rank == 1:\n",
    "    #Receive Metadata first (We must know how large the buffer is that we allocate\n",
    "    imageSize_asArray = np.empty(1, dtype=np.int64)\n",
    "    req_metadata = comm.Irecv(imageSize_asArray, source=0, tag=metaDataTag)\n",
    "    req_metadata.Wait() #make sure we have data here (comment out line and se what happens)\n",
    "    imageSize = imageSize_asArray[0]\n",
    "    #Now that we know what to expect we can allocate receive buffer\n",
    "    data = np.empty(imageSize**2, dtype=np.int64)\n",
    "    req_data = comm.Irecv(data, source=0, tag=dataTag)\n",
    "    req_data.Wait()\n",
    "    detImage_Recv = data.reshape(imageSize, imageSize)\n",
    "    print (\"1 recvd:\\n\", detImage_Recv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163a8716-f012-417a-a4e2-ee6827c6e140",
   "metadata": {},
   "source": [
    "### <span style=\"color:Blue\">_Exercise_ </span> :\n",
    "\n",
    "Comment out line `req_metadata.Wait()` (in rank 1) and see what happens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae09279-8579-4c93-a98d-2c27d0325bca",
   "metadata": {},
   "source": [
    "## Basic Communication Patterns Continued"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c145603e-96a2-4118-a8ff-d0ef48b39281",
   "metadata": {},
   "source": [
    "### Broadcast\n",
    "As the name suggest, comm.BroadCast is used to fant out data from one rank to all the others.\n",
    "\n",
    "The broadcasting rank is typially rank zero, whereas all other ranks > 0 recieve data\n",
    "\n",
    "Example (taken from official documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d39e9f-a4b4-4946-8235-218d50b7dd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "#Rank 0 allocates and initializes/define data to be broadcast\n",
    "if rank == 0:\n",
    "    data = np.arange(100, dtype='i')\n",
    "#All other ranks allocate receive buffer\n",
    "else:\n",
    "    data = np.empty(100, dtype='i')\n",
    "#Note 'collective' operation (*every* rank calls Bcast, not just rank 0) \n",
    "comm.Bcast(data, root=0)\n",
    "#Check for all ranks that they have received the Broadcaster's data\n",
    "for i in range(100):\n",
    "    assert data[i] == i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710a58bf-62ba-4bb9-91d0-112f87b791a6",
   "metadata": {},
   "source": [
    "### Scatter\n",
    "\n",
    "Data from rank 0 is split into pieces, these pices are then distributed to other ranks, similar as for Bcast\n",
    "\n",
    "Difference between Scatter and Broadcast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abc2909-9c25-41d0-ba88-5883f774622e",
   "metadata": {},
   "source": [
    "<img src=\"MPI_ScatterBcast.png\" alt=\"Alt Text\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f2fe12-a1eb-4bab-bd55-47e8a362c4d7",
   "metadata": {},
   "source": [
    "_Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fac2c21-627d-4008-8819-94613edfeed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "matrixSize = 6 #Square matrix\n",
    "\n",
    "if matrixSize % size != 0:\n",
    "    print (\"Warning, number of rows is not multiple of number of cores\")\n",
    "\n",
    "# Only rank 0 creates the full array\n",
    "if rank == 0:\n",
    "    A = np.arange(matrixSize**2, dtype=np.float64).reshape(matrixSize, matrixSize)\n",
    "    print(f\"Root process ({rank}) created the Matrix:\\n{A}\")\n",
    "else:\n",
    "    A = None #This declaration is mandatory\n",
    "\n",
    "# Scatter the rows\n",
    "# Calculate how many rows each process gets (above we made sure that no remainder)\n",
    "matrixRowsPerProcess = matrixSize//size\n",
    "#Allocate memory for row-slices (also for process 0)\n",
    "A_rows = np.empty((matrixRowsPerProcess, matrixSize), dtype=np.float64)\n",
    "\n",
    "# Scatter the rows (collective operation)\n",
    "comm.Scatter(A, A_rows, root=0)\n",
    "\n",
    "# Print the result on each process\n",
    "print(f\"Process {rank} received:\\n{A_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6561b80-ac2f-41ec-b4aa-5ccc2bce0731",
   "metadata": {},
   "source": [
    "### Gather\n",
    "\n",
    "data from all ranks is aggregated at rank 0\n",
    "\n",
    "\n",
    "<img src=\"MPI_Gather.png\" alt=\"Alt Text\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f84a81-2a20-4ee8-b027-c05c01f34eef",
   "metadata": {},
   "source": [
    "_Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f47b530-47b6-4910-9c36-28d779e8c6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "#Each rank inits ist share\n",
    "vectorLengthLocal = 4\n",
    "vectorLocal = np.arange(vectorLengthLocal, dtype=np.float64)\n",
    "print(f\"Process {rank} created local vector: {vectorLocal}\")\n",
    "\n",
    "# Root process prepares to receive all data\n",
    "if rank == 0:\n",
    "    vector = np.empty(size*vectorLengthLocal, dtype=np.float64)\n",
    "else:\n",
    "    vector = None\n",
    "\n",
    "# Gather all local arrays to root\n",
    "comm.Gather(vectorLocal, vector, root=0)\n",
    "# Root process prints the gathered array\n",
    "if rank == 0:\n",
    "    print(f\"Root process ({rank}) gathered vector: {vector}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61dceb8-623f-4e73-a2f7-faef39ad3155",
   "metadata": {},
   "source": [
    "### <span style=\"color:Blue\">_Exercise_ </span> \n",
    "\n",
    "Implement parrallel matrix multiplication `Ax` for a (4 x 4) matrix A and a (4 x 1) vector `x` using `Bcast`,  `Scatter` and `Gather`.\n",
    "\n",
    "The code should work for 2 and 4 cores.\n",
    "\n",
    "\n",
    "At the end, rank 0 (who collects the results) makes sure that the parallel version is correct (by doing the calculation itself)\n",
    "\n",
    "<img src=\"Ax.jpg\" alt=\"Ax\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "56b1752e-eca4-44eb-9d72-39f942473270",
   "metadata": {},
   "source": [
    "Pseudo Code\n",
    "-----------\n",
    "\n",
    "  - rank 0 allocates and initializes Matrix and vector\n",
    "\n",
    "  - rank 0 allocates result\n",
    "\n",
    "  - Scatter Matrix\n",
    "\n",
    "  - Broadcast vector\n",
    "\n",
    "  - Parallel (local) Matrix multiplication\n",
    "\n",
    "  -rank 0 gathers local results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232e1b73-5206-4464-b276-1b7e6475f631",
   "metadata": {},
   "source": [
    "#### _Proposed solution_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3552ca8d-74d6-4097-9cf0-9c65d3454265",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "matrixSize = 4 #Square matrix\n",
    "matrixRowsPerRank = matrixSize//size\n",
    "vectorLength = matrixSize #Mandatory for well defined multiplication\n",
    "\n",
    "if matrixSize % size != 0:\n",
    "    if rank == 0:\n",
    "        print (\"WARNING: Number of matrix rows is not multiple of number of cores\")\n",
    "        comm.Abort(1)\n",
    "\n",
    "#Allocate and/or initialize/define data (rank dependent)\n",
    "#--------------------------------------------------------\n",
    "if rank == 0:\n",
    "    A = np.arange(matrixSize**2, dtype=np.float64).reshape(matrixSize, matrixSize)\n",
    "    x = np.arange(vectorLength, dtype=np.float64)\n",
    "    b = np.empty(vectorLength, dtype=np.float64)\n",
    "else:\n",
    "    A = None\n",
    "    x = np.empty(vectorLength, dtype=np.float64)\n",
    "    b = None\n",
    "\n",
    "# Scatter Matrix\n",
    "# --------------\n",
    "A_local = np.empty((matrixRowsPerRank, matrixSize), dtype=np.float64)\n",
    "comm.Scatter(A, A_local, root=0)\n",
    "\n",
    "# Broadcast vector\n",
    "# ----------------\n",
    "comm.Bcast(x, root=0)\n",
    "\n",
    "# Do parallel computation\n",
    "# ------------------------\n",
    "b_local = np.dot(A_local, x)\n",
    "\n",
    "# Gather local results\n",
    "# --------------------\n",
    "comm.Gather(b_local, b, root=0)\n",
    "\n",
    "# Unit Test (Possible since process 0 holds entire A in memory)\n",
    "# -------------------------------------------------------------\n",
    "if rank == 0:\n",
    "    print (\"Parallel computation equals sequential :\", np.allclose(np.dot(A, x), b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eab5957-6b7a-4166-9596-bd27f93973c2",
   "metadata": {},
   "source": [
    "### Any receive\n",
    "Using MPI.ANY_SOURCE it is possible to receive data from a unspecified rank \n",
    "\n",
    "Using MPI.ANY_SOURCE it is possible to receive data with a unspecified tag\n",
    "\n",
    "_Example_\n",
    "\n",
    "Rank 0 sends tagged data to rank 1, rank 1 is accepting any data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c730ae29-dee9-453b-a86c-177720f767fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    data = np.array([1.0, 2.0, 3.0], dtype=np.float64)\n",
    "    # Send the data to rank 1 with tag 10\n",
    "    comm.Send(data, dest=1, tag=10)\n",
    "    print(f\"[Rank 0] Sent data: {data} to Rank 1\")\n",
    "\n",
    "elif rank == 1:\n",
    "    # Prepare a NumPy array to receive the data\n",
    "    data = np.empty(3, dtype=np.float64)  # Same size as sender's array\n",
    "    #Status tells receiver that data is ready to be received\n",
    "    status = MPI.Status()\n",
    "    #Use ANY_SOURCE and ANY_TAG to receive from any rank/tag\n",
    "    comm.Recv(data, source=MPI.ANY_SOURCE, tag=MPI.ANY_TAG, status=status)\n",
    "    print(f\"[Rank 1] Received data: {data}\")\n",
    "    print(f\"[Rank 1] Message received from rank {status.Get_source()} with tag {status.Get_tag()}\")\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaefe22-3295-4ca8-a5f1-6b8c51139f7a",
   "metadata": {},
   "source": [
    "### <span style=\"color:Blue\">_Exercise_ </span> \n",
    "\n",
    "The first n-1 ranks send data (a numpy array) to the last ranks which receives all.\n",
    "\n",
    "   * Use a random tag to send\n",
    "   * The receiver rank (rank=size -1) only allocates one common buffer for all data\n",
    "   * The receiver rank prints data, sender rank and tag of the received data\n",
    "\n",
    "#### _Proposed solution_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b24473-fcf5-4227-adf3-7b899c9d3ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "if rank < size -1:\n",
    "    data = rank*np.ones(3, dtype=np.float64)\n",
    "    comm.Send(data, dest=size-1, tag=np.random.randint(size))\n",
    "    print(f\"[Rank {rank}] Sent data: {data} to Rank {size-1}\")\n",
    "else: #rank size -1\n",
    "    numberOfReceivedMessages = 0\n",
    "    data = np.empty(3, dtype=np.float64)  # Same buffer for all receives\n",
    "    while numberOfReceivedMessages < size -1:\n",
    "        status = MPI.Status()\n",
    "        comm.Recv(data, source=MPI.ANY_SOURCE, tag=MPI.ANY_TAG, status=status)\n",
    "        print(f\"[Rank {rank}] Received data: {data} from rank {status.Get_source()} with tag {status.Get_tag()}\")\n",
    "        numberOfReceivedMessages += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f279db40-98f9-4e5e-b91c-2741c9525498",
   "metadata": {},
   "source": [
    "## When to use MPI\n",
    "### When not to use MPI\n",
    "\n",
    "If you have a single program that runs sequentially on many (small) input files  -> Use SLURM Job-arrays\n",
    "\n",
    "### Use for embarrassingly parallel algorithms ? Maybe\n",
    "\n",
    "If you have large input-data (e.g. a huge detector image) that should finish asap but is embarrassingly parallel.\n",
    "\n",
    "What do we mean with embarrassingly parallel ?\n",
    "\n",
    "_Example_: \n",
    "\n",
    "Normalize a detector image such that all intensities are between (0,1)\n",
    "\n",
    "This calculation can be done independently for each pixel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73773794-dc07-4e70-ba6f-d593d1b155c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bitDepth = 8\n",
    "detectorImage = np.random.randint(2**bitDepth, size=(220, 200))\n",
    "detectorImageNormalized = detectorImage.astype(np.float64) / 2**bitDepth\n",
    "plt.imshow(detectorImage); plt.colorbar()\n",
    "plt.show()\n",
    "plt.imshow(detectorImageNormalized); plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c7b410-735a-4501-a62e-025b49f4566c",
   "metadata": {},
   "source": [
    "### <span style=\"color:Blue\">_Exercise_ </span> \n",
    "\n",
    "Parallelize the above normalization\n",
    "\n",
    "  * allocate the random image on rank 0\n",
    "  * only rank 0 needs to hold the complete result (the full normalize imaged) <BR>\n",
    "  * Use Scatter on Gather to distribute raw data and collect processed data (Reuse Matrix_Scatter code)\n",
    "  * Compare result with single process computation (done on rank 0 only)\n",
    "\n",
    "_Proposed solution_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7812ecea-36a6-446a-a09d-2946d0ae1cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "numberOfRows = 220\n",
    "numberOfColumns = 200\n",
    "bitDepth = 8\n",
    "\n",
    "if rank == 0:\n",
    "    detectorImage = np.random.randint(2**bitDepth, size=(numberOfRows,numberOfColumns)).astype(np.float64)\n",
    "else:\n",
    "    detectorImage = None\n",
    "\n",
    "numberOfRowsPerCore = numberOfRows//size\n",
    "\n",
    "detectorImageSlice = np.empty((numberOfRowsPerCore, numberOfColumns), dtype=np.float64)\n",
    "\n",
    "comm.Scatter(detectorImage, detectorImageSlice, root=0)\n",
    "\n",
    "detectorImageSliceNormalized = detectorImageSlice / 2**bitDepth\n",
    "\n",
    "if rank == 0:\n",
    "    detectorImageNormalized = np.empty((numberOfRows,numberOfColumns) , dtype=np.float64)\n",
    "else:\n",
    "    detectorImageNormalized = None\n",
    "\n",
    "comm.Gather(detectorImageSliceNormalized, detectorImageNormalized, root=0)\n",
    "\n",
    "if rank == 0:\n",
    "    print (\"Parallel computation equals sequential :\", np.allclose(detectorImage/2**bitDepth, detectorImageNormalized))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860c6b1b-c06d-4e11-9c1d-a12d5dec3892",
   "metadata": {},
   "source": [
    "### Use for non-embarrassingly parallel algorithms\n",
    "\n",
    "What does non-embarrassingly mean in the context of image analysis? <BR>\n",
    "Calculations can not be done fully independent for each pixel, neighbouring pixels are involved <BR>\n",
    "\n",
    "<img src=\"LaplaceStencil.png\" alt=\"Alt Text\" width=\"500\"/>\n",
    "\n",
    "As an example, we apply a Laplace filter to a random image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed23ee70-8b1c-4c81-a892-42d38e93ffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import laplace\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bitDepth = 8\n",
    "detectorImage = np.random.randint(2**bitDepth, size=(220, 200)).astype(np.float64)\n",
    "filteredImage = laplace(detectorImage, mode='constant', cval=0)\n",
    "plt.imshow(detectorImage); plt.colorbar()\n",
    "plt.show()\n",
    "plt.imshow(filteredImage); plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f666efde-6aba-4839-940c-dc66d2fcf2f8",
   "metadata": {},
   "source": [
    "### <span style=\"color:Blue\">_Exercise_ </span> \n",
    "\n",
    "Parallelize above code, first use Scatter + Gather as above. \n",
    "\n",
    "You only need to change a single line, i.e. the actual image processing, but maybe you also want to give better variable names\n",
    "\n",
    "Does the unit test pass? Why not? Plot the difference (sequential result - parallel result) to get a hint.\n",
    "\n",
    "_Proposed (Anti-)Solution_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f091215d-5d70-4d9b-a981-98c2a6eae68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "from scipy.ndimage import laplace\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "numberOfRows = 220\n",
    "numberOfColumns = 200\n",
    "\n",
    "bitDepth = 8\n",
    "if rank == 0:\n",
    "    detectorImage = np.random.randint(2**bitDepth, size=(numberOfRows,numberOfColumns)).astype(np.float64)\n",
    "else:\n",
    "    detectorImage = None\n",
    "\n",
    "numberOfRowsPerCore = numberOfRows//size\n",
    "\n",
    "detectorImageSlice = np.empty((numberOfRowsPerCore, numberOfColumns), dtype=np.float64)\n",
    "\n",
    "comm.Scatter(detectorImage, detectorImageSlice, root=0)\n",
    "\n",
    "#This is the only line that needs to be changed\n",
    "detectorImageSliceFiltered = laplace(detectorImageSlice, mode='constant', cval=0)\n",
    "\n",
    "if rank == 0:\n",
    "    detectorImageFiltered = np.empty((numberOfRows,numberOfColumns) , dtype=np.float64)\n",
    "else:\n",
    "    detectorImageFiltered = None\n",
    "\n",
    "comm.Gather(detectorImageSliceFiltered, detectorImageFiltered, root=0)\n",
    "\n",
    "if rank == 0:\n",
    "    print (\"Parallel computation equals sequential :\", np.allclose(laplace(detectorImage, mode='constant', cval=0), detectorImageFiltered))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b8c450-dad2-4c39-b854-19255d63ce46",
   "metadata": {},
   "source": [
    "### <span style=\"color:Blue\">_Exercise_ </span> : \n",
    "\n",
    "Make the above code work\n",
    "\n",
    "   * Assume 2 ranks only\n",
    "   * Both ranks are allowed to allocate initial 2d array (inefficient for a real application)\n",
    "   * The 2d array is split in a upper and lower part, as above, one for each rank\n",
    "   * Rank 1 sends his (partial) filter result to rank 0 (point to point, no Gather/Scatter)\n",
    "   * After having received image stripe from rank 1, rank 0 does the unit test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8161794c-018e-431a-b55d-68f528e9e188",
   "metadata": {},
   "source": [
    "### The 2D Heat equation\n",
    "\n",
    "The 2D heat equation reads\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial u}{\\partial t} = \\alpha \\Delta u ;\n",
    "\\quad  \\quad\n",
    "\\Delta u = \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2u}{\\partial y^2};\n",
    "\\quad \\quad\n",
    "u = u(x,y,t)\n",
    "\\end{equation}\n",
    "\n",
    "We integrate this equation according a simple Euler schema to get the time evolution\n",
    "\n",
    "\\begin{equation}\n",
    "u(x,y,k+1) = u(x,y,k) + dt \\cdot \\alpha \\cdot \\Delta u\n",
    "\\end{equation}\n",
    "where $u(x,y,k)$ is the 2D heat distribution at discretized time $k$ and $dt$ labels a time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b4f7ec-9e32-4b54-b40c-cc809b07a8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import laplace\n",
    "\n",
    "numberOfGridPoints = 50\n",
    "dt = 0.1\n",
    "alpha = 1\n",
    "numberOfIterations = 100000\n",
    "u = np.zeros((numberOfGridPoints, numberOfGridPoints))\n",
    "#initial condition (heat peak in center)\n",
    "u[numberOfGridPoints//2, numberOfGridPoints//2] = 100\n",
    "#boundary condition (Dirichlet boundary condition)\n",
    "b = 1.0\n",
    "\n",
    "def update(u, alpha, dt, b):\n",
    "    return u + dt*alpha*laplace(u, mode='constant', cval=b)\n",
    "\n",
    "for _ in range(numberOfIterations):\n",
    "    u = update(u, alpha, dt, b)\n",
    "'''\n",
    "#Plotting only\n",
    "import pylab as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "def animate(i):\n",
    "    global u\n",
    "    u = update(u, alpha, dt, b)\n",
    "    ax.imshow(u)\n",
    "    ax.set_axis_off()\n",
    "\n",
    "anim = FuncAnimation(fig, animate, frames=numberOfIterations, interval=10)\n",
    "\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2b0986-44b8-4931-9015-18bbaf856898",
   "metadata": {},
   "source": [
    "### <span style=\"color:Blue\">_Exercise_ </span> : \n",
    "\n",
    "Parallelize the above solver\n",
    "   * Use the same structure as above (for the 2D filtering)\n",
    "   * You must exchange boundary (HALO) between ranks after every iteration step\n",
    "\n",
    "\n",
    "Ilustration of the HALO concept:\n",
    "\n",
    "\n",
    "<img src=\"halo.svg\" alt=\"Alt Text\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee408ad8-5a55-4d17-8395-613f7be6f2fb",
   "metadata": {},
   "source": [
    "_Proposed Solution_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ffaf8b-1abf-4f67-9eaf-8cf41ec9ec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "from scipy.ndimage import laplace\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "assert size == 2\n",
    "\n",
    "nr = 220\n",
    "nc = 200\n",
    "assert nr % 2 == 0\n",
    "nh = nr//2\n",
    "\n",
    "dt = 0.1\n",
    "alpha = 1\n",
    "bound = 1\n",
    "numberOfIterations = 100\n",
    "\n",
    "doExchangeHalo = True #Set to False to make UT fail\n",
    "\n",
    "def update(u):\n",
    "    return u + dt*alpha*laplace(u, mode='constant', cval=bound)\n",
    "\n",
    "u_0 = np.arange(nr*nc, dtype = np.float64).reshape(nr,nc)\n",
    "\n",
    "if rank == 0:\n",
    "    u_upper = u_0[:nh +1, :]\n",
    "else:\n",
    "    u_lower = u_0[nh -1:, :]\n",
    "\n",
    "for _ in range(numberOfIterations):\n",
    "    if rank == 0:\n",
    "        u_upper = update(u_upper)\n",
    "        if doExchangeHalo:\n",
    "            comm.Send(u_upper[-2,:], dest=1, tag=12)\n",
    "            comm.Recv(u_upper[-1,:], source=1, tag=13)\n",
    "    else:\n",
    "        u_lower = update(u_lower)\n",
    "        if doExchangeHalo:\n",
    "            comm.Recv(u_lower[0,:], source=0, tag=12)\n",
    "            comm.Send(u_lower[1,:], dest=0, tag=13)\n",
    "\n",
    "if rank == 0:\n",
    "    u_f_total = np.zeros_like(u_0)\n",
    "    u_f_total[:nh, :] = u_upper[:-1, :]\n",
    "    comm.Recv(u_f_total[nh:,:], source=1, tag=14)\n",
    "else: #<- only since we know that we are running on 2 cores\n",
    "    comm.Send(u_lower[1:,:], dest=0, tag=14)\n",
    "\n",
    "\n",
    "if rank == 0:\n",
    "    u = u_0\n",
    "    for _ in range(numberOfIterations):\n",
    "        u = update(u)\n",
    "    u_f_total_seq = u\n",
    "    print (\"Parallel integration equals sequential :\", np.allclose(u_f_total, u_f_total_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635e2994-b575-46cf-b39a-ed94c0dc9a50",
   "metadata": {},
   "source": [
    "## Unit tests\n",
    "\n",
    "If possible, always compare the result of the parallelized algorithm with the result of the sequential algorithm.\n",
    "\n",
    "(Use a dummy example with small input data and low computational burden, e.g a redcued number of iteration steps)\n",
    "\n",
    "If the unit test fails this does not necessarily mean that there is a bug in the parallel code.\n",
    "\n",
    "_Example_ (Machine Learning)\n",
    "\n",
    "We use some (synthetic) data to train a (random forest) classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92700f86-176a-4919-b5b0-41d63e5a68f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate or load your dataset (here, a synthetic dataset)\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a local model\n",
    "model = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "print (y_pred)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819932a5-b540-4b97-82fa-9633be1d164c",
   "metadata": {},
   "source": [
    "### <span style=\"color:Blue\">_Exercise_ </span> \n",
    "\n",
    "Parallelize above code\n",
    "\n",
    "  * Every rank reads its portion of the syntethic overall data (e.g. in a module fashion)\n",
    "  * Every rank trains its own model according 'rank-local' data (embarassingly parallel)\n",
    "  * Every rank makes its rank-local prediction (On the full test set)\n",
    "  * Rank 0 gathers all local predictions and does a majority vote on the overall predictions\n",
    "  * Hint You can use a one-liner: `all_preds = comm.gather(y_pred_local, root=0)`\n",
    "    This returns a list (list index equals rank ID)\n",
    "    \n",
    "\n",
    "Is the Accuracy the same as for the sequential algorithm ? (Is the splitting of the model 'linear'?)\n",
    "\n",
    "_Proposed Solution_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127c596f-8569-4523-9f1a-6450f11e8f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialize MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "# Generate or load your dataset (a synthetic dataset). Every process holds all data which is\n",
    "# clearly inefficient, in a productive application, each rank would reads its share from e.g a hdf5 file\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split training data across processes (in a modulo fashion)\n",
    "X_train_local = X_train[rank::size]\n",
    "y_train_local = y_train[rank::size]\n",
    "\n",
    "# Train a local model (On local test set)\n",
    "model = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "model.fit(X_train_local, y_train_local)\n",
    "\n",
    "# Make local predictions (On full test set)\n",
    "y_pred_local = model.predict(X_test)\n",
    "\n",
    "# Gather predictions from all processes to rank 0\n",
    "# This syntax gathers a list (index = rank). (For gathering non-equal size arrays)\n",
    "all_preds = comm.gather(y_pred_local, root=0)\n",
    "\n",
    "if rank == 0:\n",
    "    # Stack predictions and compute ensemble prediction\n",
    "    all_preds = np.vstack(all_preds)\n",
    "    # For classification: majority vote\n",
    "    y_pred_ensemble = np.round(np.mean(all_preds, axis=0)).astype(int)\n",
    "    print (y_pred_ensemble)\n",
    "    # For regression: average\n",
    "    #y_pred_ensemble = np.mean(all_preds, axis=0)\n",
    "    # Evaluate ensemble performance\n",
    "    accuracy = np.mean(y_pred_ensemble == y_test)\n",
    "    print(f\"Ensemble accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5f66f1-ed6d-440f-9810-76c28796d0a2",
   "metadata": {},
   "source": [
    "## Integration with Slurm\n",
    "So far all examples were launched from command line and executed on login node.\n",
    "\n",
    "Real world jobs (HPC use case with large memory consumption and and intense number crunching) must be submitted to the cluster.\n",
    "\n",
    "_Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9220d1fc-120c-4359-88c4-2a1bde4ab233",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks-per-node=4\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH -J ringComm\n",
    "#SBATCH -o ringComm.out\n",
    "#SBATCH -e ringComm.err\n",
    "export PATH=/psi/home/studer_a1/data/condaforge/miniforge/bin:$PATH\n",
    "source activate mpi\n",
    "#It is not needed to specify number of processes here, done by Slurm\n",
    "mpiexec python ringComm.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3037269-b729-4cb0-a882-6e57b8c019fd",
   "metadata": {},
   "source": [
    "The ring buffer example highlights again the blocking/non-blocking spects of the different send/receive modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e925ad2-d40c-4b99-8529-8e89745a8709",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This python program sends data (a numpy array) in a ring topology using mpi4py.\n",
    "The main purpose is to demonstrate the effect of different send/receive modes,\n",
    "especially blocking versus non-blocking. We need at least two ranks, i.e.\n",
    "$ mpiexec -n 2 python ringComm.py\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "sendFunction = comm.Send #default send, blocking (returns after application buffer can be reused[*])\n",
    "recvFunction = comm.Recv #default recv, blocking (returns after application buffer is filled)\n",
    "#sendFunction = comm.Ssend #Synchronous send (returns after connection with reciever), deadlock [**]\n",
    "#sendFunction = comm.Isend #Non-blocking send, recommended to avoid deadlock\n",
    "#recvFunction = comm.Irecv #Non-blocking recv (not needed, if used, add a MPI.wait to avoid complaint,\n",
    "#like: req = comm.Irecv(...), data2beRecv = req.wait() )\n",
    "\n",
    "\n",
    "arraySize = 1\n",
    "\n",
    "#Create numpy array with data unique to this rank/Process\n",
    "data2beSent = np.ones(arraySize, dtype='i')*rank\n",
    "\n",
    "#Allocate buffer for data to be received, should match with what is sent\n",
    "data2beRecv = np.empty(arraySize, dtype='i')\n",
    "\n",
    "#Define source and destination for this rank (nearest neighbours)\n",
    "leftNeighbor = (rank -1 + size) % size\n",
    "rightNeighbor = (rank + 1) % size\n",
    "\n",
    "#Send\n",
    "sendFunction([data2beSent, MPI.INT], dest=rightNeighbor, tag=0)\n",
    "#Recv\n",
    "recvFunction([data2beRecv, MPI.INT], source=leftNeighbor, tag=0)\n",
    "\n",
    "#Send-And-Recv (Should always work, communication patterns hidden, so not suited for demonstration purpose)\n",
    "#comm.Sendrecv(sendbuf=data2beSent, dest=rightNeighbor, sendtag=0,\n",
    "#              recvbuf=data2beRecv, source=leftNeighbor,recvtag=0)\n",
    "\n",
    "#Check if algorithm output matches our expectations\n",
    "print(f\"Process {rank} received data {data2beRecv} from process {leftNeighbor}\")\n",
    "\n",
    "'''\n",
    "[*] Might deadlock for large numpy arrays (if MPI library decides that reallocating data\n",
    "in library space is inefficient (i.e. copying data from application space to library space -> memory is doubled)\n",
    "MPI might do a comm.Ssend under the hood, then the receiver must be ready at the time of sending.\n",
    "[**]If you hear the fan kicking in: this indicates 'busy-waiting' (worst case scenario)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaa9c8b-676c-4566-9d6d-f6dd0652aa85",
   "metadata": {},
   "source": [
    "## Outlook: Running Jobs on a GPU cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfe8c7b-b2b6-4313-93d5-3674641ed5e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
