{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a942da2b-436d-4bb1-ba95-5e9d30d895b1",
   "metadata": {},
   "source": [
    "# MPI4py Hands On"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5863e07-aea4-47eb-b5be-d76443eed5cb",
   "metadata": {},
   "source": [
    "## Hello MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f041ddc5-7e0f-4ceb-8eae-ad37876fccd7",
   "metadata": {},
   "source": [
    "General Note: The following examples are all small (low computational burden). Hence we are allowed to run on login nodes.\n",
    "\n",
    "As a initial test, try to run the following python script (named helloMPI.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03925851-3b8f-4115-af80-5aae3f23bb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "# Initialize MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "# Print a message from each process\n",
    "print(f\"Hello from process {rank} of {size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3605b5f-d4d0-4814-b63d-98a40e7d9b52",
   "metadata": {},
   "source": [
    "On a terminal, run the program as:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f80c220d-5099-42d0-affa-6bc73e3068a9",
   "metadata": {},
   "source": [
    "$ mpirun -n 2 python helloMPI.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0acb07-9120-428c-a58a-b0cb5e194e3c",
   "metadata": {},
   "source": [
    "If everything is installed properly, you should see as output: (If not, make sure you loaded the proper conda env in your terminal)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4f8e78ed-65c0-41dd-8e3b-610bc9744e16",
   "metadata": {},
   "source": [
    "Hello from process 0 of 2\n",
    "Hello from process 1 of 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903ede82-292c-44fe-8d4c-5e4979623910",
   "metadata": {},
   "source": [
    "So here we started two python processes, each process printing simply its ID (rank) and total number of processes started. No IPC so far"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef43155-1097-426b-b833-d7c617d41c97",
   "metadata": {},
   "source": [
    "## Basic Communication Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36c6504-7ed2-4a74-98d1-41fdbc641a91",
   "metadata": {},
   "source": [
    "General Note: We will only use numpy arrays to do Inter-Process-Communication. This is the most common use-case in HPC and DataScience. But you can in principle exchange any python data between processes. (Less efficient than numpy arrays, since data needs to be pickled/serialized under the hood, whereas the numpy array can be treated like a C-pointer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6d8237-490f-427c-a2c7-1a1c0bba702b",
   "metadata": {},
   "source": [
    "### Point-to-Point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5bb576-b235-496a-ba4f-893fea93942b",
   "metadata": {},
   "source": [
    "We send a 1D numpy array (10 items, datatype float64) from process 0 to process 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316ac629-8d44-4683-989c-e895cd9dec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "# automatic MPI datatype discovery\n",
    "if rank == 0:\n",
    "    data = np.arange(10, dtype=np.float64)\n",
    "    comm.Send(data, dest=1, tag=13)\n",
    "    print (\"0 sent:\", data)\n",
    "elif rank == 1:\n",
    "    data = np.empty(10, dtype=np.float64)\n",
    "    comm.Recv(data, source=0, tag=13)\n",
    "    print (\"1 recv:\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf145193-d05d-47c4-b7cb-ba3b7cb367b2",
   "metadata": {},
   "source": [
    "Run with two processes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bc57148e-44fa-43a4-8d80-5c0f0983b18b",
   "metadata": {},
   "source": [
    "$ mpirun -n 2 python Point2Point_1d.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f475c9a-4f39-43f2-a5e4-39fec6913eee",
   "metadata": {},
   "source": [
    "#### _Exercise_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ef3f19-55a0-496c-9dbf-b1e3cbfac323",
   "metadata": {},
   "source": [
    "Send a 2d array (size 3x3, datatype int) from rank 0 to rank 1 (In productive applications, these would typically be detector images)\n",
    "\n",
    "_Hint_: You can treat metadata (i.e. image size) as hard coded (global)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503a2850-65e7-4ec9-86c7-50eaf52dcaa5",
   "metadata": {},
   "source": [
    "_Proposed solution_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59daa023-91bf-4c06-ae9f-5659494a7041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "imageSize = 3\n",
    "\n",
    "if rank == 0:\n",
    "    #We emulate a (small) detector image, typically rank 0 would load it from file\n",
    "    detImage_2BeSent = np.arange(imageSize**2, dtype=np.int64).reshape(imageSize,imageSize)\n",
    "    comm.Send(detImage_2BeSent, dest=1, tag=13)\n",
    "    print (\"0 sent:\\n\", detImage_2BeSent)\n",
    "elif rank == 1:\n",
    "    data = np.empty(imageSize**2, dtype=np.int64)\n",
    "    comm.Recv(data, source=0, tag=13)\n",
    "    detImage_Recv = data.reshape(imageSize,imageSize)\n",
    "    print (\"1 recvd:\\n\", detImage_Recv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350feae1-7747-4df9-92b6-174fcb4dd0d8",
   "metadata": {},
   "source": [
    "#### _Exercise_"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6b5fdc64-1d74-4142-a06b-37bfd0e33edb",
   "metadata": {},
   "source": [
    "Now we try to improve the code in the sense that we make it more generic. \n",
    "Assume the receiver does not know the metadata (i.e. the image size)\n",
    "Hence the sender must as well send the metadata (besides data)\n",
    "_Hint_: Use MPI-tags to distinguish metadata and data\n",
    "\n",
    "PseudoCode:\n",
    "\n",
    "dataTag=..\n",
    "metaDataTag=..\n",
    "\n",
    "if rank == 0:\n",
    "  emulateData\n",
    "  sendData\n",
    "  sendMetaData\n",
    "if rank == 1:\n",
    "  receive Metadata\n",
    "  receive Data \n",
    "  Reshape received Data according received metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9406f756-7586-462e-ac9b-4536dde4cfbd",
   "metadata": {},
   "source": [
    "_Proposed solution_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb1b333-1d1c-40d3-a857-24da617e1b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "dataTag = 13\n",
    "metaDataTag = 14\n",
    "\n",
    "if rank == 0:\n",
    "    #We emulate a (small) detector image, typically rank 0 would load it from file\n",
    "    #Hence only rank 0 knows its size (might be varying). Datatype is assumed fixed\n",
    "    imageSize = 3 #this information would e.g be derived from hdf5 metadata of image directly\n",
    "    #imageSize = 50 #Deadlocks, but this is MPI-implementation dependent\n",
    "    detImage_2BeSent = np.arange(imageSize**2, dtype=np.int64).reshape(imageSize,imageSize)\n",
    "    #Send data first\n",
    "    comm.Send(detImage_2BeSent, dest=1, tag=dataTag)\n",
    "    print (\"0 sent:\\n\", detImage_2BeSent)\n",
    "    #Send Meta data (we could also send an int here, but for a non quadratic image would be an array)\n",
    "    comm.Send(np.array(imageSize, dtype=np.int64), dest=1, tag=metaDataTag)\n",
    "\n",
    "elif rank == 1:\n",
    "    #Receive Metadata first (We must know how large the buffer is that we allocate\n",
    "    imageSize_asArray = np.empty(1, dtype=np.int64)\n",
    "    comm.Recv(imageSize_asArray, source=0, tag=metaDataTag)\n",
    "    imageSize = imageSize_asArray[0]\n",
    "    #Now that we know what to expect we can allocate receive buffer\n",
    "    data = np.empty(imageSize**2, dtype=np.int64)\n",
    "    comm.Recv(data, source=0, tag=dataTag)\n",
    "    detImage_Recv = data.reshape(imageSize, imageSize)\n",
    "    print (\"1 recvd:\\n\", detImage_Recv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909dda05-0435-4bdd-94e3-571d09c74d35",
   "metadata": {},
   "source": [
    "_Question_: Why is this code working? The `Send` and `Recv` are blocking, but the sequence of `Send`s does not match the Sequence of `Recv`s (order is reversed!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76f82d0-8741-4b3e-a04e-9baf8f0466e9",
   "metadata": {},
   "source": [
    "## Blocking versus Non-Blocking Communication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77c1fe4-02c9-4c61-a48d-55f39a3a5131",
   "metadata": {},
   "source": [
    "### What means blocking? Why Deadlocks depend on data size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a50d030-408e-48ec-a3cd-62ad93314835",
   "metadata": {},
   "source": [
    "When you use blocking sends (MPI.Send) and blocking receives (MPI.Recv) in MPI, the order of operations matters, especially for large data. \n",
    "\n",
    "If the order of MPI.Recv calls on one process does not match the order of MPI.Send calls on the other, a deadlock can occur. Hereâ€™s why:\n",
    "How Blocking Communication Works\n",
    "\n",
    "    Blocking Send (MPI.Send): The call does not return until the message buffer can be reused (i.e., the message is either copied out or the matching receive is ready).\n",
    "    Blocking Receive (MPI.Recv): The call does not return until the message is received.\n",
    "\n",
    "Why Small Data Works\n",
    "\n",
    "For small messages, MPI implementations often use eager protocol:\n",
    "\n",
    "    The data is copied into a temporary buffer and sent immediately.\n",
    "    The send call returns as soon as the data is buffered, even if the receive is not yet posted.\n",
    "    This can hide mismatched order for small messages.\n",
    "\n",
    "Why Large Data Fails\n",
    "\n",
    "For large messages, MPI typically uses rendezvous protocol:\n",
    "\n",
    "    The send call waits until the matching receive is posted before transferring data.\n",
    "    If the receive order is wrong, both processes can end up waiting for each other, causing a deadlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80ae06e-0f32-45e0-b17c-c682fab89108",
   "metadata": {},
   "source": [
    "### Making blocking/non-blocking more explicit explicit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423347f3-f8bd-4be6-89f6-8f80913b7228",
   "metadata": {},
   "source": [
    "Take away from above and motivation:\n",
    "\n",
    "Blocking Send (MPI.Send): The call does not return until the  <span style=\"color:blue\">send message buffer can be reused</span>.\n",
    "\n",
    "Note that nothing is said about the state of the receiver. \n",
    "This is confusing or at least vague\n",
    "\n",
    "And: <span style=\"color:red\">This is also mean</span>: If you test your MPI-application with a tiny dummy dataset on the login node, all works well,\n",
    "if you then submit your large real-world job to the cluster, it deadlocks)\n",
    "\n",
    "But this is how it is defined in the MPI standard. (And the standard is always and by defintion correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3570e25d-30ae-4155-86ff-8f8417a4ab99",
   "metadata": {},
   "source": [
    "#### Synchronous Send and Receive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a12573-c9a5-4b5b-b99b-8325a3e669a6",
   "metadata": {},
   "source": [
    "You can use `comm.Ssend` to enforce synchronous send. `comm.Ssend` waits for a hand shake with the corresponding receiver, only afterwards it returns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a22ba6a-353d-47e8-98a4-9305b61ccfc5",
   "metadata": {},
   "source": [
    "#### _Exercise_\n",
    "Edit the above script and replace a single `comm.Send` by a `comm.Ssend` such that the application deadlocks for sure (also for small datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c043c5df-773b-433c-b561-18ec4d7f63b4",
   "metadata": {},
   "source": [
    "#### Aynchronous Send and Receive\n",
    "You can use `comm.Isend` to enforce asynchronous (or non-blocking) send\n",
    "`comm.Isend` should be used tother with `comm.Irecv`, the non-blocking version of `comm.Recv`\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a674e5-1be9-437f-9911-aace53132020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    # Non-blocking send\n",
    "    data = np.asarray([1, 2, 3])\n",
    "    req_send = comm.Isend(data, dest=1, tag=77)\n",
    "    # Do some computation while the send is in progress\n",
    "    result = 42**2\n",
    "    # Wait for the send to complete\n",
    "    req_send.Wait()\n",
    "    print (\"Sent:\", data)\n",
    "elif rank == 1:\n",
    "    #Allocate memory for the receive buffer\n",
    "    data = np.empty(3, np.int64)\n",
    "    # Non-blocking receive\n",
    "    req_recv = comm.Irecv(data, source=0, tag=77)\n",
    "    # Do some computation while the receive is in progress\n",
    "    result = 100/26\n",
    "    # Wait for the receive to complete\n",
    "    req_recv.Wait()\n",
    "    print(\"Received:\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ad0ba0-39c1-4fb2-b971-96f19ab86246",
   "metadata": {},
   "source": [
    "The above example also highlights that non-blocking calls can be used to interleave communication with calculation.\n",
    "\n",
    "This might give a performance increase as additional benefit. But this is not in the focus here. (The focus is on correct code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cec73c-650b-4403-9ae6-344eeb2941a3",
   "metadata": {},
   "source": [
    "#### _Exercise_\n",
    "\n",
    "Modify the above script such that it never deadlocks, no matter what the sequence of sends/receives is and no matter how big data is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df90c1a-7cfc-4512-b8e9-1f1049ea51bf",
   "metadata": {},
   "source": [
    "#### _Proposed Solution_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bb6361-b025-4ce2-892b-39ee0983897d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae09279-8579-4c93-a98d-2c27d0325bca",
   "metadata": {},
   "source": [
    "## Basic Communication Patterns Continued"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c145603e-96a2-4118-a8ff-d0ef48b39281",
   "metadata": {},
   "source": [
    "### Broadcast\n",
    "As the name suggest, comm.BroadCast is used to fant out data from one rank to all the others.\n",
    "\n",
    "The broadcasting rank is typially rank zero, whereas all other ranks > 0 recieve data\n",
    "\n",
    "Example (taken from official documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d39e9f-a4b4-4946-8235-218d50b7dd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "#Rank 0 allocates and initializes/define data to be broadcast\n",
    "if rank == 0:\n",
    "    data = np.arange(100, dtype='i')\n",
    "#All other ranks allocate receive buffer\n",
    "else:\n",
    "    data = np.empty(100, dtype='i')\n",
    "#Note 'collective' operation (*every* rank calls Bcast, not just rank 0) \n",
    "comm.Bcast(data, root=0)\n",
    "#Check for all ranks that they have received the Broadcaster's data\n",
    "for i in range(100):\n",
    "    assert data[i] == i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710a58bf-62ba-4bb9-91d0-112f87b791a6",
   "metadata": {},
   "source": [
    "### Scatter and Gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f8dd6a-0847-4aaa-b50d-fffcbbfe4e36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e61dceb8-623f-4e73-a2f7-faef39ad3155",
   "metadata": {},
   "source": [
    "#### _Exercise_\n",
    "\n",
    "Implement parrallel matrix multiplication Ax for a (16 x 7) matrix A and a (7 x 1) vector x using broadcast scatter and gather.\n",
    "The code should work for two cores only.\n",
    "\n",
    "At the end, rank 0 (who collects the results) makes sure that the parallel version is correct (by doing the calculation itself)\n",
    "\n",
    "#### _Proposed solution_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3552ca8d-74d6-4097-9cf0-9c65d3454265",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eab5957-6b7a-4166-9596-bd27f93973c2",
   "metadata": {},
   "source": [
    "### Anonymous receive\n",
    "MPI.any..\n",
    "Exercise: The first n-1 rank send data the the last ranks which recieives all (while number of received messages < n-1..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5f66f1-ed6d-440f-9810-76c28796d0a2",
   "metadata": {},
   "source": [
    "## Integration with Slurm\n",
    "So far all examples were launched from command line and executed on login node.\n",
    "\n",
    "Real world jobs (HPC use case with large memory consumption and and intense number crunching) must be submitted to the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9220d1fc-120c-4359-88c4-2a1bde4ab233",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:conda-datascience_py36]",
   "language": "python",
   "name": "conda-env-conda-datascience_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
