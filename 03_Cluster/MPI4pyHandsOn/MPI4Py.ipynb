{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a942da2b-436d-4bb1-ba95-5e9d30d895b1",
   "metadata": {},
   "source": [
    "# MPI4py Hands On"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5863e07-aea4-47eb-b5be-d76443eed5cb",
   "metadata": {},
   "source": [
    "## Hello MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f041ddc5-7e0f-4ceb-8eae-ad37876fccd7",
   "metadata": {},
   "source": [
    "General Note: The following examples are all small (low computational burden). Hence we are allowed to run on login nodes.\n",
    "\n",
    "As a initial test, try to run the following python script (named helloMPI.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03925851-3b8f-4115-af80-5aae3f23bb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "# Initialize MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "# Print a message from each process\n",
    "print(f\"Hello from process {rank} of {size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3605b5f-d4d0-4814-b63d-98a40e7d9b52",
   "metadata": {},
   "source": [
    "On a terminal, run the program as:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f80c220d-5099-42d0-affa-6bc73e3068a9",
   "metadata": {},
   "source": [
    "$ mpirun -n 2 python helloMPI.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0acb07-9120-428c-a58a-b0cb5e194e3c",
   "metadata": {},
   "source": [
    "If everything is installed properly, you should see as output: (If not, make sure you loaded the proper conda env in your terminal)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4f8e78ed-65c0-41dd-8e3b-610bc9744e16",
   "metadata": {},
   "source": [
    "Hello from process 0 of 2\n",
    "Hello from process 1 of 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903ede82-292c-44fe-8d4c-5e4979623910",
   "metadata": {},
   "source": [
    "So here we started two python processes, each process printing simply its ID (rank) and total number of processes started. No IPC so far"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef43155-1097-426b-b833-d7c617d41c97",
   "metadata": {},
   "source": [
    "## Basic Communication Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36c6504-7ed2-4a74-98d1-41fdbc641a91",
   "metadata": {},
   "source": [
    "General Note: We will only use numpy arrays to do Inter-Process-Communication. This is the most common use-case in HPC and DataScience. But you can in principle exchange any python data between processes. (Less efficient than numpy arrays, since data needs to be pickled/serialized under the hood, whereas the numpy array can be treated like a C-pointer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6d8237-490f-427c-a2c7-1a1c0bba702b",
   "metadata": {},
   "source": [
    "### Point-to-Point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5bb576-b235-496a-ba4f-893fea93942b",
   "metadata": {},
   "source": [
    "We send a 1D numpy array (10 items, datatype float64) from process 0 to process 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316ac629-8d44-4683-989c-e895cd9dec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "# automatic MPI datatype discovery\n",
    "if rank == 0:\n",
    "    data = np.arange(10, dtype=np.float64)\n",
    "    comm.Send(data, dest=1, tag=13)\n",
    "    print (\"0 sent:\", data)\n",
    "elif rank == 1:\n",
    "    data = np.empty(10, dtype=np.float64)\n",
    "    comm.Recv(data, source=0, tag=13)\n",
    "    print (\"1 recv:\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf145193-d05d-47c4-b7cb-ba3b7cb367b2",
   "metadata": {},
   "source": [
    "Run with two processes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bc57148e-44fa-43a4-8d80-5c0f0983b18b",
   "metadata": {},
   "source": [
    "$ mpirun -n 2 python Point2Point_1d.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f475c9a-4f39-43f2-a5e4-39fec6913eee",
   "metadata": {},
   "source": [
    "#### _Exercise_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ef3f19-55a0-496c-9dbf-b1e3cbfac323",
   "metadata": {},
   "source": [
    "Send a 2d array (size 3x3, datatype int) from rank 0 to rank 1 (In productive applications, these would typically be detector images)\n",
    "\n",
    "_Hint_: You can treat metadata (i.e. image size) as hard coded (global)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503a2850-65e7-4ec9-86c7-50eaf52dcaa5",
   "metadata": {},
   "source": [
    "_Proposed solution_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59daa023-91bf-4c06-ae9f-5659494a7041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "imageSize = 3\n",
    "\n",
    "if rank == 0:\n",
    "    #We emulate a (small) detector image, typically rank 0 would load it from file\n",
    "    detImage_2BeSent = np.arange(imageSize**2, dtype=np.int64).reshape(imageSize,imageSize)\n",
    "    comm.Send(detImage_2BeSent, dest=1, tag=13)\n",
    "    print (\"0 sent:\\n\", detImage_2BeSent)\n",
    "elif rank == 1:\n",
    "    data = np.empty(imageSize**2, dtype=np.int64)\n",
    "    comm.Recv(data, source=0, tag=13)\n",
    "    detImage_Recv = data.reshape(imageSize,imageSize)\n",
    "    print (\"1 recvd:\\n\", detImage_Recv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350feae1-7747-4df9-92b6-174fcb4dd0d8",
   "metadata": {},
   "source": [
    "#### _Exercise_"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6b5fdc64-1d74-4142-a06b-37bfd0e33edb",
   "metadata": {},
   "source": [
    "Now we try to improve the code in the sense that we make it more generic. \n",
    "Assume the receiver does not know the metadata (i.e. the image size)\n",
    "Hence the sender must as well send the metadata (besides data)\n",
    "_Hint_: Use MPI-tags to distinguish metadata and data\n",
    "\n",
    "PseudoCode:\n",
    "\n",
    "dataTag=..\n",
    "metaDataTag=..\n",
    "\n",
    "if rank == 0:\n",
    "  emulateData\n",
    "  sendMetaData\n",
    "  sendData\n",
    "if rank == 1:\n",
    "  receive Metadata\n",
    "  receive Data \n",
    "  Reshape received Data according received metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9406f756-7586-462e-ac9b-4536dde4cfbd",
   "metadata": {},
   "source": [
    "_Proposed solution_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb1b333-1d1c-40d3-a857-24da617e1b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "dataTag = 13\n",
    "metaDataTag = 14\n",
    "\n",
    "if rank == 0:\n",
    "    #We emulate a (small) detector image, typically rank 0 would load it from file\n",
    "    #Hence only rank 0 knows its size (might be varying). Datatype is assumed fixed\n",
    "    imageSize = 3 #this information would e.g be derived from hdf5 metadata of image directly\n",
    "    #imageSize = 50 #Deadlocks, but this is MPI-implementation dependent\n",
    "    #Send Meta data (we could also send an int here, but for a non quadratic image would be an array)\n",
    "    comm.Send(np.array(imageSize, dtype=np.int64), dest=1, tag=metaDataTag)\n",
    "    detImage_2BeSent = np.arange(imageSize**2, dtype=np.int64).reshape(imageSize,imageSize)\n",
    "    #Send data\n",
    "    comm.Send(detImage_2BeSent, dest=1, tag=dataTag)\n",
    "    print (\"0 sent:\\n\", detImage_2BeSent)\n",
    "\n",
    "elif rank == 1:\n",
    "    #Receive Metadata first (We must know how large the buffer is that we allocate\n",
    "    imageSize_asArray = np.empty(1, dtype=np.int64)\n",
    "    comm.Recv(imageSize_asArray, source=0, tag=metaDataTag)\n",
    "    imageSize = imageSize_asArray[0]\n",
    "    #Now that we know what to expect we can allocate receive buffer\n",
    "    data = np.empty(imageSize**2, dtype=np.int64)\n",
    "    comm.Recv(data, source=0, tag=dataTag)\n",
    "    detImage_Recv = data.reshape(imageSize, imageSize)\n",
    "    print (\"1 recvd:\\n\", detImage_Recv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909dda05-0435-4bdd-94e3-571d09c74d35",
   "metadata": {},
   "source": [
    "_Exercode_ No make the sender rank send the data first, such that it is out of order with the receiver.\n",
    "\n",
    "_Question_: Why is this code working? \n",
    "\n",
    "The `Send` and `Recv` are blocking, but the sequence of `Send`s does not match the Sequence of `Recv`s (order is reversed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76f82d0-8741-4b3e-a04e-9baf8f0466e9",
   "metadata": {},
   "source": [
    "## Blocking versus Non-Blocking Communication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77c1fe4-02c9-4c61-a48d-55f39a3a5131",
   "metadata": {},
   "source": [
    "### What means blocking? Why Deadlocks depend on data size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a50d030-408e-48ec-a3cd-62ad93314835",
   "metadata": {},
   "source": [
    "When you use blocking sends (MPI.Send) and blocking receives (MPI.Recv) in MPI, the order of operations matters, especially for large data. \n",
    "\n",
    "If the order of MPI.Recv calls on one process does not match the order of MPI.Send calls on the other, a deadlock can occur. Hereâ€™s why:\n",
    "How Blocking Communication Works\n",
    "\n",
    "    Blocking Send (MPI.Send): The call does not return until the message buffer can be reused (i.e., the message is either copied out or the matching receive is ready).\n",
    "    Blocking Receive (MPI.Recv): The call does not return until the message is received.\n",
    "\n",
    "Why Small Data Works\n",
    "\n",
    "For small messages, MPI implementations often use eager protocol:\n",
    "\n",
    "    The data is copied into a temporary buffer and sent immediately.\n",
    "    The send call returns as soon as the data is buffered, even if the receive is not yet posted.\n",
    "    This can hide mismatched order for small messages.\n",
    "\n",
    "Why Large Data Fails\n",
    "\n",
    "For large messages, MPI typically uses rendezvous protocol:\n",
    "\n",
    "    The send call waits until the matching receive is posted before transferring data.\n",
    "    If the receive order is wrong, both processes can end up waiting for each other, causing a deadlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80ae06e-0f32-45e0-b17c-c682fab89108",
   "metadata": {},
   "source": [
    "### Making blocking/non-blocking more explicit explicit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423347f3-f8bd-4be6-89f6-8f80913b7228",
   "metadata": {},
   "source": [
    "Take away from above and motivation:\n",
    "\n",
    "Blocking Send (MPI.Send): The call does not return until the  <span style=\"color:blue\">send message buffer can be reused</span>.\n",
    "\n",
    "Note that nothing is said about the state of the receiver. \n",
    "This is confusing or at least vague\n",
    "\n",
    "And: <span style=\"color:red\">This is also mean</span>: If you test your MPI-application with a tiny dummy dataset on the login node, all works well,\n",
    "if you then submit your large real-world job to the cluster, it deadlocks)\n",
    "\n",
    "But this is how it is defined in the MPI standard. (And the standard is always and by defintion correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3570e25d-30ae-4155-86ff-8f8417a4ab99",
   "metadata": {},
   "source": [
    "#### Synchronous Send and Receive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a12573-c9a5-4b5b-b99b-8325a3e669a6",
   "metadata": {},
   "source": [
    "You can use `comm.Ssend` to enforce synchronous send. `comm.Ssend` waits for a hand shake with the corresponding receiver, only afterwards it returns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a22ba6a-353d-47e8-98a4-9305b61ccfc5",
   "metadata": {},
   "source": [
    "#### _Exercise_\n",
    "Edit the above script and replace a single `comm.Send` by a `comm.Ssend` such that the application deadlocks for sure (also for small datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c043c5df-773b-433c-b561-18ec4d7f63b4",
   "metadata": {},
   "source": [
    "#### Aynchronous Send and Receive\n",
    "You can use `comm.Isend` to enforce asynchronous (or non-blocking) send\n",
    "`comm.Isend` should be used tother with `comm.Irecv`, the non-blocking version of `comm.Recv`\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a674e5-1be9-437f-9911-aace53132020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    # Non-blocking send\n",
    "    data = np.asarray([1, 2, 3])\n",
    "    req_send = comm.Isend(data, dest=1, tag=77)\n",
    "    # Do some computation while the send is in progress\n",
    "    result = 42**2\n",
    "    # Wait for the send to complete\n",
    "    req_send.Wait()\n",
    "    print (\"Sent:\", data)\n",
    "elif rank == 1:\n",
    "    #Allocate memory for the receive buffer\n",
    "    data = np.empty(3, np.int64)\n",
    "    # Non-blocking receive\n",
    "    req_recv = comm.Irecv(data, source=0, tag=77)\n",
    "    # Do some computation while the receive is in progress\n",
    "    result = 100/26\n",
    "    # Wait for the receive to complete\n",
    "    req_recv.Wait()\n",
    "    print(\"Received:\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ad0ba0-39c1-4fb2-b971-96f19ab86246",
   "metadata": {},
   "source": [
    "The above example also highlights that non-blocking calls can be used to interleave communication with calculation.\n",
    "\n",
    "This might give a performance increase as additional benefit. But this is not in the focus here. (The focus is on correct code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cec73c-650b-4403-9ae6-344eeb2941a3",
   "metadata": {},
   "source": [
    "#### _Exercise_\n",
    "\n",
    "Modify the above script such that it never deadlocks, no matter what the sequence of sends/receives is and no matter how big data is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df90c1a-7cfc-4512-b8e9-1f1049ea51bf",
   "metadata": {},
   "source": [
    "#### _Proposed Solution_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bb6361-b025-4ce2-892b-39ee0983897d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "dataTag = 13\n",
    "metaDataTag = 14\n",
    "\n",
    "if rank == 0:\n",
    "    #We emulate a (small) detector image, typically rank 0 would load it from file\n",
    "    #Hence only rank 0 knows its size (might be varying). Datatype is assumed fixed\n",
    "    imageSize = 3 #this information would e.g be derived from hdf5 metadata from image directly\n",
    "    detImage_2BeSent = np.arange(imageSize**2, dtype=np.int64).reshape(imageSize,imageSize)\n",
    "    #Send data first (non-blocking)\n",
    "    req_data = comm.Isend(detImage_2BeSent, dest=1, tag=dataTag)\n",
    "    print (\"0 sent:\\n\", detImage_2BeSent)\n",
    "    #Send Meta data\n",
    "    req_metadata = comm.Isend(np.array(imageSize, dtype=np.int64), dest=1, tag=metaDataTag)\n",
    "    req_data.Wait()\n",
    "    req_metadata.Wait()\n",
    "elif rank == 1:\n",
    "    #Receive Metadata first (We must know how large the buffer is that we allocate\n",
    "    imageSize_asArray = np.empty(1, dtype=np.int64)\n",
    "    req_metadata = comm.Irecv(imageSize_asArray, source=0, tag=metaDataTag)\n",
    "    req_metadata.Wait() #make sure we have data here (comment out line and se what happens)\n",
    "    imageSize = imageSize_asArray[0]\n",
    "    #Now that we know what to expect we can allocate receive buffer\n",
    "    data = np.empty(imageSize**2, dtype=np.int64)\n",
    "    req_data = comm.Irecv(data, source=0, tag=dataTag)\n",
    "    req_data.Wait()\n",
    "    detImage_Recv = data.reshape(imageSize, imageSize)\n",
    "    print (\"1 recvd:\\n\", detImage_Recv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163a8716-f012-417a-a4e2-ee6827c6e140",
   "metadata": {},
   "source": [
    "_Execrcise_ :Comment out line `req_metadata.Wait()` and see what happens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae09279-8579-4c93-a98d-2c27d0325bca",
   "metadata": {},
   "source": [
    "## Basic Communication Patterns Continued"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c145603e-96a2-4118-a8ff-d0ef48b39281",
   "metadata": {},
   "source": [
    "### Broadcast\n",
    "As the name suggest, comm.BroadCast is used to fant out data from one rank to all the others.\n",
    "\n",
    "The broadcasting rank is typially rank zero, whereas all other ranks > 0 recieve data\n",
    "\n",
    "Example (taken from official documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d39e9f-a4b4-4946-8235-218d50b7dd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "#Rank 0 allocates and initializes/define data to be broadcast\n",
    "if rank == 0:\n",
    "    data = np.arange(100, dtype='i')\n",
    "#All other ranks allocate receive buffer\n",
    "else:\n",
    "    data = np.empty(100, dtype='i')\n",
    "#Note 'collective' operation (*every* rank calls Bcast, not just rank 0) \n",
    "comm.Bcast(data, root=0)\n",
    "#Check for all ranks that they have received the Broadcaster's data\n",
    "for i in range(100):\n",
    "    assert data[i] == i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710a58bf-62ba-4bb9-91d0-112f87b791a6",
   "metadata": {},
   "source": [
    "### Scatter\n",
    "\n",
    "Data from rank 0 is split into pieces, these pices are then distributed to other ranks, similar as for Bcast\n",
    "\n",
    "Difference between Scatter and Broadcast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abc2909-9c25-41d0-ba88-5883f774622e",
   "metadata": {},
   "source": [
    "<img src=\"MPI_ScatterBcast.png\" alt=\"Alt Text\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f2fe12-a1eb-4bab-bd55-47e8a362c4d7",
   "metadata": {},
   "source": [
    "_Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fac2c21-627d-4008-8819-94613edfeed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "matrixSize = 6 #Square matrix\n",
    "\n",
    "if matrixSize % size != 0:\n",
    "    print (\"Warning, number of rows is not multiple of number of cores\")\n",
    "\n",
    "# Only rank 0 creates the full array\n",
    "if rank == 0:\n",
    "    A = np.arange(matrixSize**2, dtype=np.float64).reshape(matrixSize, matrixSize)\n",
    "    print(f\"Root process ({rank}) created the Matrix:\\n{A}\")\n",
    "else:\n",
    "    A = None #This declaration is mandatory\n",
    "\n",
    "# Scatter the rows\n",
    "# Calculate how many rows each process gets (above we made sure that no remainder)\n",
    "matrixRowsPerProcess = matrixSize//size\n",
    "#Allocate memory for row-slices (also for process 0)\n",
    "A_rows = np.empty((matrixRowsPerProcess, matrixSize), dtype=np.float64)\n",
    "\n",
    "# Scatter the rows (collective operation)\n",
    "comm.Scatter(A, A_rows, root=0)\n",
    "\n",
    "# Print the result on each process\n",
    "print(f\"Process {rank} received:\\n{A_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6561b80-ac2f-41ec-b4aa-5ccc2bce0731",
   "metadata": {},
   "source": [
    "### Gather\n",
    "\n",
    "data from all ranks is aggregated at rank 0\n",
    "\n",
    "\n",
    "<img src=\"MPI_Gather.png\" alt=\"Alt Text\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f84a81-2a20-4ee8-b027-c05c01f34eef",
   "metadata": {},
   "source": [
    "_Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f47b530-47b6-4910-9c36-28d779e8c6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "#Each rank inits ist share\n",
    "vectorLengthLocal = 4\n",
    "vectorLocal = np.arange(vectorLengthLocal, dtype=np.float64)\n",
    "print(f\"Process {rank} created local vector: {vectorLocal}\")\n",
    "\n",
    "# Root process prepares to receive all data\n",
    "if rank == 0:\n",
    "    vector = np.empty(size*vectorLengthLocal, dtype=np.float64)\n",
    "else:\n",
    "    vector = None\n",
    "\n",
    "# Gather all local arrays to root\n",
    "comm.Gather(vectorLocal, vector, root=0)\n",
    "# Root process prints the gathered array\n",
    "if rank == 0:\n",
    "    print(f\"Root process ({rank}) gathered vector: {vector}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61dceb8-623f-4e73-a2f7-faef39ad3155",
   "metadata": {},
   "source": [
    "#### _Exercise_\n",
    "\n",
    "Implement parrallel matrix multiplication `Ax` for a (4 x 4) matrix A and a (4 x 1) vector `x` using `Bcast`,  `Scatter` and `Gather`.\n",
    "\n",
    "The code should work for 2 and 4 cores.\n",
    "\n",
    "\n",
    "At the end, rank 0 (who collects the results) makes sure that the parallel version is correct (by doing the calculation itself)\n",
    "\n",
    "<img src=\"Ax.jpg\" alt=\"Ax\" width=\"500\"/>\n",
    "\n",
    "#### _Proposed solution_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3552ca8d-74d6-4097-9cf0-9c65d3454265",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "matrixSize = 4 #Square matrix\n",
    "matrixRowsPerRank = matrixSize//size\n",
    "vectorLength = matrixSize #Mandatory for well defined multiplication\n",
    "\n",
    "if matrixSize % size != 0:\n",
    "    if rank == 0:\n",
    "        print (\"WARNING: Number of matrix rows is not multiple of number of cores\")\n",
    "        comm.Abort(1)\n",
    "\n",
    "#Allocate and/or initialize/define data (rank dependent)\n",
    "#--------------------------------------------------------\n",
    "if rank == 0:\n",
    "    A = np.arange(matrixSize**2, dtype=np.float64).reshape(matrixSize, matrixSize)\n",
    "    x = np.arange(vectorLength, dtype=np.float64)\n",
    "    b = np.empty(vectorLength, dtype=np.float64)\n",
    "else:\n",
    "    A = None\n",
    "    x = np.empty(vectorLength, dtype=np.float64)\n",
    "    b = None\n",
    "\n",
    "# Scatter Matrix\n",
    "# --------------\n",
    "A_local = np.empty((matrixRowsPerRank, matrixSize), dtype=np.float64)\n",
    "comm.Scatter(A, A_local, root=0)\n",
    "\n",
    "# Broadcast vector\n",
    "# ----------------\n",
    "comm.Bcast(x, root=0)\n",
    "\n",
    "# Do parallel computation\n",
    "# ------------------------\n",
    "b_local = np.dot(A_local, x)\n",
    "\n",
    "# Gather local results\n",
    "# --------------------\n",
    "comm.Gather(b_local, b, root=0)\n",
    "\n",
    "# Unit Test (Possible since process 0 holds entire A in memory)\n",
    "# -------------------------------------------------------------\n",
    "if rank == 0:\n",
    "    print (\"Parallel computation equals sequential :\", np.allclose(np.dot(A, x), b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eab5957-6b7a-4166-9596-bd27f93973c2",
   "metadata": {},
   "source": [
    "### Anonymous receive\n",
    "MPI.any..\n",
    "Exercise: The first n-1 rank send data the the last ranks which recieives all (while number of received messages < n-1..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f279db40-98f9-4e5e-b91c-2741c9525498",
   "metadata": {},
   "source": [
    "## When to use MPI\n",
    "### When not to use MPI\n",
    "\n",
    "If you have a single program that runs sequentially on many (small) input files  -> Use SLURM Job-arrays\n",
    "\n",
    "### Use for embarrassingly parallel algorithms ? Maybe\n",
    "\n",
    "If you have large input-data (e.g. a huge detector image) that should finish asap but is embarrassingly parallel.\n",
    "\n",
    "What do we mean with embarrassingly parallel ?\n",
    "\n",
    "_Example_: Normalize a detector image such that all intensities are between (0,1)\n",
    "\n",
    "This calculation can be done independently for each pixel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73773794-dc07-4e70-ba6f-d593d1b155c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bitDepth = 8\n",
    "detectorImage = np.random.randint(2**bitDepth, size=(220, 200))\n",
    "detectorImageNormalized = detectorImage.astype(np.float64) / 2**bitDepth\n",
    "plt.imshow(detectorImage); plt.colorbar()\n",
    "plt.show()\n",
    "plt.imshow(detectorImageNormalized); plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c7b410-735a-4501-a62e-025b49f4566c",
   "metadata": {},
   "source": [
    "_Excercise_\n",
    "\n",
    "Parallelize the above normalization, allocate the random image on rank 0, only rank 0 needs to hold the complete result (the full normalize imaged) <BR>\n",
    "Use Scatter on Gather to distribute raw data and collect processed data (Reuse Matrix_Scatter code)\n",
    "\n",
    "Compare result with single process computation (done on rank 0 exclusively)\n",
    "\n",
    "_Proposed solution_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7812ecea-36a6-446a-a09d-2946d0ae1cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import scipy.datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "numberOfRows = 220\n",
    "numberOfColumns = 200\n",
    "bitDepth = 8\n",
    "\n",
    "if rank == 0:\n",
    "    detectorImage = np.random.randint(2**bitDepth, size=(numberOfRows,numberOfColumns)).astype(np.float64)\n",
    "else:\n",
    "    detectorImage = None\n",
    "\n",
    "numberOfRowsPerCore = numberOfRows//size\n",
    "\n",
    "detectorImageSlice = np.empty((numberOfRowsPerCore, numberOfColumns), dtype=np.float64)\n",
    "\n",
    "comm.Scatter(detectorImage, detectorImageSlice, root=0)\n",
    "\n",
    "detectorImageSliceNormalized = detectorImageSlice / 2**bitDepth\n",
    "\n",
    "if rank == 0:\n",
    "    detectorImageNormalized = np.empty((numberOfRows,numberOfColumns) , dtype=np.float64)\n",
    "else:\n",
    "    detectorImageNormalized = None\n",
    "\n",
    "comm.Gather(detectorImageSliceNormalized, detectorImageNormalized, root=0)\n",
    "\n",
    "if rank == 0:\n",
    "    print (\"Parallel computation equals sequential :\", np.allclose(detectorImage/2**bitDepth, detectorImageNormalized))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860c6b1b-c06d-4e11-9c1d-a12d5dec3892",
   "metadata": {},
   "source": [
    "### Use for non-embarrassingly parallel algorithms\n",
    "\n",
    "What does non-embarrassingly mean in the context of image analysis? <BR>\n",
    "Calculations can not be done fully independent for each pixel, neighbouring pixels are involved <BR>\n",
    "As an example, we apply a Laplace filter to a random image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed23ee70-8b1c-4c81-a892-42d38e93ffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import laplace\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bitDepth = 8\n",
    "detectorImage = np.random.randint(2**bitDepth, size=(220, 200)).astype(np.float64)\n",
    "filteredImage = laplace(detectorImage, mode='constant', cval=0)\n",
    "plt.imshow(detectorImage); plt.colorbar()\n",
    "plt.show()\n",
    "plt.imshow(filteredImage); plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f666efde-6aba-4839-940c-dc66d2fcf2f8",
   "metadata": {},
   "source": [
    "_Exercise_\n",
    "\n",
    "Parallelize above code, first use Scatter + Gather as above. \n",
    "\n",
    "You only need to change a single line, i.e. the actual image processing, but maybe you also want to give better variable names)\n",
    "\n",
    "Does the unit test pass? Why not? Plot the difference (sequential result - parallel result) to get a hint.\n",
    "\n",
    "_Proposed (Anti-)Solution_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f091215d-5d70-4d9b-a981-98c2a6eae68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "from scipy.ndimage import laplace\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "numberOfRows = 220\n",
    "numberOfColumns = 200\n",
    "\n",
    "bitDepth = 8\n",
    "if rank == 0:\n",
    "    detectorImage = np.random.randint(2**bitDepth, size=(numberOfRows,numberOfColumns)).astype(np.float64)\n",
    "else:\n",
    "    detectorImage = None\n",
    "\n",
    "numberOfRowsPerCore = numberOfRows//size\n",
    "\n",
    "detectorImageSlice = np.empty((numberOfRowsPerCore, numberOfColumns), dtype=np.float64)\n",
    "\n",
    "comm.Scatter(detectorImage, detectorImageSlice, root=0)\n",
    "\n",
    "#This is the only line that neds to be changed\n",
    "detectorImageSliceFiltered = laplace(detectorImageSlice, mode='constant', cval=0)\n",
    "\n",
    "if rank == 0:\n",
    "    detectorImageFiltered = np.empty((numberOfRows,numberOfColumns) , dtype=np.float64)\n",
    "else:\n",
    "    detectorImageFiltered = None\n",
    "\n",
    "comm.Gather(detectorImageSliceFiltered, detectorImageFiltered, root=0)\n",
    "\n",
    "if rank == 0:\n",
    "    print (\"Parallel computation equals sequential :\", np.allclose(laplace(detectorImage, mode='constant', cval=0), detectorImageFiltered))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b8c450-dad2-4c39-b854-19255d63ce46",
   "metadata": {},
   "source": [
    "_Exercise_: Make the above code work\n",
    "\n",
    "   * Assume 2 ranks only\n",
    "   * Both ranks are allowed to allocate initial 2d array (inefficient for a real application)\n",
    "   * The 2d array is split in a upper and lower part, as above, one for each rank\n",
    "   * Rank 1 sends his (partial) filter result to rank 0 (point to point, no Gather/Scatter)\n",
    "   * After having received image stripe from rank 1, rank 0 does the unit test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8161794c-018e-431a-b55d-68f528e9e188",
   "metadata": {},
   "source": [
    "### The 2D Heat equation\n",
    "\n",
    "The 2D heat equation reads\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial u}{\\partial t} = \\alpha \\Delta u ;\n",
    "\\quad  \\quad\n",
    "\\Delta u = \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2u}{\\partial y^2};\n",
    "\\quad \\quad\n",
    "u = u(x,y,t)\n",
    "\\end{equation}\n",
    "\n",
    "We integrate this equation according a simple Euler schema to get the time evolution\n",
    "\n",
    "\\begin{equation}\n",
    "u(x,y,k+1) = u(x,y,k) + dt \\cdot \\alpha \\cdot \\Delta u\n",
    "\\end{equation}\n",
    "where $u(x,y,k)$ is the 2D heat distribution at discretized time $k$ and $dt$ labels a time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9b4f7ec-9e32-4b54-b40c-cc809b07a8a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mndimage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m laplace\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpylab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import laplace\n",
    "\n",
    "import pylab as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "numberOfGridPoints = 50\n",
    "dt = 0.1\n",
    "alpha = 1\n",
    "numberOfIterations = 100000\n",
    "u = np.zeros((numberOfGridPoints, numberOfGridPoints))\n",
    "#initial condition (heat peak in center)\n",
    "u[numberOfGridPoints//2, numberOfGridPoints//2] = 100\n",
    "#boundary condition (Dirichlet boundary condition)\n",
    "b = 1.0\n",
    "\n",
    "def update(u, alpha, dt, b):\n",
    "    return u + dt*alpha*laplace(u, mode='constant', cval=b)\n",
    "\n",
    "\n",
    "#Plotting only\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "def animate(i):\n",
    "    global u\n",
    "    u = update(u, alpha, dt, b)\n",
    "    ax.imshow(u)\n",
    "    ax.set_axis_off()\n",
    "\n",
    "anim = FuncAnimation(fig, animate, frames=numberOfIterations, interval=10)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2b0986-44b8-4931-9015-18bbaf856898",
   "metadata": {},
   "source": [
    "_Exercise_: Parallelize the above solver\n",
    "   * Use the same structure as above (for the 2D filtering)\n",
    "   * You must exchange after every time step\n",
    "\n",
    "\n",
    "Ilustration of the HALO concept:\n",
    "\n",
    "\n",
    "<img src=\"halo.svg\" alt=\"Alt Text\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5f66f1-ed6d-440f-9810-76c28796d0a2",
   "metadata": {},
   "source": [
    "## Integration with Slurm\n",
    "So far all examples were launched from command line and executed on login node.\n",
    "\n",
    "Real world jobs (HPC use case with large memory consumption and and intense number crunching) must be submitted to the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9220d1fc-120c-4359-88c4-2a1bde4ab233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbaa9c8b-676c-4566-9d6d-f6dd0652aa85",
   "metadata": {},
   "source": [
    "## Outlook: Running Jobs on a GPU cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfe8c7b-b2b6-4313-93d5-3674641ed5e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
