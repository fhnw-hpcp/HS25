{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19d755b7",
   "metadata": {},
   "source": [
    "![GPU Basics Slide](img/02_GPU_Basics/Folie1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9e3724",
   "metadata": {},
   "source": [
    "![GPU Basics Slide](img/02_GPU_Basics/Folie2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec88a81a",
   "metadata": {},
   "source": [
    "![GPU Basics Slide](img/02_GPU_Basics/Folie3.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cbf9cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda, float32, config, vectorize\n",
    "from numba.core.errors import NumbaPerformanceWarning\n",
    "\n",
    "config.CUDA_ENABLE_PYNVJITLINK = 1\n",
    "warnings.filterwarnings(\"ignore\", category=NumbaPerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ac5e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b49c220",
   "metadata": {},
   "source": [
    "Check if we have a CUDA device available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868c42b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Managed Device <CUdevice 0>>\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "print(cuda.gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6741f8fe",
   "metadata": {},
   "source": [
    "Now let's implement GPU computation using the `@vectorize` decorator with the target set to 'cuda'. This will automatically execute the code on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fefea43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We get results from the GPU: [3.5241649e-01 1.0848190e-04 1.0519932e-02 7.4898817e-02 1.6896421e-01]\n"
     ]
    }
   ],
   "source": [
    "vectorA = np.random.rand(65536).astype(np.float32)\n",
    "vectorB = np.random.rand(65536).astype(np.float32)\n",
    "\n",
    "@vectorize([float32(float32, float32)], target='cuda')\n",
    "def squared_error(d_vectorA, d_vectorB):\n",
    "    return (d_vectorA - d_vectorB) ** 2\n",
    "\n",
    "vectorC = squared_error(vectorA, vectorB)\n",
    "print(f\"We get results from the GPU: {vectorC[0:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d94505b",
   "metadata": {},
   "source": [
    "What happened exactly? How does this work under the hood? Let's have a closer look."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824e7e47",
   "metadata": {},
   "source": [
    "![GPU Basics Slide](img/02_GPU_Basics/Folie4.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db58a07",
   "metadata": {},
   "source": [
    "![GPU Basics Slide](img/02_GPU_Basics/Folie5.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccda96c",
   "metadata": {},
   "source": [
    "![GPU Basics Slide](img/02_GPU_Basics/Folie6.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ec76a6",
   "metadata": {},
   "source": [
    "![GPU Basics Slide](img/02_GPU_Basics/Folie7.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7282ba3",
   "metadata": {},
   "source": [
    "![GPU Basics Slide](img/02_GPU_Basics/Folie8.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595dde2f",
   "metadata": {},
   "source": [
    "![GPU Basics Slide](img/02_GPU_Basics/Folie9.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda8404b",
   "metadata": {},
   "source": [
    "![GPU Basics Slide](img/02_GPU_Basics/Folie10.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4f097a",
   "metadata": {},
   "source": [
    "### Task 0\n",
    "\n",
    "To analyze profiling results, please install the NVIDIA tools on your **local machine**.\n",
    "\n",
    "> **Note:** You do **not** need an NVIDIA GPU on your computer.  \n",
    "> The profiling itself runs on the server. You only need the tools locally to open and inspect the result files in the GUI.\n",
    "\n",
    "- [Nsight Systems (Nsys)](https://developer.nvidia.com/nsight-systems/get-started)\n",
    "- [Nsight Compute (Ncu)](https://developer.nvidia.com/tools-overview/nsight-compute/get-started)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfec6381",
   "metadata": {},
   "source": [
    "Now let us use the NVIDIA Nsight Systems (nsys) profiling tool to examine the timeline shown in the slide above.\n",
    "\n",
    "We need to isolate our code into a Python file to execute it easily with nsys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d6b526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting profiling/cuda_vectorize.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile profiling/cuda_vectorize.py\n",
    "import numpy as np\n",
    "from numba import vectorize, float32, cuda\n",
    "\n",
    "vectorA = np.random.rand(65536).astype(np.float32)\n",
    "vectorB = np.random.rand(65536).astype(np.float32)\n",
    "\n",
    "@vectorize([float32(float32, float32)], target='cuda')\n",
    "def squared_error(d_vectorA, d_vectorB):\n",
    "    return (d_vectorA - d_vectorB) ** 2\n",
    "\n",
    "vectorC = squared_error(vectorA, vectorB)\n",
    "print(f\"We get results from the GPU: {vectorC[0:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff889aad",
   "metadata": {},
   "source": [
    "We can wrap that Python file with the nsys profiler (which is preinstalled in this environment) to analyze the execution timeline and understand the code's performance characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f550eae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: CPU IP/backtrace sampling not supported, disabling.\n",
      "Try the 'nsys status --environment' command to learn more.\n",
      "\n",
      "WARNING: CPU context switch tracing not supported, disabling.\n",
      "Try the 'nsys status --environment' command to learn more.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data...\n",
      "/home/azureuser/src/HS25/.venv/lib/python3.12/site-packages/numba_cuda/numba/cuda/dispatcher.py:686: NumbaPerformanceWarning: Grid size 64 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "We do actually get results from teh GPU: [0.5192787  0.03461697 0.03134166 0.5295557  0.61284894]\n",
      "Generating '/tmp/nsys-report-9c5e.qdstrm'\n",
      "[1/6] [========================100%] cuda_vectorize.nsys-rep\n",
      "[2/6] [========================100%] cuda_vectorize.sqlite\n",
      "[3/6] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)         Name       \n",
      " --------  ---------------  ---------  --------  --------  --------  --------  -----------  ------------------\n",
      "     47.9           321914          3  107304.7    6323.0      5210    310381     175870.1  cuMemAlloc_v2     \n",
      "     15.0           100713          2   50356.5   50356.5     36240     64473      19963.7  cuMemcpyHtoD_v2   \n",
      "     11.3            75814          1   75814.0   75814.0     75814     75814          0.0  cuModuleLoadDataEx\n",
      "     10.8            72368          1   72368.0   72368.0     72368     72368          0.0  cuMemcpyDtoH_v2   \n",
      "     10.6            71286          1   71286.0   71286.0     71286     71286          0.0  cuMemGetInfo_v2   \n",
      "      4.1            27473          1   27473.0   27473.0     27473     27473          0.0  cuLaunchKernel    \n",
      "      0.4             2385          1    2385.0    2385.0      2385      2385          0.0  cuInit            \n",
      "\n",
      "[4/6] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)                                                  Name                                                \n",
      " --------  ---------------  ---------  --------  --------  --------  --------  -----------  ----------------------------------------------------------------------------------------------------\n",
      "    100.0             5056          1    5056.0    5056.0      5056      5056          0.0  __main__::__vectorized_squared_error[abi:v3,cw51cXTLSUwv1sDUaKthrqNgqqmjgEhasCBDCg1bcEDrKABNtAWlPGDâ€¦\n",
      "\n",
      "[5/6] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)           Operation          \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ----------------------------\n",
      "     67.1            45343      2   22671.5   22671.5     22527     22816        204.4  [CUDA memcpy Host-to-Device]\n",
      "     32.9            22240      1   22240.0   22240.0     22240     22240          0.0  [CUDA memcpy Device-to-Host]\n",
      "\n",
      "[6/6] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)           Operation          \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ----------------------------\n",
      "      0.524      2     0.262     0.262     0.262     0.262        0.000  [CUDA memcpy Host-to-Device]\n",
      "      0.262      1     0.262     0.262     0.262     0.262        0.000  [CUDA memcpy Device-to-Host]\n",
      "\n",
      "Generated:\n",
      "    /home/azureuser/src/HS25/02_Python_CUDA/profiling/cuda_vectorize.nsys-rep\n",
      "    /home/azureuser/src/HS25/02_Python_CUDA/profiling/cuda_vectorize.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile -f true -o profiling/cuda_vectorize -t cuda --stats true python3 profiling/cuda_vectorize.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611c83e4",
   "metadata": {},
   "source": [
    "In fact, we do see the same timeline as in the slides. The big gap between the H2D and the kernel launch is due to\n",
    "* Python is slow in general\n",
    "* The function needs to be compiled first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd13392",
   "metadata": {},
   "source": [
    "![Nsys profiling of vectorized CUDA kernel](img/01_vectorize_nsys.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715ebd95",
   "metadata": {},
   "source": [
    "### Task 1: JIT impact\n",
    "\n",
    "Modify the `cuda_vectorize.py` file such that the function is called multiple times. Does the big gap disappear?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59ef377",
   "metadata": {},
   "source": [
    "Let us dig deeper. What exactly happens on the GPU? How is the calculation distributed among the thousands of cores a GPU has? How can we control this ourselves?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc5ecea",
   "metadata": {},
   "source": [
    "![GPU Basics Slide](img/02_GPU_Basics/Folie11.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9bad29",
   "metadata": {},
   "source": [
    "![GPU Basics Slide](img/02_GPU_Basics/Folie12.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeeacd3",
   "metadata": {},
   "source": [
    "![GPU Basics Slide](img/02_GPU_Basics/Folie13.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e53321",
   "metadata": {},
   "source": [
    "![GPU Basics Slide](img/02_GPU_Basics/Folie14.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f83999",
   "metadata": {},
   "source": [
    "![GPU Basics Slide](img/02_GPU_Basics/Folie15.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d567db59",
   "metadata": {},
   "source": [
    "![GPU Basics Slide](img/02_GPU_Basics/Folie16.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2835ea64",
   "metadata": {},
   "source": [
    "### Task 2: Understand the CUDA Execution Model\n",
    "\n",
    "Modify the `hello_kernel` below to launch at least 8 different kernel configurations. Manually calculate the global thread ID inside the kernel. Experiment with both 1D and 2D grid/block layouts. For each configuration, consider what happens on the GPU and how the execution differs on the hardware. Document your observations and reasoning for each configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c9ce5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from thread 4 - I'm in block 1 of size 4 and local threadIdx 0\n",
      "Hello from thread 5 - I'm in block 1 of size 4 and local threadIdx 1\n",
      "Hello from thread 6 - I'm in block 1 of size 4 and local threadIdx 2\n",
      "Hello from thread 7 - I'm in block 1 of size 4 and local threadIdx 3\n",
      "Hello from thread 0 - I'm in block 0 of size 4 and local threadIdx 0\n",
      "Hello from thread 1 - I'm in block 0 of size 4 and local threadIdx 1\n",
      "Hello from thread 2 - I'm in block 0 of size 4 and local threadIdx 2\n",
      "Hello from thread 3 - I'm in block 0 of size 4 and local threadIdx 3\n",
      "Hello from thread 8 - I'm in block 2 of size 4 and local threadIdx 0\n",
      "Hello from thread 9 - I'm in block 2 of size 4 and local threadIdx 1\n",
      "Hello from thread 10 - I'm in block 2 of size 4 and local threadIdx 2\n",
      "Hello from thread 11 - I'm in block 2 of size 4 and local threadIdx 3\n",
      "Hello from thread 12 - I'm in block 3 of size 4 and local threadIdx 0\n",
      "Hello from thread 13 - I'm in block 3 of size 4 and local threadIdx 1\n",
      "Hello from thread 14 - I'm in block 3 of size 4 and local threadIdx 2\n",
      "Hello from thread 15 - I'm in block 3 of size 4 and local threadIdx 3\n"
     ]
    }
   ],
   "source": [
    "@cuda.jit\n",
    "def hello_kernel():\n",
    "    # ToDo: calculate the global thread ID manually\n",
    "    global_id = cuda.grid(1)\n",
    "    print(\n",
    "        \"Hello from thread\", global_id, \n",
    "        \"- I'm in block\", cuda.blockIdx.x, \n",
    "        \"of size\", cuda.blockDim.x, \n",
    "        \"and local threadIdx\", cuda.threadIdx.x\n",
    "    )\n",
    "\n",
    "# ToDo: test a lot of different configurations and understand what is happening on the GPU\n",
    "hello_kernel[4, 4]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ea01d9",
   "metadata": {},
   "source": [
    "ToDo: write down findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72be65d4",
   "metadata": {},
   "source": [
    "### Task 3: Write a numba.cuda kernel with explicit calls\n",
    "\n",
    "Now it is your turn. Write the `squared_error_kernel` code with explicit data transfers and allocations. Try out different kernel launch parameters and think about which ones make sense. Once it works, profile it using nsys. Do you see the exact same behavior as it the vectorized version above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35010006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03245101 0.00085454 0.02855932 0.01089774 0.0108772  0.07373396]\n"
     ]
    }
   ],
   "source": [
    "#ToDo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e837a4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
