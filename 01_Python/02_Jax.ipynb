{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![JAX Slide](img/02_JAX/Folie1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![JAX Slide](img/02_JAX/Folie2.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-08-20 09:28:31,355:jax._src.xla_bridge:872: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX version: 0.7.0\n",
      "JAX devices: [CpuDevice(id=0)]\n",
      "JAX default backend: cpu\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"JAX devices: {jax.devices()}\")\n",
    "print(f\"JAX default backend: {jax.default_backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![JAX Slide](img/02_JAX/Folie3.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy array: [1 2 3 4 5], type: <class 'numpy.ndarray'>\n",
      "JAX array: [1 2 3 4 5], type: <class 'jaxlib._jax.ArrayImpl'>\n",
      "\n",
      "Conversion works seamlessly\n",
      "JAX->NumPy: [1 2 3 4 5]\n",
      "NumPy->JAX: [1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "# Create arrays in NumPy and JAX\n",
    "np_array = np.array([1, 2, 3, 4, 5])\n",
    "jax_array = jnp.array([1, 2, 3, 4, 5])\n",
    "\n",
    "print(f\"NumPy array: {np_array}, type: {type(np_array)}\")\n",
    "print(f\"JAX array: {jax_array}, type: {type(jax_array)}\")\n",
    "\n",
    "# Converting between them\n",
    "np_from_jax = np.array(jax_array)\n",
    "jax_from_np = jnp.array(np_array)\n",
    "\n",
    "print(f\"\\nConversion works seamlessly\")\n",
    "print(f\"JAX->NumPy: {np_from_jax}\")\n",
    "print(f\"NumPy->JAX: {jax_from_np}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: [1 2 3]\n",
      "After x.at[0].set(999): original x = [1 2 3], new y = [999   2   3]\n",
      "NumPy in-place modification: [999   2   3]\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate immutability\n",
    "x = jnp.array([1, 2, 3])\n",
    "print(f\"Original: {x}\")\n",
    "\n",
    "# This creates a NEW array\n",
    "y = x.at[0].set(999)\n",
    "print(f\"After x.at[0].set(999): original x = {x}, new y = {y}\")\n",
    "\n",
    "# Compare with NumPy (mutable)\n",
    "np_x = np.array([1, 2, 3])\n",
    "np_x[0] = 999  # Modifies in-place\n",
    "print(f\"NumPy in-place modification: {np_x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![JAX Slide](img/02_JAX/Folie4.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![JAX Slide](img/02_JAX/Folie5.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![JAX Slide](img/02_JAX/Folie6.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance comparison:\n",
      "284 μs ± 14 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "252 μs ± 111 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "\n",
      "Results match: True\n"
     ]
    }
   ],
   "source": [
    "# Simple function without JIT\n",
    "def slow_function(x):\n",
    "    return jnp.sum(x**2) + jnp.mean(x**3) - jnp.std(x)\n",
    "\n",
    "# Same function with JIT\n",
    "@jax.jit\n",
    "def fast_function(x):\n",
    "    return jnp.sum(x**2) + jnp.mean(x**3) - jnp.std(x)\n",
    "\n",
    "# Create test data\n",
    "data = jnp.array(np.random.rand(100_000))\n",
    "\n",
    "# Warm up the JIT function (compilation happens here)\n",
    "_ = fast_function(data)\n",
    "\n",
    "print(\"Performance comparison:\")\n",
    "%timeit slow_function(data).block_until_ready()\n",
    "%timeit fast_function(data).block_until_ready()\n",
    "\n",
    "# Verify results are the same\n",
    "print(f\"\\nResults match: {jnp.allclose(slow_function(data), fast_function(data))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding `.block_until_ready()`\n",
    "\n",
    "JAX executes asynchronously by default. For accurate timing, we need to wait for computation to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without .block_until_ready():\n",
      "Time: 0.126603 seconds (misleading!)\n",
      "\n",
      "With .block_until_ready():\n",
      "Time: 0.028470 seconds (accurate)\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate asynchronous execution\n",
    "large_data = jnp.array(np.random.rand(10_000_000))\n",
    "\n",
    "print(\"Without .block_until_ready():\")\n",
    "start = time.time()\n",
    "result = jnp.sum(large_data**2)  # Returns immediately\n",
    "end = time.time()\n",
    "print(f\"Time: {end - start:.6f} seconds (misleading!)\")\n",
    "\n",
    "print(\"\\nWith .block_until_ready():\")\n",
    "start = time.time()\n",
    "result = jnp.sum(large_data**2).block_until_ready()  # Waits for completion\n",
    "end = time.time()\n",
    "print(f\"Time: {end - start:.6f} seconds (accurate)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Automatic Differentiation\n",
    "\n",
    "JAX provides automatic differentiation with `grad()`, `jacrev()`, and `jacfwd()`. This is crucial for optimization and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(2.0) = 12.0\n",
      "f'(2.0) = 7.0\n",
      "Analytical f'(x) = 2x + 3 = 7.0\n",
      "f''(2.0) = 2.0\n",
      "Analytical f''(x) = 2\n"
     ]
    }
   ],
   "source": [
    "# Define a simple function\n",
    "def quadratic(x):\n",
    "    return x**2 + 3*x + 2\n",
    "\n",
    "# Get its derivative\n",
    "quadratic_grad = jax.grad(quadratic)\n",
    "\n",
    "# Test values\n",
    "x = 2.0\n",
    "print(f\"f({x}) = {quadratic(x)}\")\n",
    "print(f\"f'({x}) = {quadratic_grad(x)}\")\n",
    "print(f\"Analytical f'(x) = 2x + 3 = {2*x + 3}\")\n",
    "\n",
    "# Higher-order derivatives\n",
    "quadratic_hessian = jax.grad(jax.grad(quadratic))\n",
    "print(f\"f''({x}) = {quadratic_hessian(x)}\")\n",
    "print(f\"Analytical f''(x) = 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 6.3319\n",
      "Gradients: {'b': Array(1.8523035, dtype=float32, weak_type=True), 'w': Array(-4.158302, dtype=float32, weak_type=True)}\n",
      "True parameters: w=2.5, b=-1.0\n"
     ]
    }
   ],
   "source": [
    "# More complex example: gradient of a loss function\n",
    "def mse_loss(params, x, y):\n",
    "    \"\"\"Mean squared error loss for linear regression\"\"\"\n",
    "    predictions = params['w'] * x + params['b']\n",
    "    return jnp.mean((predictions - y)**2)\n",
    "\n",
    "# Generate synthetic data\n",
    "key = jax.random.PRNGKey(42)\n",
    "x_data = jax.random.normal(key, (100,))\n",
    "true_w, true_b = 2.5, -1.0\n",
    "y_data = true_w * x_data + true_b + 0.1 * jax.random.normal(key, (100,))\n",
    "\n",
    "# Initial parameters\n",
    "params = {'w': 0.0, 'b': 0.0}\n",
    "\n",
    "# Compute gradients\n",
    "loss_grad = jax.grad(mse_loss)\n",
    "grads = loss_grad(params, x_data, y_data)\n",
    "\n",
    "print(f\"Initial loss: {mse_loss(params, x_data, y_data):.4f}\")\n",
    "print(f\"Gradients: {grads}\")\n",
    "print(f\"True parameters: w={true_w}, b={true_b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Simple Gradient Descent\n",
    "\n",
    "Implement gradient descent optimization using JAX autodiff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization progress:\n",
      "Step   0: loss=6.331944, w=0.0000, b=0.0000\n",
      "Step  20: loss=0.005290, w=2.5212, b=-0.9815\n",
      "Step  40: loss=0.000005, w=2.5976, b=-0.9996\n",
      "Step  60: loss=0.000000, w=2.5999, b=-1.0000\n",
      "Step  80: loss=0.000000, w=2.6000, b=-1.0000\n",
      "\n",
      "Final: loss=0.000000, w=2.6000, b=-1.0000\n",
      "True parameters: w=2.5000, b=-1.0000\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: Implement gradient descent\n",
    "@jax.jit\n",
    "def gradient_step(params, x, y, learning_rate):\n",
    "    \"\"\"Single gradient descent step\"\"\"\n",
    "    # TODO: Compute gradients and update parameters\n",
    "    # Hint: use jax.grad(mse_loss)(params, x, y)\n",
    "    \n",
    "    grads = jax.grad(mse_loss)(params, x, y)\n",
    "    \n",
    "    # Update parameters\n",
    "    new_params = {\n",
    "        'w': params['w'] - learning_rate * grads['w'],\n",
    "        'b': params['b'] - learning_rate * grads['b']\n",
    "    }\n",
    "    \n",
    "    return new_params\n",
    "\n",
    "# Run optimization\n",
    "params = {'w': 0.0, 'b': 0.0}\n",
    "learning_rate = 0.1\n",
    "\n",
    "print(\"Optimization progress:\")\n",
    "for i in range(100):\n",
    "    if i % 20 == 0:\n",
    "        loss = mse_loss(params, x_data, y_data)\n",
    "        print(f\"Step {i:3d}: loss={loss:.6f}, w={params['w']:.4f}, b={params['b']:.4f}\")\n",
    "    \n",
    "    params = gradient_step(params, x_data, y_data, learning_rate)\n",
    "\n",
    "final_loss = mse_loss(params, x_data, y_data)\n",
    "print(f\"\\nFinal: loss={final_loss:.6f}, w={params['w']:.4f}, b={params['b']:.4f}\")\n",
    "print(f\"True parameters: w={true_w:.4f}, b={true_b:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Function Transformations\n",
    "\n",
    "JAX's strength lies in its composable function transformations: `jit`, `grad`, `vmap`, `pmap`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance comparison:\n",
      "254 ms ± 56.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "1.14 ms ± 222 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "\n",
      "Results match: True\n"
     ]
    }
   ],
   "source": [
    "# vmap: Vectorization over batch dimensions\n",
    "def compute_distance(point1, point2):\n",
    "    \"\"\"Euclidean distance between two points\"\"\"\n",
    "    return jnp.sqrt(jnp.sum((point1 - point2)**2))\n",
    "\n",
    "# Create test data: 1000 points in 3D\n",
    "key = jax.random.key(0)\n",
    "points = jax.random.normal(key, (1000, 3))\n",
    "origin = jnp.zeros(3)\n",
    "\n",
    "# Method 1: Loop (slow)\n",
    "def distances_loop(points, origin):\n",
    "    distances = []\n",
    "    for point in points:\n",
    "        distances.append(compute_distance(point, origin))\n",
    "    return jnp.array(distances)\n",
    "\n",
    "# Method 2: vmap (fast)\n",
    "distances_vectorized = jax.vmap(compute_distance, in_axes=(0, None))\n",
    "\n",
    "# Compare performance\n",
    "print(\"Performance comparison:\")\n",
    "%timeit distances_loop(points, origin).block_until_ready()\n",
    "%timeit distances_vectorized(points, origin).block_until_ready()\n",
    "\n",
    "# Verify results match\n",
    "result1 = distances_loop(points[:10], origin)\n",
    "result2 = distances_vectorized(points[:10], origin)\n",
    "print(f\"\\nResults match: {jnp.allclose(result1, result2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding `vmap` Arguments\n",
    "\n",
    "- `in_axes`: Which axes to vectorize over for each input\n",
    "- `out_axes`: Which axis to use for outputs\n",
    "- `None`: Don't vectorize this input (broadcast it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A shape: (5, 3), B shape: (4, 3)\n",
      "All pairwise distances shape: (5, 4)\n",
      "Nested vmap result shape: (5, 4)\n",
      "Results match: True\n"
     ]
    }
   ],
   "source": [
    "# Different vmap configurations\n",
    "A = jax.random.normal(key, (5, 3))\n",
    "B = jax.random.normal(key, (4, 3))\n",
    "\n",
    "print(f\"A shape: {A.shape}, B shape: {B.shape}\")\n",
    "\n",
    "# Compute distances between all pairs\n",
    "# vmap over first axis of A, don't vmap B (broadcast)\n",
    "dist_A_to_all_B = jax.vmap(lambda a: jax.vmap(lambda b: compute_distance(a, b))(B))(A)\n",
    "print(f\"All pairwise distances shape: {dist_A_to_all_B.shape}\")\n",
    "\n",
    "# Alternative: nest vmap calls\n",
    "pairwise_distance = jax.vmap(jax.vmap(compute_distance, in_axes=(None, 0)), in_axes=(0, None))\n",
    "result = pairwise_distance(A, B)\n",
    "print(f\"Nested vmap result shape: {result.shape}\")\n",
    "print(f\"Results match: {jnp.allclose(dist_A_to_all_B, result)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 JAX and Random Numbers\n",
    "\n",
    "JAX uses explicit random keys for reproducibility and parallel safety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial key: Array((), dtype=key<fry>) overlaying:\n",
      "[ 0 42]\n",
      "After split: key=Array((), dtype=key<fry>) overlaying:\n",
      "[1832780943  270669613]\n",
      "Subkey1: Array((), dtype=key<fry>) overlaying:\n",
      "[  64467757 2916123636]\n",
      "Subkey2: Array((), dtype=key<fry>) overlaying:\n",
      "[2465931498  255383827]\n",
      "\n",
      "Sample 1: [ 0.60576403  0.7990441  -0.908927   -0.63525754 -1.2226585 ]\n",
      "Sample 2: [ 0.4323065   0.5872638  -1.1416743  -0.37379906 -0.19910173]\n",
      "\n",
      "Repeating subkey1: [ 0.60576403  0.7990441  -0.908927   -0.63525754 -1.2226585 ]\n",
      "Identical to sample1: True\n"
     ]
    }
   ],
   "source": [
    "# JAX requires explicit random state management\n",
    "key = jax.random.key(42)\n",
    "print(f\"Initial key: {key}\")\n",
    "\n",
    "# Split key to generate independent streams\n",
    "key, subkey1, subkey2 = jax.random.split(key, 3)\n",
    "print(f\"After split: key={key}\")\n",
    "print(f\"Subkey1: {subkey1}\")\n",
    "print(f\"Subkey2: {subkey2}\")\n",
    "\n",
    "# Use subkeys for random generation\n",
    "sample1 = jax.random.normal(subkey1, (5,))\n",
    "sample2 = jax.random.normal(subkey2, (5,))\n",
    "\n",
    "print(f\"\\nSample 1: {sample1}\")\n",
    "print(f\"Sample 2: {sample2}\")\n",
    "\n",
    "# Using same key gives same results (reproducibility)\n",
    "sample1_repeat = jax.random.normal(subkey1, (5,))\n",
    "print(f\"\\nRepeating subkey1: {sample1_repeat}\")\n",
    "print(f\"Identical to sample1: {jnp.allclose(sample1, sample1_repeat)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Monte Carlo π Estimation\n",
    "\n",
    "Use JAX to estimate π using Monte Carlo sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo π estimation:\n",
      "N=   1000: π≈3.060000, error=0.081593\n",
      "N=  10000: π≈3.162800, error=0.021207\n",
      "N= 100000: π≈3.139520, error=0.002073\n",
      "N=1000000: π≈3.142648, error=0.001055\n",
      "\n",
      "True π: 3.141593\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: Monte Carlo estimation of π\n",
    "def estimate_pi(key, n_samples):\n",
    "    \"\"\"Estimate π by sampling random points in unit square\"\"\"\n",
    "    # TODO: Generate random points in [-1, 1] x [-1, 1]\n",
    "    # Count how many fall inside unit circle\n",
    "    # π ≈ 4 * (points inside circle) / (total points)\n",
    "    \n",
    "    # Generate random points\n",
    "    points = jax.random.uniform(key, (n_samples, 2), minval=-1.0, maxval=1.0)\n",
    "    \n",
    "    # Check which points are inside unit circle\n",
    "    distances_squared = jnp.sum(points**2, axis=1)\n",
    "    inside_circle = distances_squared <= 1.0\n",
    "    \n",
    "    # Estimate π\n",
    "    pi_estimate = 4.0 * jnp.mean(inside_circle)\n",
    "    \n",
    "    return pi_estimate\n",
    "\n",
    "# Test with different sample sizes\n",
    "key = jax.random.key(123)\n",
    "sample_sizes = [1000, 10000, 100000, 1000000]\n",
    "\n",
    "print(\"Monte Carlo π estimation:\")\n",
    "for n in sample_sizes:\n",
    "    key, subkey = jax.random.split(key)\n",
    "    pi_est = estimate_pi(subkey, n)\n",
    "    error = abs(pi_est - jnp.pi)\n",
    "    print(f\"N={n:7d}: π≈{pi_est:.6f}, error={error:.6f}\")\n",
    "\n",
    "print(f\"\\nTrue π: {jnp.pi:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![JAX Slide](img/02_JAX/Folie7.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Optimized Linear Algebra\n",
    "\n",
    "JAX provides optimized linear algebra operations through XLA compilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix size: 2000×2000 (16.0 MB each)\n",
      "\n",
      "Matrix multiplication performance:\n",
      "NumPy:\n",
      "41.5 ms ± 3.84 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "JAX (JIT):\n",
      "25.1 ms ± 763 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "\n",
      "Results match: True\n"
     ]
    }
   ],
   "source": [
    "# Compare JAX vs NumPy for matrix operations\n",
    "size = 2000\n",
    "np_A = np.random.rand(size, size).astype(np.float32)\n",
    "np_B = np.random.rand(size, size).astype(np.float32)\n",
    "\n",
    "jax_A = jnp.array(np_A)\n",
    "jax_B = jnp.array(np_B)\n",
    "\n",
    "print(f\"Matrix size: {size}×{size} ({np_A.nbytes/1e6:.1f} MB each)\")\n",
    "\n",
    "print(\"\\nMatrix multiplication performance:\")\n",
    "print(\"NumPy:\")\n",
    "%timeit np_A @ np_B\n",
    "\n",
    "print(\"JAX (JIT):\")\n",
    "jax_matmul = jax.jit(lambda a, b: a @ b)\n",
    "# Warm up\n",
    "_ = jax_matmul(jax_A, jax_B).block_until_ready()\n",
    "%timeit jax_matmul(jax_A, jax_B).block_until_ready()\n",
    "\n",
    "# Verify results match\n",
    "np_result = np_A @ np_B\n",
    "jax_result = jax_matmul(jax_A, jax_B)\n",
    "print(f\"\\nResults match: {np.allclose(np_result, jax_result, rtol=1e-5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Advanced Example: K-Means Clustering\n",
    "\n",
    "Let's implement K-means clustering to showcase JAX's capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 4998 points in 2D with 3 clusters\n",
      "\n",
      "Running K-means:\n",
      "Iteration  0: max centroid shift = 2.402447\n",
      "Iteration  3: max centroid shift = 0.000000\n",
      "Converged after 3 iterations\n",
      "\n",
      "Final centroids:\n",
      "Cluster 0: [ 1.995,  1.996]\n",
      "Cluster 1: [-1.995,  2.002]\n",
      "Cluster 2: [-0.009, -2.994]\n",
      "\n",
      "True centers:\n",
      "Cluster 0: [ 2.000,  2.000]\n",
      "Cluster 1: [-2.000,  2.000]\n",
      "Cluster 2: [ 0.000, -3.000]\n"
     ]
    }
   ],
   "source": [
    "def kmeans_step(points, centroids):\n",
    "    \"\"\"Single step of K-means: assign points and update centroids\"\"\"\n",
    "    # Compute distances from each point to each centroid\n",
    "    # Using broadcasting: (N, 1, D) - (1, K, D) -> (N, K, D)\n",
    "    distances = jnp.linalg.norm(\n",
    "        points[:, None, :] - centroids[None, :, :], \n",
    "        axis=2\n",
    "    )\n",
    "    \n",
    "    # Assign each point to closest centroid\n",
    "    assignments = jnp.argmin(distances, axis=1)\n",
    "    \n",
    "    # Update centroids\n",
    "    new_centroids = jnp.array([\n",
    "        jnp.mean(points[assignments == k], axis=0)\n",
    "        for k in range(len(centroids))\n",
    "    ])\n",
    "    \n",
    "    return new_centroids, assignments\n",
    "\n",
    "# Generate synthetic clustered data\n",
    "key = jax.random.key(42)\n",
    "n_points, n_dims, n_clusters = 5000, 2, 3\n",
    "\n",
    "# Create 3 clusters\n",
    "cluster_centers = jnp.array([[2, 2], [-2, 2], [0, -3]])\n",
    "points = []\n",
    "for center in cluster_centers:\n",
    "    key, subkey = jax.random.split(key)\n",
    "    cluster_points = center + 0.5 * jax.random.normal(subkey, (n_points//n_clusters, 2))\n",
    "    points.append(cluster_points)\n",
    "\n",
    "points = jnp.vstack(points)\n",
    "print(f\"Generated {len(points)} points in {n_dims}D with {n_clusters} clusters\")\n",
    "\n",
    "# Initialize centroids randomly\n",
    "key, subkey = jax.random.split(key)\n",
    "centroids = jax.random.normal(subkey, (n_clusters, n_dims))\n",
    "\n",
    "# Run K-means\n",
    "print(\"\\nRunning K-means:\")\n",
    "for i in range(20):\n",
    "    new_centroids, assignments = kmeans_step(points, centroids)\n",
    "    \n",
    "    # Check convergence\n",
    "    centroid_shift = jnp.max(jnp.linalg.norm(new_centroids - centroids, axis=1))\n",
    "    centroids = new_centroids\n",
    "    \n",
    "    if i % 5 == 0 or centroid_shift < 1e-4:\n",
    "        print(f\"Iteration {i:2d}: max centroid shift = {centroid_shift:.6f}\")\n",
    "    \n",
    "    if centroid_shift < 1e-4:\n",
    "        print(f\"Converged after {i} iterations\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nFinal centroids:\")\n",
    "for i, centroid in enumerate(centroids):\n",
    "    print(f\"Cluster {i}: [{centroid[0]:6.3f}, {centroid[1]:6.3f}]\")\n",
    "\n",
    "print(f\"\\nTrue centers:\")\n",
    "for i, center in enumerate(cluster_centers):\n",
    "    print(f\"Cluster {i}: [{center[0]:6.3f}, {center[1]:6.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Optimize K-means\n",
    "\n",
    "The current implementation has a Python loop in the centroid update. Make it fully vectorized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results match: True\n",
      "Assignments match: True\n",
      "\n",
      "Performance comparison:\n",
      "Original version:\n",
      "2.15 ms ± 47.6 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Optimized version:\n",
      "1.05 ms ± 13.8 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3: Vectorized K-means centroid update\n",
    "def kmeans_step_optimized(points, centroids):\n",
    "    \"\"\"Fully vectorized K-means step\"\"\"\n",
    "    # Compute distances (same as before)\n",
    "    distances = jnp.linalg.norm(\n",
    "        points[:, None, :] - centroids[None, :, :], \n",
    "        axis=2\n",
    "    )\n",
    "    \n",
    "    # Assign points to clusters\n",
    "    assignments = jnp.argmin(distances, axis=1)\n",
    "    \n",
    "    # TODO: Vectorized centroid update\n",
    "    # Hint: Use jnp.eye() to create one-hot encoding of assignments\n",
    "    # Then use matrix operations to compute means\n",
    "    \n",
    "    # One-hot encode assignments: (N, K)\n",
    "    one_hot = jnp.eye(len(centroids))[assignments]\n",
    "    \n",
    "    # Count points per cluster\n",
    "    cluster_sizes = jnp.sum(one_hot, axis=0, keepdims=True)  # (1, K)\n",
    "    \n",
    "    # Sum points per cluster: (D, K)\n",
    "    cluster_sums = points.T @ one_hot  # (D, N) @ (N, K) = (D, K)\n",
    "    \n",
    "    # Compute new centroids: (K, D)\n",
    "    # Avoid division by zero\n",
    "    cluster_sizes = jnp.maximum(cluster_sizes, 1)\n",
    "    new_centroids = (cluster_sums / cluster_sizes).T\n",
    "    \n",
    "    return new_centroids, assignments\n",
    "\n",
    "# Test both versions\n",
    "centroids_test = jax.random.normal(jax.random.key(0), (n_clusters, n_dims))\n",
    "\n",
    "# Verify they give same results\n",
    "centroids1, assignments1 = kmeans_step(points, centroids_test)\n",
    "centroids2, assignments2 = kmeans_step_optimized(points, centroids_test)\n",
    "\n",
    "print(f\"Results match: {jnp.allclose(centroids1, centroids2, rtol=1e-5)}\")\n",
    "print(f\"Assignments match: {jnp.array_equal(assignments1, assignments2)}\")\n",
    "\n",
    "# Performance comparison\n",
    "print(\"\\nPerformance comparison:\")\n",
    "print(\"Original version:\")\n",
    "%timeit kmeans_step(points, centroids_test)\n",
    "\n",
    "print(\"Optimized version:\")\n",
    "%timeit kmeans_step_optimized(points, centroids_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Performance Analysis and Best Practices\n",
    "\n",
    "### JIT Compilation Overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call (includes compilation):\n",
      "25.8 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "\n",
      "Subsequent calls (compiled):\n",
      "10.2 μs ± 320 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n",
      "\n",
      "Non-JIT version:\n",
      "85.4 μs ± 4.38 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate JIT compilation overhead\n",
    "def simple_computation(x):\n",
    "    return jnp.sum(x**2) + jnp.mean(x)\n",
    "\n",
    "simple_computation_jit = jax.jit(simple_computation)\n",
    "\n",
    "data = jnp.array(np.random.rand(1000))\n",
    "\n",
    "# First call includes compilation time\n",
    "print(\"First call (includes compilation):\")\n",
    "%timeit -n1 -r1 simple_computation_jit(data).block_until_ready()\n",
    "\n",
    "# Subsequent calls are fast\n",
    "print(\"\\nSubsequent calls (compiled):\")\n",
    "%timeit simple_computation_jit(data).block_until_ready()\n",
    "\n",
    "# Compare with non-JIT version\n",
    "print(\"\\nNon-JIT version:\")\n",
    "%timeit simple_computation(data).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When JIT Helps Most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complex computation (many operations):\n",
      "887 μs ± 7.46 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "785 μs ± 3.86 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "\n",
      "Simple operation (single operation):\n",
      "24.2 μs ± 345 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "21.4 μs ± 278 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# JIT is most beneficial for:\n",
    "# 1. Complex computations with many operations\n",
    "# 2. Repeated function calls\n",
    "# 3. Operations that can be fused\n",
    "\n",
    "def complex_computation(x):\n",
    "    \"\"\"Many operations that can be fused by XLA\"\"\"\n",
    "    y = jnp.sin(x)\n",
    "    z = jnp.cos(x**2)\n",
    "    w = jnp.exp(-x)\n",
    "    return jnp.sum(y * z * w) + jnp.mean(y + z + w)\n",
    "\n",
    "def simple_operation(x):\n",
    "    \"\"\"Single operation - less benefit from JIT\"\"\"\n",
    "    return jnp.sum(x)\n",
    "\n",
    "# Create JIT versions\n",
    "complex_jit = jax.jit(complex_computation)\n",
    "simple_jit = jax.jit(simple_operation)\n",
    "\n",
    "data = jnp.array(np.random.rand(100_000))\n",
    "\n",
    "# Warm up\n",
    "_ = complex_jit(data)\n",
    "_ = simple_jit(data)\n",
    "\n",
    "print(\"Complex computation (many operations):\")\n",
    "%timeit complex_computation(data).block_until_ready()\n",
    "%timeit complex_jit(data).block_until_ready()\n",
    "\n",
    "print(\"\\nSimple operation (single operation):\")\n",
    "%timeit simple_operation(data).block_until_ready()\n",
    "%timeit simple_jit(data).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Memory Management and Arrays\n",
    "\n",
    "Understanding JAX's memory model is crucial for performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: [1 2 3 4 5]\n",
      "x + 1: [2 3 4 5 6]\n",
      "x * 2: [ 2  4  6  8 10]\n",
      "Original x unchanged: [1 2 3 4 5]\n",
      "\n",
      "Updated at index 0: [999   2   3   4   5]\n",
      "Original still: [1 2 3 4 5]\n",
      "Updated slice: [  1 888 777   4   5]\n",
      "Added to index 2: [  1   2 103   4   5]\n"
     ]
    }
   ],
   "source": [
    "# JAX arrays are immutable - operations create new arrays\n",
    "x = jnp.array([1, 2, 3, 4, 5])\n",
    "print(f\"Original: {x}\")\n",
    "\n",
    "# These create NEW arrays\n",
    "y = x + 1\n",
    "z = x * 2\n",
    "\n",
    "print(f\"x + 1: {y}\")\n",
    "print(f\"x * 2: {z}\")\n",
    "print(f\"Original x unchanged: {x}\")\n",
    "\n",
    "# For updates, use .at[] syntax\n",
    "x_updated = x.at[0].set(999)\n",
    "print(f\"\\nUpdated at index 0: {x_updated}\")\n",
    "print(f\"Original still: {x}\")\n",
    "\n",
    "# Multiple updates\n",
    "x_multi = x.at[1:3].set(jnp.array([888, 777]))\n",
    "print(f\"Updated slice: {x_multi}\")\n",
    "\n",
    "# Addition to existing values\n",
    "x_added = x.at[2].add(100)\n",
    "print(f\"Added to index 2: {x_added}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: JAX Best Practices\n",
    "\n",
    "### ✅ Do:\n",
    "- Use `@jax.jit` for repeated complex computations\n",
    "- Use `vmap` instead of Python loops\n",
    "- Manage random keys explicitly with `split()`\n",
    "- Use `.block_until_ready()` for accurate timing\n",
    "- Prefer functional programming patterns\n",
    "- Use autodiff (`grad`) for optimization\n",
    "\n",
    "### ❌ Don't:\n",
    "- JIT simple single operations (overhead > benefit)\n",
    "- Use Python loops for array operations\n",
    "- Rely on global random state\n",
    "- Mutate arrays (use `.at[]` syntax)\n",
    "- Ignore compilation overhead in timing\n",
    "\n",
    "### JAX vs NumPy Performance:\n",
    "- **Simple operations**: NumPy often faster (less overhead)\n",
    "- **Complex operations**: JAX wins with fusion and optimization\n",
    "- **Repeated computations**: JAX dominates after JIT compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX Performance Showcase\n",
      "========================================\n",
      "Analyzed 10000×50 matrix in 0.0100 seconds\n",
      "Condition number: 1.31+0.00j\n",
      "Max eigenvalue: 1.1396+0.0000j\n"
     ]
    }
   ],
   "source": [
    "# Final performance showcase\n",
    "print(\"JAX Performance Showcase\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Large-scale computation\n",
    "n = 10000\n",
    "key = jax.random.key(42)\n",
    "X = jax.random.normal(key, (n, 50))\n",
    "\n",
    "@jax.jit\n",
    "def complex_analysis(X):\n",
    "    # Covariance matrix\n",
    "    cov = (X.T @ X) / len(X)\n",
    "    # Eigenvalues\n",
    "    eigenvals = jnp.linalg.eigvals(cov)\n",
    "    # Statistics\n",
    "    return {\n",
    "        'mean': jnp.mean(X, axis=0),\n",
    "        'std': jnp.std(X, axis=0),\n",
    "        'max_eigenval': jnp.max(eigenvals),\n",
    "        'condition_number': jnp.max(eigenvals) / jnp.min(eigenvals)\n",
    "    }\n",
    "\n",
    "# Warm up\n",
    "_ = complex_analysis(X)\n",
    "\n",
    "# Time the analysis\n",
    "import time\n",
    "start = time.time()\n",
    "result = complex_analysis(X)\n",
    "for key in result:\n",
    "    _ = result[key].block_until_ready()\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Analyzed {n}×50 matrix in {end-start:.4f} seconds\")\n",
    "print(f\"Condition number: {result['condition_number']:.2f}\")\n",
    "print(f\"Max eigenvalue: {result['max_eigenval']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
