{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Numpy Slide](img/01_Numpy/Folie1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Numpy Slide](img/01_Numpy/Folie2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Numpy Slide](img/01_Numpy/Folie3.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Numpy Slide](img/01_Numpy/Folie4.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array size: 1000000 elements\n",
      "Expected size (8 bytes per element): 8000000 bytes\n",
      "Python list memory: 8000056 bytes. (Overhead: 56 bytes)\n",
      "NumPy sys memory: 8000112 bytes. (Overhead: 112 bytes)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# Setup for performance comparison\n",
    "size = 1_000_000\n",
    "python_list = list(range(size))\n",
    "numpy_array = np.arange(size)\n",
    "\n",
    "print(f\"Array size: {size} elements\")\n",
    "print(f\"Expected size (8 bytes per element): {8 * size} bytes\")\n",
    "\n",
    "print(f\"Python list memory: {sys.getsizeof(python_list)} bytes. (Overhead: {sys.getsizeof(python_list) - (8 * size)} bytes)\")\n",
    "print(f\"NumPy sys memory: {sys.getsizeof(numpy_array)} bytes. (Overhead: {sys.getsizeof(numpy_array) - (8 * size)} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each element takes 8 bytes with the python list having a very tiny overhead of 56 bytes compared to the overhead of the NumPy array which clocks in at 112 bytes. Still, both overheads are negligible when working with big arrays. \n",
    "\n",
    "> *Note*: You can also get the size (in bytes) of the numpy array without the overhead by using the `.nbytes` property\n",
    "\n",
    "Next, we compare a basic multiplication on the list compared to the numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python list comprehension: 0.0376 seconds\n",
      "NumPy vectorized operation: 0.0025 seconds\n",
      "NumPy is 14.9x faster!\n"
     ]
    }
   ],
   "source": [
    "# Performance comparison: Element-wise multiplication\n",
    "def python_multiply(data):\n",
    "    return [x * 2 for x in data]\n",
    "\n",
    "def numpy_multiply(data):\n",
    "    return data * 2\n",
    "\n",
    "# Time Python approach\n",
    "start = time.time()\n",
    "python_result = python_multiply(python_list)\n",
    "python_time = time.time() - start\n",
    "\n",
    "# Time NumPy approach\n",
    "start = time.time()\n",
    "numpy_result = numpy_multiply(numpy_array)\n",
    "numpy_time = time.time() - start\n",
    "\n",
    "print(f\"Python list comprehension: {python_time:.4f} seconds\")\n",
    "print(f\"NumPy vectorized operation: {numpy_time:.4f} seconds\")\n",
    "print(f\"NumPy is {python_time / numpy_time:.1f}x faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, that's technically not the best way to measure performance. Measuring performance should be done with a dedicated library that measures the same code multiple times and averages the runtime while also reporting the standard deviation. For Python, one such library is `timeit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.9 ms ± 709 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "525 μs ± 19.2 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit python_multiply(python_list)\n",
    "%timeit numpy_multiply(numpy_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can say with confidence, that numpy is much, much faster than raw python!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 NumPy Array Internals\n",
    "\n",
    "Understanding NumPy's internal structure is crucial for writing high-performance code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `ndarray` Structure\n",
    "\n",
    "Every NumPy array consists of:\n",
    "1. **Data buffer**: Raw binary data\n",
    "2. **Metadata**: Shape, strides, dtype, etc.\n",
    "3. **View information**: How to interpret the data buffer\n",
    "\n",
    "![Numpy Slide](img/01_Numpy/Folie5.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array:\n",
      "[[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]\n",
      " [12 13 14 15]]\n",
      "\n",
      "Shape: (4, 4)\n",
      "Data type: int8\n",
      "Strides: (4, 1) bytes\n",
      "Total size: 16 bytes\n",
      "Is C-contiguous: True\n",
      "Is Fortran-contiguous: False\n"
     ]
    }
   ],
   "source": [
    "# Create a 2D array to explore its internals\n",
    "x = np.array([[0, 1, 2, 3],\n",
    "              [4, 5, 6, 7],\n",
    "              [8, 9, 10, 11],\n",
    "              [12, 13, 14, 15]], dtype=np.int8)\n",
    "\n",
    "print(\"Array:\")\n",
    "print(x)\n",
    "print(f\"\\nShape: {x.shape}\")\n",
    "print(f\"Data type: {x.dtype}\")\n",
    "print(f\"Strides: {x.strides} bytes\")\n",
    "print(f\"Total size: {x.nbytes} bytes\")\n",
    "print(f\"Is C-contiguous: {x.flags.c_contiguous}\")\n",
    "print(f\"Is Fortran-contiguous: {x.flags.f_contiguous}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Strides - The Key to Performance\n",
    "\n",
    "**Strides** tell NumPy how many bytes to skip to move to the next element along each axis.\n",
    "\n",
    "For our array with `dtype=int8` (1 byte per element):\n",
    "- To move to next column: skip 1 byte\n",
    "- To move to next row: skip 4 bytes (entire row)\n",
    "\n",
    "**Formula**: `element[i,j] = data_ptr + i*stride[0] + j*stride[1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array shape: (4, 4)\n",
      "Strides: (4, 1)\n",
      "Element size: 1 bytes\n",
      "Expected strides: (4, 1)\n",
      "\n",
      "Memory layout (conceptual):\n",
      "  0  1  2  3  |   4  5  6  7  |   8  9 10 11  |  12 13 14 15\n"
     ]
    }
   ],
   "source": [
    "# Visualize how strides work\n",
    "def explain_strides(arr):\n",
    "    print(f\"Array shape: {arr.shape}\")\n",
    "    print(f\"Strides: {arr.strides}\")\n",
    "    print(f\"Element size: {arr.itemsize} bytes\")\n",
    "    \n",
    "    # Calculate expected strides\n",
    "    expected_col_stride = arr.itemsize\n",
    "    expected_row_stride = arr.shape[1] * arr.itemsize\n",
    "    print(f\"Expected strides: ({expected_row_stride}, {expected_col_stride})\")\n",
    "    \n",
    "    # Show memory layout\n",
    "    print(\"\\nMemory layout (conceptual):\")\n",
    "    flat_view = arr.ravel()\n",
    "    for i in range(len(flat_view)):\n",
    "        if i % arr.shape[1] == 0 and i > 0:\n",
    "            print(f\"  |  {flat_view[i]:2d}\", end=\"\")\n",
    "        else:\n",
    "            print(f\" {flat_view[i]:2d}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "explain_strides(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Predict the Strides\n",
    "\n",
    "For each transformation below, predict the strides before running the code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original array (4, 4) with strides (4, 1)\n",
      "\n",
      "==================================================\n",
      "Reshaped to (2, 8): strides = (8, 1)\n",
      "[[ 0  1  2  3  4  5  6  7]\n",
      " [ 8  9 10 11 12 13 14 15]]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1.1: Reshape to (2, 8)\n",
    "print(\"Original array (4, 4) with strides\", x.strides)\n",
    "y = x.reshape((2, 8))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Reshaped to {y.shape}: strides = {y.strides}\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped to (1, 16): strides = (16, 1)\n",
      "[[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1.2: Reshape to (1, 16) \n",
    "z = x.reshape((1, 16))\n",
    "print(f\"Reshaped to {z.shape}: strides = {z.strides}\")\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "With int16 dtype: strides = (8, 2)\n",
      "Compare with int8: strides = (4, 1)\n",
      "Notice how strides scale with element size!\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1.3: Different data type\n",
    "a = np.array(x, dtype=np.int16)  # 2 bytes per element\n",
    "\n",
    "print(f\"\\nWith int16 dtype: strides = {a.strides}\")\n",
    "print(f\"Compare with int8: strides = {x.strides}\")\n",
    "print(\"Notice how strides scale with element size!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Views vs Copies: Memory Management Deep Dive\n",
    "\n",
    "It is important for HPC performance to understand when operations create new memory vs when they just change metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original array:\n",
      "[[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]]\n",
      "Original base: [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "\n",
      "After ravel() - same base object? False\n",
      "Shares memory? True\n",
      "\n",
      "After flatten() - same base object? False\n",
      "Shares memory? False\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate view vs copy with ravel() and flatten()\n",
    "x = np.arange(12).reshape(3, 4)\n",
    "print(\"Original array:\")\n",
    "print(x)\n",
    "print(f\"Original base: {x.base}\")\n",
    "\n",
    "# ravel() tries to return a view (metadata change only)\n",
    "y_ravel = x.ravel()\n",
    "print(f\"\\nAfter ravel() - same base object? {y_ravel.base is x}\")\n",
    "print(f\"Shares memory? {np.shares_memory(x, y_ravel)}\")\n",
    "\n",
    "# flatten() always returns a copy (new memory)\n",
    "y_flatten = x.flatten()\n",
    "print(f\"\\nAfter flatten() - same base object? {y_flatten.base is x}\")\n",
    "print(f\"Shares memory? {np.shares_memory(x, y_flatten)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original x: [0 1 2 3 4]\n",
      "After modifying ravel view: [999   1   2   3   4]\n",
      "After modifying flatten copy: [0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "# Consequence: modifying views affects original\n",
    "x = np.arange(5)\n",
    "print(f\"Original x: {x}\")\n",
    "\n",
    "y_ravel = x.ravel()  # Creates a view\n",
    "y_ravel[0] = 999     # Modify the view\n",
    "print(f\"After modifying ravel view: {x}\")\n",
    "\n",
    "# Reset\n",
    "x = np.arange(5)\n",
    "y_flatten = x.flatten()  # Creates a copy\n",
    "y_flatten[0] = 999       # Modify the copy\n",
    "print(f\"After modifying flatten copy: {x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Impact: When Views Become Copies\n",
    "\n",
    "Sometimes operations that normally create views are forced to create copies due to memory layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array size: 200.0 MB\n",
      "50.8 ns ± 2.84 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)\n",
      "45 ns ± 3.12 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Create a large array for timing\n",
    "x = np.random.rand(5000, 5000)\n",
    "print(f\"Array size: {x.nbytes / 1e6:.1f} MB\")\n",
    "\n",
    "# Case 1: Simple operations (fast - metadata only)\n",
    "%timeit x.T          # Transpose - just swap strides\n",
    "%timeit x.ravel()    # Ravel of C-contiguous array - view possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.4 ms ± 1.23 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "30.6 ms ± 1.06 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Case 2: Transpose + ravel (slow - copy required)\n",
    "%timeit x.T.ravel()  # Can't create view of non-contiguous transpose\n",
    "\n",
    "# Case 3: Flatten (always slow - always copies)\n",
    "%timeit x.flatten()  # Always creates copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insight**: The transpose `x.T` creates a view with modified strides, but it's no longer C-contiguous. When we then `ravel()` it, NumPy can't express the flattened result with simple strides, so it must copy the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.flags.c_contiguous: True\n",
      "x.T.flags.c_contiguous: False\n",
      "x.T.flags.f_contiguous: True\n"
     ]
    }
   ],
   "source": [
    "# Verify the contiguity issue\n",
    "print(f\"x.flags.c_contiguous: {x.flags.c_contiguous}\")\n",
    "print(f\"x.T.flags.c_contiguous: {x.T.flags.c_contiguous}\")\n",
    "print(f\"x.T.flags.f_contiguous: {x.T.flags.f_contiguous}\")\n",
    "\n",
    "# This is why x.T.ravel() is slow - it requires copying to make contiguous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Array Creation and Basic Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient Array Creation Methods\n",
    "\n",
    "Different creation methods have different performance characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array creation timing:\n",
      "183 μs ± 6.44 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "295 ns ± 12.4 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n",
      "220 μs ± 20.3 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "216 μs ± 10.6 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "236 μs ± 20.4 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Compare different array creation methods\n",
    "size = (1000, 1000)\n",
    "\n",
    "print(\"Array creation timing:\")\n",
    "%timeit np.zeros(size)           # Pre-allocated, initialized to 0\n",
    "%timeit np.empty(size)           # Pre-allocated, uninitialized (fastest)\n",
    "%timeit np.ones(size)            # Pre-allocated, initialized to 1\n",
    "%timeit np.full(size, 3.14)      # Pre-allocated, initialized to value\n",
    "%timeit np.arange(size[0] * size[1]).reshape(size)  # Sequential, then reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Types and Memory Usage\n",
    "\n",
    "Choosing the right data type can significantly impact memory usage and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage by data type:\n",
      "Data Type   \t\tBytes/element   Total MB  \n",
      "--------------------------------------------------\n",
      "<class 'numpy.int8'>\t1               5.0       \n",
      "<class 'numpy.int16'>\t2               10.0      \n",
      "<class 'numpy.int32'>\t4               20.0      \n",
      "<class 'numpy.int64'>\t8               40.0      \n",
      "<class 'numpy.float32'>\t4               20.0      \n",
      "<class 'numpy.float64'>\t8               40.0      \n"
     ]
    }
   ],
   "source": [
    "# Memory usage comparison for different dtypes\n",
    "size = 5_000_000\n",
    "dtypes = [np.int8, np.int16, np.int32, np.int64, np.float32, np.float64]\n",
    "\n",
    "print(\"Memory usage by data type:\")\n",
    "print(f\"{'Data Type':<12}\\t\\t{'Bytes/element':<15} {'Total MB':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "base_memory = None\n",
    "for dtype in dtypes:\n",
    "    arr = np.ones(size, dtype=dtype)\n",
    "    memory_mb = arr.nbytes / 1e6\n",
    "    if base_memory is None:\n",
    "        base_memory = memory_mb\n",
    "    \n",
    "    print(f\"{str(dtype):<12}\\t{arr.itemsize:<15} {memory_mb:<10.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Memory-Efficient Array Operations\n",
    "\n",
    "Create arrays with different dtypes and observe the memory and performance implications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float32 array: 20.0 MB\n",
      "Float64 array: 40.0 MB\n",
      "\n",
      "Performance comparison for sum operation:\n",
      "773 μs ± 69.7 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "1.55 ms ± 61.3 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "\n",
      "Performance comparison for element-wise multiplication:\n",
      "1.27 ms ± 55 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "6.15 ms ± 126 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Exercise: Compare performance of operations on different dtypes\n",
    "size = 5_000_000\n",
    "\n",
    "# Create arrays with different precisions\n",
    "arr_f32 = np.random.rand(size).astype(np.float32)\n",
    "arr_f64 = np.random.rand(size).astype(np.float64)\n",
    "\n",
    "print(f\"Float32 array: {arr_f32.nbytes / 1e6:.1f} MB\")\n",
    "print(f\"Float64 array: {arr_f64.nbytes / 1e6:.1f} MB\")\n",
    "\n",
    "print(\"\\nPerformance comparison for sum operation:\")\n",
    "%timeit np.sum(arr_f32)\n",
    "%timeit np.sum(arr_f64)\n",
    "\n",
    "print(\"\\nPerformance comparison for element-wise multiplication:\")\n",
    "%timeit arr_f32 * 2.0\n",
    "%timeit arr_f64 * 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Session 2: Broadcasting and Vectorization\n",
    "\n",
    "## 2.1 Understanding Broadcasting - The Heart of NumPy Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting Rules\n",
    "\n",
    "Broadcasting allows NumPy to perform operations on arrays with different shapes efficiently. The rules are:\n",
    "\n",
    "1. **Start from the trailing dimension** and work backward\n",
    "2. **Dimensions are compatible** if:\n",
    "   - They are equal, OR\n",
    "   - One of them is 1, OR  \n",
    "   - One of them is missing (treated as 1)\n",
    "3. **Result shape** is the maximum size along each dimension\n",
    "\n",
    "```\n",
    "Examples:\n",
    "A: (5, 4)     B: (4,)       → Result: (5, 4)  ✓\n",
    "A: (5, 1)     B: (1, 4)     → Result: (5, 4)  ✓\n",
    "A: (5, 4)     B: (3,)       → Result: Error   ✗\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Array + scalar\n",
      "[1,2,3,4,5] + 10 = [11 12 13 14 15]\n",
      "Shapes: (5,) + scalar → (5,)\n",
      "\n",
      "Example 2: 2D array + 1D array\n",
      "Array b shape: (2, 3)\n",
      "Array c shape: (3,)\n",
      "Result shape: (2, 3)\n",
      "Result:\n",
      "[[11 22 33]\n",
      " [14 25 36]]\n"
     ]
    }
   ],
   "source": [
    "# Basic broadcasting examples\n",
    "print(\"Example 1: Array + scalar\")\n",
    "a = np.array([1, 2, 3, 4, 5])\n",
    "result = a + 10\n",
    "print(f\"[1,2,3,4,5] + 10 = {result}\")\n",
    "print(f\"Shapes: {a.shape} + scalar → {result.shape}\")\n",
    "\n",
    "print(\"\\nExample 2: 2D array + 1D array\")\n",
    "b = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])\n",
    "c = np.array([10, 20, 30])\n",
    "result = b + c\n",
    "print(f\"Array b shape: {b.shape}\")\n",
    "print(f\"Array c shape: {c.shape}\")\n",
    "print(f\"Result shape: {result.shape}\")\n",
    "print(\"Result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating New Dimensions with `np.newaxis`\n",
    "\n",
    "Often we need to add dimensions to enable broadcasting. `np.newaxis` is an alias for `None` and adds a dimension of size 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original x: [0 1 2 3 4]\n",
      "Shape: (5,)\n",
      "\n",
      "Column vector: \n",
      "[[0]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "Shape: (5, 1)\n",
      "\n",
      "Row vector: \n",
      "[[0 1 2 3 4]]\n",
      "Shape: (1, 5)\n",
      "\n",
      "Memory sharing - original and column: True\n",
      "Memory sharing - original and row: True\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate np.newaxis\n",
    "x = np.arange(5)\n",
    "print(f\"Original x: {x}\")\n",
    "print(f\"Shape: {x.shape}\")\n",
    "\n",
    "# Convert to column vector\n",
    "x_col = x[:, np.newaxis]\n",
    "print(f\"\\nColumn vector: \\n{x_col}\")\n",
    "print(f\"Shape: {x_col.shape}\")\n",
    "\n",
    "# Convert to row vector  \n",
    "x_row = x[np.newaxis, :]\n",
    "print(f\"\\nRow vector: \\n{x_row}\")\n",
    "print(f\"Shape: {x_row.shape}\")\n",
    "\n",
    "# Verify these are views (no memory copying)\n",
    "print(f\"\\nMemory sharing - original and column: {np.shares_memory(x, x_col)}\")\n",
    "print(f\"Memory sharing - original and row: {np.shares_memory(x, x_row)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Broadcasting Puzzle\n",
    "\n",
    "Predict the output shapes and results before running!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What will be the shape and result of x[:, newaxis] * x?\n",
      "\n",
      "Result shape: (5, 5)\n",
      "Multiplication table:\n",
      "[[ 1  2  3  4  5]\n",
      " [ 2  4  6  8 10]\n",
      " [ 3  6  9 12 15]\n",
      " [ 4  8 12 16 20]\n",
      " [ 5 10 15 20 25]]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.1: Create a multiplication table\n",
    "x = np.arange(1, 6)  # [1, 2, 3, 4, 5]\n",
    "\n",
    "print(\"What will be the shape and result of x[:, newaxis] * x?\")\n",
    "\n",
    "multiplication_table = x[:, np.newaxis] * x\n",
    "print(f\"\\nResult shape: {multiplication_table.shape}\")\n",
    "print(\"Multiplication table:\")\n",
    "print(multiplication_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array a shape: (2, 3, 1)\n",
      "Array b shape: (4,)\n",
      "What will be the result shape of a + b?\n",
      "\n",
      "Actual result shape: (2, 3, 4)\n",
      "This creates all combinations of elements!\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.2: 3D broadcasting\n",
    "a = np.random.rand(2, 3, 1)  # Shape (2, 3, 1)\n",
    "b = np.random.rand(4)        # Shape (4,)\n",
    "\n",
    "print(f\"Array a shape: {a.shape}\")\n",
    "print(f\"Array b shape: {b.shape}\")\n",
    "print(\"What will be the result shape of a + b?\")\n",
    "\n",
    "result = a + b\n",
    "print(f\"\\nActual result shape: {result.shape}\")\n",
    "print(\"This creates all combinations of elements!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Advanced Broadcasting Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise Operations on Vector Collections\n",
    "\n",
    "A common HPC pattern: given N vectors, compute some pairwise operation between all pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vectors (6 vectors of dimension 3):\n",
      "[[0.37454012 0.95071431 0.73199394]\n",
      " [0.59865848 0.15601864 0.15599452]\n",
      " [0.05808361 0.86617615 0.60111501]\n",
      " [0.70807258 0.02058449 0.96990985]\n",
      " [0.83244264 0.21233911 0.18182497]\n",
      " [0.18340451 0.30424224 0.52475643]]\n",
      "Shape: (6, 3)\n"
     ]
    }
   ],
   "source": [
    "# Generate sample data: 6 vectors of dimension 3\n",
    "np.random.seed(42)  # For reproducible results\n",
    "vectors = np.random.rand(6, 3)\n",
    "print(\"Input vectors (6 vectors of dimension 3):\")\n",
    "print(vectors)\n",
    "print(f\"Shape: {vectors.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairwise differences shape: (6, 6, 3)\n",
      "\n",
      "Example: Difference between vector 0 and vector 1:\n",
      "Vector 0: [0.37454012 0.95071431 0.73199394]\n",
      "Vector 1: [0.59865848 0.15601864 0.15599452]\n",
      "Difference: [-0.22411837  0.79469567  0.57599942]\n",
      "Manual check: [-0.22411837  0.79469567  0.57599942]\n",
      "\n",
      "Diagonal elements (should be ~0): [0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Compute all pairwise differences using broadcasting\n",
    "# Goal: diff[i,j,k] = vectors[i,k] - vectors[j,k]\n",
    "\n",
    "# Method: Use broadcasting to create (6,1,3) - (1,6,3) → (6,6,3)\n",
    "pairwise_diff = vectors[:, np.newaxis, :] - vectors[np.newaxis, :, :]\n",
    "\n",
    "print(f\"Pairwise differences shape: {pairwise_diff.shape}\")\n",
    "print(\"\\nExample: Difference between vector 0 and vector 1:\")\n",
    "print(f\"Vector 0: {vectors[0]}\")\n",
    "print(f\"Vector 1: {vectors[1]}\")\n",
    "print(f\"Difference: {pairwise_diff[0, 1]}\")\n",
    "print(f\"Manual check: {vectors[0] - vectors[1]}\")\n",
    "\n",
    "# Verify diagonal is zero (vector - itself)\n",
    "print(f\"\\nDiagonal elements (should be ~0): {np.diag(pairwise_diff[:,:,0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Usage Analysis\n",
    "\n",
    "Let's understand the memory implications of this broadcasting operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 6 vectors of dimension 3\n",
      "Input memory: 144 bytes (0.1 KB)\n",
      "Output memory: 864 bytes (0.8 KB)\n",
      "Memory expansion: 6.0x\n",
      "\n",
      "For N=6 vectors, we create N×N×dim = 6×6×3 = 108 elements\n",
      "This scales as O(N²) - can become memory-intensive for large N!\n"
     ]
    }
   ],
   "source": [
    "# Analyze memory usage\n",
    "n_vectors, dim = vectors.shape\n",
    "input_memory = vectors.nbytes\n",
    "output_memory = pairwise_diff.nbytes\n",
    "\n",
    "print(f\"Input: {n_vectors} vectors of dimension {dim}\")\n",
    "print(f\"Input memory: {input_memory} bytes ({input_memory/1024:.1f} KB)\")\n",
    "print(f\"Output memory: {output_memory} bytes ({output_memory/1024:.1f} KB)\")\n",
    "print(f\"Memory expansion: {output_memory/input_memory:.1f}x\")\n",
    "\n",
    "print(f\"\\nFor N={n_vectors} vectors, we create N×N×dim = {n_vectors}×{n_vectors}×{dim} = {n_vectors**2 * dim} elements\")\n",
    "print(f\"This scales as O(N²) - can become memory-intensive for large N!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Vectorization Strategies\n",
    "\n",
    "### The Golden Rule: Eliminate Python Loops\n",
    "\n",
    "Let's see how to replace common loop patterns with vectorized operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance comparison:\n",
      "91.8 ms ± 7.62 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "1.03 ms ± 62.1 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "\n",
      "Results are equal: True\n"
     ]
    }
   ],
   "source": [
    "# Example: Apply a function to each element\n",
    "data = np.random.rand(100000)\n",
    "\n",
    "# BAD: Python loop\n",
    "def apply_function_loop(x):\n",
    "    result = np.empty_like(x)\n",
    "    for i in range(len(x)):\n",
    "        result[i] = np.exp(x[i]) + np.sin(x[i])\n",
    "    return result\n",
    "\n",
    "# GOOD: Vectorized\n",
    "def apply_function_vectorized(x):\n",
    "    return np.exp(x) + np.sin(x)\n",
    "\n",
    "# Performance comparison\n",
    "print(\"Performance comparison:\")\n",
    "%timeit apply_function_loop(data)\n",
    "%timeit apply_function_vectorized(data)\n",
    "\n",
    "# Verify they give the same result\n",
    "result1 = apply_function_loop(data[:100])  # Use smaller array for speed\n",
    "result2 = apply_function_vectorized(data[:100])\n",
    "print(f\"\\nResults are equal: {np.allclose(result1, result2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Vectorization Challenge\n",
    "\n",
    "Convert the following loop-based operations to vectorized NumPy operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.1: Conditional operations\n",
    "# Loop version: Set values > 0.5 to their square, others to 0\n",
    "data = np.random.rand(1000)\n",
    "\n",
    "def conditional_loop(x):\n",
    "    result = np.empty_like(x)\n",
    "    for i in range(len(x)):\n",
    "        if x[i] > 0.5:\n",
    "            result[i] = x[i] ** 2\n",
    "        else:\n",
    "            result[i] = 0.0\n",
    "    return result\n",
    "\n",
    "# TODO: Implement vectorized version\n",
    "def conditional_vectorized(x):\n",
    "    # Hint: Use np.where() or boolean indexing\n",
    "    pass\n",
    "\n",
    "# Uncomment to test:\n",
    "#result_loop = conditional_loop(data)\n",
    "#result_vectorized = conditional_vectorized(data)\n",
    "#print(f\"Results match: {np.allclose(result_loop, result_vectorized)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop vs where(): True\n",
      "Loop vs indexing: True\n",
      "\n",
      "Performance comparison:\n",
      "135 μs ± 8.53 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "2.52 μs ± 72.4 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n",
      "5.84 μs ± 136 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Solution for Exercise 4.1\n",
    "def conditional_vectorized(x):\n",
    "    return np.where(x > 0.5, x**2, 0.0)\n",
    "\n",
    "# Alternative using boolean indexing\n",
    "def conditional_vectorized_alt(x):\n",
    "    result = np.zeros_like(x)\n",
    "    mask = x > 0.5\n",
    "    result[mask] = x[mask] ** 2\n",
    "    return result\n",
    "\n",
    "# Test and compare performance\n",
    "result_loop = conditional_loop(data)\n",
    "result_vec1 = conditional_vectorized(data)\n",
    "result_vec2 = conditional_vectorized_alt(data)\n",
    "\n",
    "print(f\"Loop vs where(): {np.allclose(result_loop, result_vec1)}\")\n",
    "print(f\"Loop vs indexing: {np.allclose(result_loop, result_vec2)}\")\n",
    "\n",
    "print(\"\\nPerformance comparison:\")\n",
    "%timeit conditional_loop(data)\n",
    "%timeit conditional_vectorized(data)\n",
    "%timeit conditional_vectorized_alt(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results match: True\n",
      "\n",
      "Performance on full matrix:\n",
      "489 ms ± 8.35 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "629 μs ± 5.57 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4.2: Cumulative operations\n",
    "# Convert nested loop to vectorized operation\n",
    "matrix = np.random.rand(500, 500)\n",
    "\n",
    "# Loop version: Compute row-wise running averages\n",
    "def running_average_loop(mat):\n",
    "    result = np.empty_like(mat)\n",
    "    for i in range(mat.shape[0]):\n",
    "        for j in range(mat.shape[1]):\n",
    "            result[i, j] = np.mean(mat[i, :j+1])\n",
    "    return result\n",
    "\n",
    "# Vectorized version using cumsum\n",
    "def running_average_vectorized(mat):\n",
    "    cumsum = np.cumsum(mat, axis=1)\n",
    "    indices = np.arange(1, mat.shape[1] + 1)\n",
    "    return cumsum / indices\n",
    "\n",
    "# Test on smaller matrix for verification\n",
    "small_mat = matrix[:5, :5]\n",
    "result_loop = running_average_loop(small_mat)\n",
    "result_vec = running_average_vectorized(small_mat)\n",
    "\n",
    "print(f\"Results match: {np.allclose(result_loop, result_vec)}\")\n",
    "print(\"\\nPerformance on full matrix:\")\n",
    "%timeit running_average_loop(matrix)\n",
    "%timeit running_average_vectorized(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Session 3: Optimization and Real-World Applications\n",
    "\n",
    "## 3.1 Euclidean Distance Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Problem\n",
    "\n",
    "Given N vectors in D-dimensional space, compute the matrix of Euclidean distances between all pairs:\n",
    "\n",
    "$$d_{ij} = \\sqrt{\\sum_{k=1}^{D} (x_{ik} - x_{jk})^2}$$\n",
    "\n",
    "This is fundamental in:\n",
    "- Machine learning (k-NN, clustering)\n",
    "- Molecular dynamics simulations\n",
    "- Computer graphics\n",
    "- Scientific computing\n",
    "\n",
    "We'll implement and compare multiple approaches, analyzing their trade-offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 1000 points in 50D space\n",
      "Input size: 0.4 MB\n",
      "Output will be: 1000×1000 = 1,000,000 distances\n",
      "Output size: 8.0 MB (float64)\n"
     ]
    }
   ],
   "source": [
    "# Generate test data\n",
    "np.random.seed(42)\n",
    "n_points = 1000\n",
    "n_features = 50\n",
    "X = np.random.rand(n_points, n_features) * 10.0\n",
    "\n",
    "print(f\"Dataset: {n_points} points in {n_features}D space\")\n",
    "print(f\"Input size: {X.nbytes / 1e6:.1f} MB\")\n",
    "print(f\"Output will be: {n_points}×{n_points} = {n_points**2:,} distances\")\n",
    "print(f\"Output size: {(n_points**2 * 8) / 1e6:.1f} MB (float64)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: Broadcasting (Intuitive but Memory-Intensive)\n",
    "\n",
    "The most straightforward approach uses broadcasting to compute all pairwise differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small test result shape: (10, 10)\n",
      "\n",
      "First few distances:\n",
      "[[ 0.          8.43172737  9.81770581]\n",
      " [ 8.43172737  0.         12.39060453]\n",
      " [ 9.81770581 12.39060453  0.        ]]\n",
      "\n",
      "Diagonal (should be ~0):\n",
      "[0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "def euclidean_broadcast(x, y):\n",
    "    \"\"\"\n",
    "    Euclidean distance matrix using broadcasting.\n",
    "    \n",
    "    Args:\n",
    "        x: (N, D) array of N vectors in D dimensions\n",
    "        y: (M, D) array of M vectors in D dimensions\n",
    "        \n",
    "    Returns:\n",
    "        (N, M) array of distances\n",
    "    \"\"\"\n",
    "    # Shape: (N, 1, D) - (1, M, D) → (N, M, D)\n",
    "    diff = x[:, np.newaxis, :] - y[np.newaxis, :, :]\n",
    "    \n",
    "    # Sum of squares along last dimension\n",
    "    return np.sqrt((diff * diff).sum(axis=2))\n",
    "\n",
    "# Test with small dataset first\n",
    "X_small = X[:10, :5]\n",
    "distances_broadcast = euclidean_broadcast(X_small, X_small)\n",
    "\n",
    "print(f\"Small test result shape: {distances_broadcast.shape}\")\n",
    "print(\"\\nFirst few distances:\")\n",
    "print(distances_broadcast[:3, :3])\n",
    "print(\"\\nDiagonal (should be ~0):\")\n",
    "print(np.diag(distances_broadcast)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Analysis of Broadcasting Approach\n",
    "\n",
    "Let's understand the memory requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 100 points in 10D:\n",
      "Input memory:             0.0 MB\n",
      "Intermediate memory:      0.8 MB (diff array)\n",
      "Output memory:            0.1 MB\n",
      "Peak memory usage:        0.9 MB\n",
      "Memory amplification: 100.0x\n",
      "\n",
      "For 500 points in 20D:\n",
      "Input memory:             0.1 MB\n",
      "Intermediate memory:     40.0 MB (diff array)\n",
      "Output memory:            2.0 MB\n",
      "Peak memory usage:       42.1 MB\n",
      "Memory amplification: 500.0x\n",
      "\n",
      "For 1000 points in 50D:\n",
      "Input memory:             0.4 MB\n",
      "Intermediate memory:    400.0 MB (diff array)\n",
      "Output memory:            8.0 MB\n",
      "Peak memory usage:      408.4 MB\n",
      "Memory amplification: 1000.0x\n",
      "\n",
      "For 2000 points in 100D:\n",
      "Input memory:             1.6 MB\n",
      "Intermediate memory:   3200.0 MB (diff array)\n",
      "Output memory:           32.0 MB\n",
      "Peak memory usage:     3233.6 MB\n",
      "Memory amplification: 2000.0x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def analyze_memory_usage(n_points, n_features):\n",
    "    input_size = n_points * n_features * 8  # float64\n",
    "    intermediate_size = n_points * n_points * n_features * 8  # diff array\n",
    "    output_size = n_points * n_points * 8  # distance matrix\n",
    "    \n",
    "    print(f\"For {n_points} points in {n_features}D:\")\n",
    "    print(f\"Input memory:        {input_size / 1e6:8.1f} MB\")\n",
    "    print(f\"Intermediate memory: {intermediate_size / 1e6:8.1f} MB (diff array)\")\n",
    "    print(f\"Output memory:       {output_size / 1e6:8.1f} MB\")\n",
    "    print(f\"Peak memory usage:   {(input_size + intermediate_size + output_size) / 1e6:8.1f} MB\")\n",
    "    print(f\"Memory amplification: {intermediate_size / input_size:.1f}x\\n\")\n",
    "\n",
    "# Analyze different problem sizes\n",
    "sizes = [(100, 10), (500, 20), (1000, 50), (2000, 100)]\n",
    "for n, d in sizes:\n",
    "    analyze_memory_usage(n, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: Mathematical Optimization (The \"Trick\")\n",
    "\n",
    "We can avoid the large intermediate array using the algebraic identity:\n",
    "\n",
    "$$\\|x_i - x_j\\|^2 = \\|x_i\\|^2 + \\|x_j\\|^2 - 2 x_i \\cdot x_j$$\n",
    "\n",
    "This reduces memory usage and can be faster due to optimized BLAS operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results match: True\n",
      "Max difference: 3.37e-07\n"
     ]
    }
   ],
   "source": [
    "def euclidean_trick(x, y):\n",
    "    \"\"\"\n",
    "    Euclidean distance matrix using the algebraic trick.\n",
    "    \n",
    "    Uses: ||x-y||² = ||x||² + ||y||² - 2⟨x,y⟩\n",
    "    \"\"\"\n",
    "    # Compute squared norms: ||x_i||² for each row\n",
    "    x_sqnorms = np.einsum('ij,ij->i', x, x)[:, np.newaxis]  # Shape: (N, 1)\n",
    "    y_sqnorms = np.einsum('ij,ij->i', y, y)[np.newaxis, :]  # Shape: (1, M)\n",
    "    \n",
    "    # Compute dot products: x_i · y_j\n",
    "    xy_dots = x @ y.T  # Shape: (N, M)\n",
    "    \n",
    "    # Apply the formula (with abs for numerical stability)\n",
    "    squared_distances = np.abs(x_sqnorms + y_sqnorms - 2.0 * xy_dots)\n",
    "    \n",
    "    return np.sqrt(squared_distances)\n",
    "\n",
    "# Test and verify it gives same results\n",
    "distances_trick = euclidean_trick(X_small, X_small)\n",
    "\n",
    "print(f\"Results match: {np.allclose(distances_broadcast, distances_trick, atol=1e-6)}\")\n",
    "print(f\"Max difference: {np.abs(distances_broadcast - distances_trick).max():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Dive: Einstein Summation\n",
    "\n",
    "`np.einsum` is a powerful tool for expressing tensor operations concisely and efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A shape: (3, 4)\n",
      "Traditional (A*A).sum(axis=1): [1.54320485 0.9392774  1.39845356]\n",
      "Einstein np.einsum('ij,ij->i'): [1.54320485 0.9392774  1.39845356]\n",
      "Results match: True\n",
      "\n",
      "Matrix multiplication A@B vs einsum: True\n",
      "Max difference: 2.22e-16\n",
      "\n",
      "Performance comparison for squared norms:\n",
      "171 μs ± 3.61 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "49.6 μs ± 2.99 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Understanding np.einsum with examples\n",
    "A = np.random.rand(3, 4)\n",
    "print(f\"Matrix A shape: {A.shape}\")\n",
    "\n",
    "# Example 1: Row-wise sum of squares\n",
    "method1 = (A * A).sum(axis=1)  # Traditional way\n",
    "method2 = np.einsum('ij,ij->i', A, A)  # Einstein summation\n",
    "\n",
    "print(f\"Traditional (A*A).sum(axis=1): {method1}\")\n",
    "print(f\"Einstein np.einsum('ij,ij->i'): {method2}\")\n",
    "print(f\"Results match: {np.allclose(method1, method2)}\")\n",
    "\n",
    "# Example 2: Matrix multiplication\n",
    "B = np.random.rand(4, 5)\n",
    "method1 = A @ B\n",
    "method2 = np.einsum('ik,kj->ij', A, B)\n",
    "\n",
    "print(f\"\\nMatrix multiplication A@B vs einsum: {np.allclose(method1, method2)}\")\n",
    "print(f\"Max difference: {np.abs(method1 - method2).max():.2e}\")\n",
    "\n",
    "# Performance comparison for row-wise squared norms\n",
    "large_matrix = np.random.rand(1000, 300)\n",
    "print(\"\\nPerformance comparison for squared norms:\")\n",
    "%timeit (large_matrix * large_matrix).sum(axis=1)\n",
    "%timeit np.einsum('ij,ij->i', large_matrix, large_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Comparison: Broadcasting vs Trick\n",
    "\n",
    "Let's compare both methods across different problem sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing 100 points, 20 features ===\n",
      "Memory - Input: 0.0 MB, Output: 0.1 MB\n",
      "Broadcasting peak: 1.7 MB\n",
      "Trick peak: 0.1 MB\n",
      "Timing broadcast method...\n",
      "471 μs ± 41.6 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "Timing trick method...\n",
      "31.8 μs ± 350 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "Speedup: 13.80x (0.000s vs 0.000s)\n",
      "\n",
      "=== Testing 300 points, 30 features ===\n",
      "Memory - Input: 0.1 MB, Output: 0.7 MB\n",
      "Broadcasting peak: 22.4 MB\n",
      "Trick peak: 0.8 MB\n",
      "Timing broadcast method...\n",
      "9.77 ms ± 419 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Timing trick method...\n",
      "966 μs ± 459 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "Speedup: 14.60x (0.001s vs 0.009s)\n",
      "\n",
      "=== Testing 500 points, 40 features ===\n",
      "Memory - Input: 0.2 MB, Output: 2.0 MB\n",
      "Broadcasting peak: 82.2 MB\n",
      "Trick peak: 2.2 MB\n",
      "Timing broadcast method...\n",
      "36.5 ms ± 982 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Timing trick method...\n",
      "1.65 ms ± 24.6 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "Speedup: 21.80x (0.002s vs 0.035s)\n"
     ]
    }
   ],
   "source": [
    "# Performance testing on different sizes\n",
    "test_sizes = [(100, 20), (300, 30), (500, 40)]\n",
    "\n",
    "for n_points, n_features in test_sizes:\n",
    "    print(f\"\\n=== Testing {n_points} points, {n_features} features ===\")\n",
    "    \n",
    "    # Generate test data\n",
    "    test_data = np.random.rand(n_points, n_features)\n",
    "    \n",
    "    # Memory requirements\n",
    "    input_mb = test_data.nbytes / 1e6\n",
    "    output_mb = (n_points**2 * 8) / 1e6\n",
    "    intermediate_mb = (n_points**2 * n_features * 8) / 1e6\n",
    "    \n",
    "    print(f\"Memory - Input: {input_mb:.1f} MB, Output: {output_mb:.1f} MB\")\n",
    "    print(f\"Broadcasting peak: {input_mb + intermediate_mb + output_mb:.1f} MB\")\n",
    "    print(f\"Trick peak: {input_mb + output_mb:.1f} MB\")\n",
    "    \n",
    "    # Time both methods\n",
    "    print(\"Timing broadcast method...\")\n",
    "    time_broadcast = %timeit -o euclidean_broadcast(test_data, test_data)\n",
    "    \n",
    "    print(\"Timing trick method...\")\n",
    "    time_trick = %timeit -o euclidean_trick(test_data, test_data)\n",
    "    \n",
    "    speedup = time_broadcast.best / time_trick.best\n",
    "    print(f\"Speedup: {speedup:.2f}x ({time_trick.best:.3f}s vs {time_broadcast.best:.3f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Performance Profiling and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line-by-Line Profiling\n",
    "\n",
    "Let's see exactly where time is spent in each function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install line_profiler if not available\n",
    "try:\n",
    "    %load_ext line_profiler\n",
    "except:\n",
    "    print(\"line_profiler not available. Install with: pip install line_profiler\")\n",
    "    print(\"Continuing without detailed profiling...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-09 s\n",
       "\n",
       "Total time: 0.00855432 s\n",
       "File: /tmp/ipykernel_416929/3430701871.py\n",
       "Function: euclidean_trick at line 1\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "     1                                           def euclidean_trick(x, y):\n",
       "     2                                               \"\"\"\n",
       "     3                                               Euclidean distance matrix using the algebraic trick.\n",
       "     4                                           \n",
       "     5                                               Uses: ||x-y||² = ||x||² + ||y||² - 2⟨x,y⟩\n",
       "     6                                               \"\"\"\n",
       "     7                                               # Compute squared norms: ||x_i||² for each row\n",
       "     8         1     126858.0 126858.0      1.5      x_sqnorms = np.einsum('ij,ij->i', x, x)[:, np.newaxis]  # Shape: (N, 1)\n",
       "     9         1      42496.0  42496.0      0.5      y_sqnorms = np.einsum('ij,ij->i', y, y)[np.newaxis, :]  # Shape: (1, M)\n",
       "    10                                           \n",
       "    11                                               # Compute dot products: x_i · y_j\n",
       "    12         1    1739015.0 1.74e+06     20.3      xy_dots = x @ y.T  # Shape: (N, M)\n",
       "    13                                           \n",
       "    14                                               # Apply the formula (with abs for numerical stability)\n",
       "    15         1    5750648.0 5.75e+06     67.2      squared_distances = np.abs(x_sqnorms + y_sqnorms - 2.0 * xy_dots)\n",
       "    16                                           \n",
       "    17         1     895300.0 895300.0     10.5      return np.sqrt(squared_distances)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Profile the trick method\n",
    "profile_data = np.random.rand(800, 40)\n",
    "\n",
    "try:\n",
    "    %lprun -f euclidean_trick euclidean_trick(profile_data, profile_data)\n",
    "except:\n",
    "    print(\"Line profiler not available, showing manual timing breakdown:\")\n",
    "    \n",
    "    # Manual timing breakdown\n",
    "    import time\n",
    "    \n",
    "    # Time each component\n",
    "    start = time.time()\n",
    "    x_sqnorms = np.einsum('ij,ij->i', profile_data, profile_data)[:, np.newaxis]\n",
    "    time1 = time.time() - start\n",
    "    \n",
    "    start = time.time()\n",
    "    y_sqnorms = np.einsum('ij,ij->i', profile_data, profile_data)[np.newaxis, :]\n",
    "    time2 = time.time() - start\n",
    "    \n",
    "    start = time.time()\n",
    "    xy_dots = profile_data @ profile_data.T\n",
    "    time3 = time.time() - start\n",
    "    \n",
    "    start = time.time()\n",
    "    result = np.sqrt(np.abs(x_sqnorms + y_sqnorms - 2.0 * xy_dots))\n",
    "    time4 = time.time() - start\n",
    "    \n",
    "    total = time1 + time2 + time3 + time4\n",
    "    print(f\"Compute x squared norms: {time1:.4f}s ({100*time1/total:.1f}%)\")\n",
    "    print(f\"Compute y squared norms: {time2:.4f}s ({100*time2/total:.1f}%)\")\n",
    "    print(f\"Matrix multiplication:   {time3:.4f}s ({100*time3/total:.1f}%)\")\n",
    "    print(f\"Final computation:       {time4:.4f}s ({100*time4/total:.1f}%)\")\n",
    "    print(f\"Total:                   {total:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding NumPy's Multithreading\n",
    "\n",
    "NumPy uses OpenMP for parallel operations, especially in BLAS routines like matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy configuration:\n",
      "Build Dependencies:\n",
      "  blas:\n",
      "    detection method: pkgconfig\n",
      "    found: true\n",
      "    include directory: /opt/_internal/cpython-3.12.7/lib/python3.12/site-packages/scipy_openblas64/include\n",
      "    lib directory: /opt/_internal/cpython-3.12.7/lib/python3.12/site-packages/scipy_openblas64/lib\n",
      "    name: scipy-openblas\n",
      "    openblas configuration: OpenBLAS 0.3.29  USE64BITINT DYNAMIC_ARCH NO_AFFINITY\n",
      "      Haswell MAX_THREADS=64\n",
      "    pc file directory: /project/.openblas\n",
      "    version: 0.3.29\n",
      "  lapack:\n",
      "    detection method: pkgconfig\n",
      "    found: true\n",
      "    include directory: /opt/_internal/cpython-3.12.7/lib/python3.12/site-packages/scipy_openblas64/include\n",
      "    lib directory: /opt/_internal/cpython-3.12.7/lib/python3.12/site-packages/scipy_openblas64/lib\n",
      "    name: scipy-openblas\n",
      "    openblas configuration: OpenBLAS 0.3.29  USE64BITINT DYNAMIC_ARCH NO_AFFINITY\n",
      "      Haswell MAX_THREADS=64\n",
      "    pc file directory: /project/.openblas\n",
      "    version: 0.3.29\n",
      "Compilers:\n",
      "  c:\n",
      "    commands: cc\n",
      "    linker: ld.bfd\n",
      "    name: gcc\n",
      "    version: 10.2.1\n",
      "  c++:\n",
      "    commands: c++\n",
      "    linker: ld.bfd\n",
      "    name: gcc\n",
      "    version: 10.2.1\n",
      "  cython:\n",
      "    commands: cython\n",
      "    linker: cython\n",
      "    name: cython\n",
      "    version: 3.1.0\n",
      "Machine Information:\n",
      "  build:\n",
      "    cpu: x86_64\n",
      "    endian: little\n",
      "    family: x86_64\n",
      "    system: linux\n",
      "  host:\n",
      "    cpu: x86_64\n",
      "    endian: little\n",
      "    family: x86_64\n",
      "    system: linux\n",
      "Python Information:\n",
      "  path: /tmp/build-env-sgs8db9u/bin/python\n",
      "  version: '3.12'\n",
      "SIMD Extensions:\n",
      "  baseline:\n",
      "  - SSE\n",
      "  - SSE2\n",
      "  - SSE3\n",
      "  found:\n",
      "  - SSSE3\n",
      "  - SSE41\n",
      "  - POPCNT\n",
      "  - SSE42\n",
      "  - AVX\n",
      "  - F16C\n",
      "  - FMA3\n",
      "  - AVX2\n",
      "  not found:\n",
      "  - AVX512F\n",
      "  - AVX512CD\n",
      "  - AVX512_KNL\n",
      "  - AVX512_KNM\n",
      "  - AVX512_SKX\n",
      "  - AVX512_CLX\n",
      "  - AVX512_CNL\n",
      "  - AVX512_ICL\n",
      "\n",
      "\n",
      "Timing matrix multiplication (should use multiple cores):\n",
      "4.24 ms ± 705 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Check BLAS configuration\n",
    "print(\"NumPy configuration:\")\n",
    "np.show_config()\n",
    "\n",
    "# The matrix multiplication x @ y.T is typically the bottleneck\n",
    "# and benefits most from multiple cores\n",
    "large_data = np.random.rand(1500, 100)\n",
    "\n",
    "print(\"\\nTiming matrix multiplication (should use multiple cores):\")\n",
    "%timeit large_data @ large_data.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Profiling\n",
    "\n",
    "Let's analyze memory usage patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data: 0.3 MB\n",
      "\n",
      "Trick method:\n",
      "  Initial: 292.1 MB\n",
      "  Peak:    292.3 MB (+0.2 MB)\n",
      "  Final:   292.3 MB\n",
      "\n",
      "Broadcast method:\n",
      "  Initial: 292.3 MB\n",
      "  Peak:    292.3 MB (+0.0 MB)\n",
      "  Final:   292.3 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simple memory monitoring\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "def memory_profile_function(func, data, name):\n",
    "    \"\"\"Profile memory usage of a function\"\"\"\n",
    "    initial_memory = get_memory_usage()\n",
    "    result = func(data, data)\n",
    "    peak_memory = get_memory_usage()\n",
    "    del result  # Free result memory\n",
    "    final_memory = get_memory_usage()\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Initial: {initial_memory:.1f} MB\")\n",
    "    print(f\"  Peak:    {peak_memory:.1f} MB (+{peak_memory-initial_memory:.1f} MB)\")\n",
    "    print(f\"  Final:   {final_memory:.1f} MB\")\n",
    "    print()\n",
    "\n",
    "# Test with moderately sized data\n",
    "test_data = np.random.rand(800, 50)\n",
    "print(f\"Test data: {test_data.nbytes/1e6:.1f} MB\")\n",
    "print()\n",
    "\n",
    "memory_profile_function(euclidean_trick, test_data, \"Trick method\")\n",
    "memory_profile_function(euclidean_broadcast, test_data, \"Broadcast method\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Optimization Challenge\n",
    "\n",
    "Implement an optimized version using `einsum` for the final sum in the broadcast method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results match: True\n",
      "\n",
      "Performance comparison:\n",
      "1.84 ms ± 188 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "1.15 ms ± 96.2 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "def euclidean_broadcast_optimized(x, y):\n",
    "    \"\"\"\n",
    "    Optimized broadcast version using einsum for the final reduction.\n",
    "    \"\"\"\n",
    "    diff = x[:, np.newaxis, :] - y[np.newaxis, :, :]\n",
    "    \n",
    "    # TODO: Use einsum instead of (diff * diff).sum(axis=2)\n",
    "    squared_distances = np.einsum('ijk,ijk->ij', diff, diff)\n",
    "    \n",
    "    return np.sqrt(squared_distances)\n",
    "\n",
    "# Test the optimized version\n",
    "test_data = np.random.rand(200, 20)\n",
    "\n",
    "result_original = euclidean_broadcast(test_data, test_data)\n",
    "result_optimized = euclidean_broadcast_optimized(test_data, test_data)\n",
    "\n",
    "print(f\"Results match: {np.allclose(result_original, result_optimized)}\")\n",
    "\n",
    "print(\"\\nPerformance comparison:\")\n",
    "%timeit euclidean_broadcast(test_data, test_data)\n",
    "%timeit euclidean_broadcast_optimized(test_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Best Practices and HPC Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Each Approach\n",
    "\n",
    "| Method | Memory Usage | Speed | Best When |\n",
    "|--------|-------------|-------|----------|\n",
    "| Broadcasting | O(N²D) | Moderate | Small N, large D, simple to understand |\n",
    "| Algebraic Trick | O(N²) | Fast | Large N, moderate D, memory-limited |\n",
    "| Chunked Processing | O(chunk_size²D) | Variable | Very large N, limited memory |\n",
    "\n",
    "### Preparing for Advanced HPC Tools\n",
    "\n",
    "This NumPy foundation prepares you for:\n",
    "\n",
    "1. **Numba**: JIT compilation for custom kernels\n",
    "2. **CuPy**: GPU acceleration with CUDA\n",
    "3. **Dask**: Distributed and out-of-core computation\n",
    "4. **MPI**: Multi-node parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing chunked vs direct computation:\n",
      "Data size: 0.3 MB\n",
      "Full result would be: 11.5 MB\n",
      "10.8 ms ± 270 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "Chunked results match direct: True\n"
     ]
    }
   ],
   "source": [
    "# Example: Chunked processing for very large datasets\n",
    "def euclidean_chunked(x, y, chunk_size=1000):\n",
    "    \"\"\"\n",
    "    Compute distance matrix in chunks to manage memory usage.\n",
    "    \n",
    "    This approach trades computation time for memory efficiency.\n",
    "    \"\"\"\n",
    "    n, m = x.shape[0], y.shape[0]\n",
    "    result = np.empty((n, m), dtype=np.float64)\n",
    "    \n",
    "    for i in range(0, n, chunk_size):\n",
    "        i_end = min(i + chunk_size, n)\n",
    "        for j in range(0, m, chunk_size):\n",
    "            j_end = min(j + chunk_size, m)\n",
    "            \n",
    "            # Compute distance for this chunk\n",
    "            chunk_result = euclidean_trick(x[i:i_end], y[j:j_end])\n",
    "            result[i:i_end, j:j_end] = chunk_result\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Demonstrate chunked processing\n",
    "large_data = np.random.rand(1200, 30)\n",
    "\n",
    "print(\"Comparing chunked vs direct computation:\")\n",
    "print(f\"Data size: {large_data.nbytes/1e6:.1f} MB\")\n",
    "print(f\"Full result would be: {(1200**2 * 8)/1e6:.1f} MB\")\n",
    "\n",
    "# Time chunked version\n",
    "%timeit euclidean_chunked(large_data, large_data, chunk_size=400)\n",
    "\n",
    "# Verify results match (on smaller subset)\n",
    "subset = large_data[:100]\n",
    "result_direct = euclidean_trick(subset, subset)\n",
    "result_chunked = euclidean_chunked(subset, subset, chunk_size=50)\n",
    "print(f\"\\nChunked results match direct: {np.allclose(result_direct, result_chunked)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Performance Pitfalls\n",
    "\n",
    "1. **Creating unnecessary copies**\n",
    "2. **Using wrong dtypes** (float64 when float32 would suffice)\n",
    "3. **Not considering memory layout** (C vs Fortran order)\n",
    "4. **Ignoring broadcasting opportunities**\n",
    "5. **Mixing NumPy with Python loops**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float64 memory: 0.8 MB\n",
      "Float32 memory: 0.4 MB\n",
      "\n",
      "Float64 performance:\n",
      "7.16 ms ± 200 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Float32 performance:\n",
      "3.95 ms ± 498 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "Max difference: 5.52e-03\n"
     ]
    }
   ],
   "source": [
    "# Example of dtype impact\n",
    "data_f64 = np.random.rand(1000, 100).astype(np.float64)\n",
    "data_f32 = data_f64.astype(np.float32)\n",
    "\n",
    "print(f\"Float64 memory: {data_f64.nbytes/1e6:.1f} MB\")\n",
    "print(f\"Float32 memory: {data_f32.nbytes/1e6:.1f} MB\")\n",
    "\n",
    "print(\"\\nFloat64 performance:\")\n",
    "%timeit euclidean_trick(data_f64, data_f64)\n",
    "\n",
    "print(\"\\nFloat32 performance:\")\n",
    "%timeit euclidean_trick(data_f32, data_f32)\n",
    "\n",
    "# Check if precision loss is acceptable\n",
    "result_f64 = euclidean_trick(data_f64[:100], data_f64[:100])\n",
    "result_f32 = euclidean_trick(data_f32[:100], data_f32[:100])\n",
    "print(f\"\\nMax difference: {np.abs(result_f64 - result_f32.astype(np.float64)).max():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary and Next Steps\n",
    "\n",
    "## What We've Learned\n",
    "\n",
    "### Core Concepts\n",
    "1. **NumPy Internals**: Memory layout, strides, views vs copies\n",
    "2. **Broadcasting**: Efficient operations on arrays with different shapes\n",
    "3. **Vectorization**: Replacing Python loops with NumPy operations\n",
    "4. **Performance Optimization**: Multiple approaches to the same problem\n",
    "5. **Profiling**: Understanding where time and memory are used\n",
    "\n",
    "### Key Performance Principles\n",
    "- **Avoid Python loops** at all costs in computational kernels\n",
    "- **Understand memory patterns** - views are fast, copies are expensive\n",
    "- **Consider multiple algorithms** - there's often a memory/speed trade-off\n",
    "- **Profile before optimizing** - measure to find real bottlenecks\n",
    "- **Choose appropriate data types** - precision vs performance\n",
    "\n",
    "## Optional Exercise: Design Challenge\n",
    "\n",
    "Design an algorithm for computing pairwise similarities in a recommender system:\n",
    "- 100,000 users\n",
    "- 10,000 items\n",
    "- Sparse rating matrix (1% filled)\n",
    "- Memory budget: 8GB\n",
    "\n",
    "Consider:\n",
    "1. Data structures and memory layout\n",
    "2. Algorithmic approaches (full vs approximate)\n",
    "3. Chunking strategies\n",
    "4. Opportunities for further optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Performance Showcase\n",
      "========================================\n",
      "Computing 5000×5000 distance matrix...\n",
      "Total operations: ~1.2 billion\n",
      "Completed in 0.31 seconds\n",
      "Performance: 4.1 billion ops/second\n",
      "Result matrix: (5000, 5000) (200.0 MB)\n",
      "All diagonal elements ≈ 0: True. Max. deviation: 1.885e-07\n"
     ]
    }
   ],
   "source": [
    "# Final performance demonstration\n",
    "print(\"NumPy Performance Showcase\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create a substantial computation\n",
    "n = 5000\n",
    "X = np.random.rand(n, 50)\n",
    "\n",
    "print(f\"Computing {n}×{n} distance matrix...\")\n",
    "print(f\"Total operations: ~{n**2 * 50 / 1e9:.1f} billion\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "distances = euclidean_trick(X, X)\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed = end_time - start_time\n",
    "operations_per_sec = (n**2 * 50) / elapsed / 1e9\n",
    "\n",
    "print(f\"Completed in {elapsed:.2f} seconds\")\n",
    "print(f\"Performance: {operations_per_sec:.1f} billion ops/second\")\n",
    "print(f\"Result matrix: {distances.shape} ({distances.nbytes/1e6:.1f} MB)\")\n",
    "print(f\"All diagonal elements ≈ 0: {np.allclose(np.diag(distances), 0, atol=1e-6)}. Max. deviation: {round(np.max(np.diag(distances)), 10)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
