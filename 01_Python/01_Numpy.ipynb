{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Numpy Slide](img/01_Numpy/Folie1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Numpy Slide](img/01_Numpy/Folie2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Numpy Slide](img/01_Numpy/Folie3.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Numpy Slide](img/01_Numpy/Folie4.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# Setup for performance comparison\n",
    "size = 1_000_000\n",
    "python_list = list(range(size))\n",
    "numpy_array = np.arange(size)\n",
    "\n",
    "print(f\"Array size: {size} elements\")\n",
    "print(f\"Expected size (8 bytes per element): {8 * size} bytes\")\n",
    "\n",
    "print(f\"Python list memory: {sys.getsizeof(python_list)} bytes. (Overhead: {sys.getsizeof(python_list) - (8 * size)} bytes)\")\n",
    "print(f\"NumPy sys memory: {sys.getsizeof(numpy_array)} bytes. (Overhead: {sys.getsizeof(numpy_array) - (8 * size)} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each element takes 8 bytes with the python list having a very tiny overhead of 56 bytes compared to the overhead of the NumPy array which clocks in at 112 bytes. Still, both overheads are negligible when working with big arrays. \n",
    "\n",
    "> *Note*: You can also get the size (in bytes) of the numpy array without the overhead by using the `.nbytes` property\n",
    ">\n",
    "> *Note*: This is just a mickey mouse example - sys.getsizeof is not guaranteed to return exactly what we expect, as it behaves differently with objects, references and so on. But for this quick comparison it's enough. More information on python memory management: [RealPython](https://realpython.com/python-memory-management/) or [Geeksforgeeks](https://www.geeksforgeeks.org/python/memory-management-in-python/) \n",
    "\n",
    "Next, we compare a basic multiplication on the list compared to the numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison: Element-wise multiplication\n",
    "def python_multiply(data):\n",
    "    return [x * 2 for x in data]\n",
    "\n",
    "def numpy_multiply(data):\n",
    "    return data * 2\n",
    "\n",
    "# Time Python approach\n",
    "start = time.time()\n",
    "python_result = python_multiply(python_list)\n",
    "python_time = time.time() - start\n",
    "\n",
    "# Time NumPy approach\n",
    "start = time.time()\n",
    "numpy_result = numpy_multiply(numpy_array)\n",
    "numpy_time = time.time() - start\n",
    "\n",
    "print(f\"Python list comprehension: {python_time:.4f} seconds\")\n",
    "print(f\"NumPy vectorized operation: {numpy_time:.4f} seconds\")\n",
    "print(f\"NumPy is {python_time / numpy_time:.1f}x faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, that's technically not the best way to measure performance. Measuring performance should be done with a dedicated library that measures the same code multiple times and averages the runtime while also reporting the standard deviation. For Python, one such library is `timeit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit python_multiply(python_list)\n",
    "%timeit numpy_multiply(numpy_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can say with confidence, that numpy is much, much faster than raw python!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 NumPy Array Internals\n",
    "\n",
    "Understanding NumPy's internal structure is crucial for writing high-performance code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `ndarray` Structure\n",
    "\n",
    "Every NumPy array consists of:\n",
    "1. **Data buffer**: Raw binary data\n",
    "2. **Metadata**: Shape, strides, dtype, etc.\n",
    "3. **View information**: How to interpret the data buffer\n",
    "\n",
    "![Numpy Slide](img/01_Numpy/Folie5.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2D array to explore its internals\n",
    "x = np.array([[0, 1, 2, 3],\n",
    "              [4, 5, 6, 7],\n",
    "              [8, 9, 10, 11],\n",
    "              [12, 13, 14, 15]], dtype=np.int8)\n",
    "\n",
    "print(\"Array:\")\n",
    "print(x)\n",
    "print(f\"\\nShape: {x.shape}\")\n",
    "print(f\"Data type: {x.dtype}\")\n",
    "print(f\"Strides: {x.strides} bytes\")\n",
    "print(f\"Total size: {x.nbytes} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Strides - The Key to Performance\n",
    "\n",
    "**Strides** tell NumPy how many bytes to skip to move to the next element along each axis.\n",
    "\n",
    "For our array with `dtype=int8` (1 byte per element):\n",
    "- To move to next column: skip 1 byte\n",
    "- To move to next row: skip 4 bytes (entire row)\n",
    "\n",
    "**Formula**: `element[i,j] = data_ptr + i*stride[0] + j*stride[1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how strides work\n",
    "def explain_strides(arr):\n",
    "    print(f\"Array shape: {arr.shape}\")\n",
    "    print(f\"Strides: {arr.strides}\")\n",
    "    print(f\"Element size: {arr.itemsize} bytes\")\n",
    "    \n",
    "    # Calculate expected strides\n",
    "    expected_col_stride = arr.itemsize\n",
    "    expected_row_stride = arr.shape[1] * arr.itemsize\n",
    "    print(f\"Expected strides: ({expected_row_stride}, {expected_col_stride})\")\n",
    "    \n",
    "    # Show memory layout\n",
    "    print(\"\\nMemory layout (conceptual):\")\n",
    "    flat_view = arr.ravel()\n",
    "    for i in range(len(flat_view)):\n",
    "        if i % arr.shape[1] == 0 and i > 0:\n",
    "            print(f\"  |  {flat_view[i]:2d}\", end=\"\")\n",
    "        else:\n",
    "            print(f\" {flat_view[i]:2d}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "explain_strides(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Predict the Strides\n",
    "\n",
    "For each transformation below, predict the strides before running the code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1.1: Reshape to (2, 8)\n",
    "print(\"Original array (4, 4) with strides\", x.strides)\n",
    "y = x.reshape((2, 8))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Reshaped to {y.shape}: strides = {y.strides}\")\n",
    "explain_strides(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1.2: Reshape to (1, 16) \n",
    "z = x.reshape((1, 16))\n",
    "print(f\"Reshaped to {z.shape}: strides = {z.strides}\")\n",
    "explain_strides(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1.3: Different data type\n",
    "a = np.array(x, dtype=np.int16)  # 2 bytes per element\n",
    "\n",
    "print(f\"\\nWith int16 dtype: strides = {a.strides}\")\n",
    "print(f\"Compare with int8: strides = {x.strides}\")\n",
    "print(\"Notice how strides scale with element size!\\n\")\n",
    "explain_strides(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this conceptual memory layout looks the same as for the first example, but here we still have a stride value of (8, 2). That's because now each element takes 2 bytes, so skipping 8 bytes means skipping 4 elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Views vs Copies: Memory Management Deep Dive\n",
    "\n",
    "It is important for HPC performance to understand when operations create new memory vs when they just change metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate view vs copy with ravel() and flatten()\n",
    "x = np.arange(12).reshape(3, 4)\n",
    "print(\"Original array:\")\n",
    "print(x)\n",
    "print(f\"Original base: {x.base}\")\n",
    "\n",
    "# ravel() tries to return a view (metadata change only)\n",
    "y_ravel = x.ravel()\n",
    "print(f\"\\nAfter ravel() - same base object? {y_ravel.base is x}\")\n",
    "print(f\"Shares memory? {np.shares_memory(x, y_ravel)}\")\n",
    "\n",
    "# flatten() always returns a copy (new memory)\n",
    "y_flatten = x.flatten()\n",
    "print(f\"\\nAfter flatten() - same base object? {y_flatten.base is x}\")\n",
    "print(f\"Shares memory? {np.shares_memory(x, y_flatten)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consequence: modifying views affects original\n",
    "x = np.arange(5)\n",
    "print(f\"Original x: {x}\")\n",
    "\n",
    "y_ravel = x.ravel()  # Creates a view\n",
    "y_ravel[0] = 999     # Modify the view\n",
    "print(f\"After modifying ravel view: {x}\")\n",
    "\n",
    "# Reset\n",
    "x = np.arange(5)\n",
    "y_flatten = x.flatten()  # Creates a copy\n",
    "y_flatten[0] = 999       # Modify the copy\n",
    "print(f\"After modifying flatten copy: {x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Impact: When Views Become Copies\n",
    "\n",
    "Sometimes operations that normally create views are forced to create copies due to memory layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a large array for timing\n",
    "x = np.random.rand(5000, 5000)\n",
    "print(f\"Array size: {x.nbytes / 1e6:.1f} MB\")\n",
    "\n",
    "# Case 1: Simple operations (fast - metadata only)\n",
    "%timeit x.T          # Transpose - just swap strides\n",
    "%timeit x.ravel()    # Ravel of C-contiguous array - view possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 2: Transpose + ravel (slow - copy required)\n",
    "%timeit x.T.ravel()  # Can't create view of non-contiguous transpose\n",
    "\n",
    "# Case 3: Flatten (always slow - always copies)\n",
    "%timeit x.flatten()  # Always creates copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insight**: The transpose `x.T` creates a view with modified strides, but it's no longer C-contiguous. When we then `ravel()` it, NumPy can't express the flattened result with simple strides, so it must copy the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C-Contiguous: Row-Major aligned in memory\n",
    "# F-Contiguous: Col-Major aligned in memory\n",
    "# Numpy requires C-Contiguity for most efficient operations!\n",
    "\n",
    "# Verify the contiguity issue\n",
    "print(f\"x.flags.c_contiguous: {x.flags.c_contiguous}\")\n",
    "print(f\"x.T.flags.c_contiguous: {x.T.flags.c_contiguous}\")\n",
    "print(f\"x.T.flags.f_contiguous: {x.T.flags.f_contiguous}\")\n",
    "\n",
    "# This is why x.T.ravel() is slow - it requires copying to make contiguous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Array Creation and Basic Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient Array Creation Methods\n",
    "\n",
    "Different creation methods have different performance characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different array creation methods\n",
    "size = (1000, 1000)\n",
    "\n",
    "print(\"Array creation timing:\")\n",
    "%timeit np.zeros(size)           # Pre-allocated, initialized to 0\n",
    "%timeit np.empty(size)           # Pre-allocated, uninitialized (fastest)\n",
    "%timeit np.ones(size)            # Pre-allocated, initialized to 1\n",
    "%timeit np.full(size, 3.14)      # Pre-allocated, initialized to value\n",
    "%timeit np.arange(size[0] * size[1]).reshape(size)  # Sequential, then reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Types and Memory Usage\n",
    "\n",
    "Choosing the right data type can significantly impact memory usage and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory usage comparison for different dtypes\n",
    "size = 5_000_000\n",
    "dtypes = [np.int8, np.int16, np.int32, np.int64, np.float32, np.float64]\n",
    "\n",
    "print(\"Memory usage by data type:\")\n",
    "print(f\"{'Data Type':<12}\\t\\t{'Bytes/element':<15} {'Total MB':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "base_memory = None\n",
    "for dtype in dtypes:\n",
    "    arr = np.ones(size, dtype=dtype)\n",
    "    memory_mb = arr.nbytes / 1e6\n",
    "    if base_memory is None:\n",
    "        base_memory = memory_mb\n",
    "    \n",
    "    print(f\"{str(dtype):<12}\\t{arr.itemsize:<15} {memory_mb:<10.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Memory-Efficient Array Operations\n",
    "\n",
    "Create arrays with different dtypes and observe the memory and performance implications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Compare performance of operations on different dtypes\n",
    "size = 5_000_000\n",
    "\n",
    "# Create arrays with different precisions\n",
    "arr_f32 = np.random.rand(size).astype(np.float32)\n",
    "arr_f64 = np.random.rand(size).astype(np.float64)\n",
    "\n",
    "print(f\"Float32 array: {arr_f32.nbytes / 1e6:.1f} MB\")\n",
    "print(f\"Float64 array: {arr_f64.nbytes / 1e6:.1f} MB\")\n",
    "\n",
    "print(\"\\nPerformance comparison for sum operation:\")\n",
    "%timeit np.sum(arr_f32)\n",
    "%timeit np.sum(arr_f64)\n",
    "\n",
    "print(\"\\nPerformance comparison for element-wise multiplication:\")\n",
    "%timeit arr_f32 * 2.0\n",
    "%timeit arr_f64 * 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Session 2: Broadcasting and Vectorization\n",
    "\n",
    "## 2.1 Understanding Broadcasting - The Heart of NumPy Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting Rules\n",
    "\n",
    "Broadcasting allows NumPy to perform operations on arrays with different shapes efficiently. The rules are:\n",
    "\n",
    "1. **Start from the trailing dimension** and work backward\n",
    "2. **Dimensions are compatible** if:\n",
    "   - They are equal, OR\n",
    "   - One of them is 1, OR  \n",
    "   - One of them is missing (treated as 1)\n",
    "3. **Result shape** is the maximum size along each dimension\n",
    "\n",
    "```\n",
    "Examples:\n",
    "A: (5, 4)     B: (4,)       → Result: (5, 4)  ✓\n",
    "A: (5, 1)     B: (1, 4)     → Result: (5, 4)  ✓\n",
    "A: (5, 4)     B: (3,)       → Result: Error   ✗\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Numpy Slide](img/01_Numpy/Folie6.PNG)\n",
    "![Numpy Slide](img/01_Numpy/Folie7.PNG)\n",
    "![Numpy Slide](img/01_Numpy/Folie8.PNG)\n",
    "![Numpy Slide](img/01_Numpy/Folie9.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic broadcasting examples\n",
    "print(\"Example 1: Array + scalar\")\n",
    "a = np.array([1, 2, 3, 4, 5])\n",
    "result = a + 10\n",
    "print(f\"[1,2,3,4,5] + 10 = {result}\")\n",
    "print(f\"Shapes: {a.shape} + scalar → {result.shape}\")\n",
    "\n",
    "print(\"\\nExample 2: 2D array + 1D array\")\n",
    "b = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])\n",
    "c = np.array([10, 20, 30])\n",
    "result = b + c\n",
    "print(f\"Array b shape: {b.shape}\")\n",
    "print(f\"Array c shape: {c.shape}\")\n",
    "print(f\"Result shape: {result.shape}\")\n",
    "print(\"Result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating New Dimensions with `np.newaxis`\n",
    "\n",
    "Often we need to add dimensions to enable broadcasting. `np.newaxis` is an alias for `None` and adds a dimension of size 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate np.newaxis\n",
    "x = np.arange(5)\n",
    "print(f\"Original x: {x}\")\n",
    "print(f\"Shape: {x.shape}\")\n",
    "\n",
    "# Convert to column vector\n",
    "x_col = x[:, np.newaxis]\n",
    "print(f\"\\nColumn vector: \\n{x_col}\")\n",
    "print(f\"Shape: {x_col.shape}\")\n",
    "\n",
    "# Convert to row vector  \n",
    "x_row = x[np.newaxis, :]\n",
    "print(f\"\\nRow vector: \\n{x_row}\")\n",
    "print(f\"Shape: {x_row.shape}\")\n",
    "\n",
    "# Verify these are views (no memory copying)\n",
    "print(f\"\\nMemory sharing - original and column: {np.shares_memory(x, x_col)}\")\n",
    "print(f\"Memory sharing - original and row: {np.shares_memory(x, x_row)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Broadcasting Puzzle\n",
    "\n",
    "Predict the output shapes and results before running!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.1: Create a multiplication table\n",
    "x = np.arange(1, 6)  # [1, 2, 3, 4, 5]\n",
    "\n",
    "print(\"What will be the shape and result of x[:, newaxis] * x?\")\n",
    "\n",
    "multiplication_table = x[:, np.newaxis] * x\n",
    "print(f\"\\nResult shape: {multiplication_table.shape}\")\n",
    "print(\"Multiplication table:\")\n",
    "print(multiplication_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.2: 3D broadcasting\n",
    "a = np.random.rand(2, 3, 1)  # Shape (2, 3, 1)\n",
    "b = np.random.rand(4)        # Shape (4,)\n",
    "\n",
    "print(f\"Array a shape: {a.shape}\")\n",
    "print(f\"Array b shape: {b.shape}\")\n",
    "print(\"What will be the result shape of a + b?\")\n",
    "\n",
    "result = a + b\n",
    "print(f\"\\nActual result shape: {result.shape}\")\n",
    "print(\"This creates all combinations of elements!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Advanced Broadcasting Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise Operations on Vector Collections\n",
    "\n",
    "A common HPC pattern: given N vectors, compute some pairwise operation between all pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data: 6 vectors of dimension 3\n",
    "np.random.seed(42)  # For reproducible results\n",
    "vectors = np.random.rand(6, 3)\n",
    "print(\"Input vectors (6 vectors of dimension 3):\")\n",
    "print(vectors)\n",
    "print(f\"Shape: {vectors.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute all pairwise differences using broadcasting\n",
    "# Goal: diff[i,j,k] = vectors[i,k] - vectors[j,k]\n",
    "\n",
    "# Method: Use broadcasting to create (6,1,3) - (1,6,3) → (6,6,3)\n",
    "pairwise_diff = vectors[:, np.newaxis, :] - vectors[np.newaxis, :, :]\n",
    "\n",
    "print(f\"Pairwise differences shape: {pairwise_diff.shape}\")\n",
    "print(\"\\nExample: Difference between vector 0 and vector 1:\")\n",
    "print(f\"Vector 0: {vectors[0]}\")\n",
    "print(f\"Vector 1: {vectors[1]}\")\n",
    "print(f\"Difference: {pairwise_diff[0, 1]}\")\n",
    "print(f\"Manual check: {vectors[0] - vectors[1]}\")\n",
    "\n",
    "# Verify diagonal is zero (vector - itself)\n",
    "print(f\"\\nDiagonal elements (should be ~0): {np.diag(pairwise_diff[:,:,0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Usage Analysis\n",
    "\n",
    "Let's understand the memory implications of this broadcasting operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze memory usage\n",
    "n_vectors, dim = vectors.shape\n",
    "input_memory = vectors.nbytes\n",
    "output_memory = pairwise_diff.nbytes\n",
    "\n",
    "print(f\"Input: {n_vectors} vectors of dimension {dim}\")\n",
    "print(f\"Input memory: {input_memory} bytes ({input_memory/1024:.1f} KB)\")\n",
    "print(f\"Output memory: {output_memory} bytes ({output_memory/1024:.1f} KB)\")\n",
    "print(f\"Memory expansion: {output_memory/input_memory:.1f}x\")\n",
    "\n",
    "print(f\"\\nFor N={n_vectors} vectors, we create N×N×dim = {n_vectors}×{n_vectors}×{dim} = {n_vectors**2 * dim} elements\")\n",
    "print(f\"This scales as O(N²) - can become memory-intensive for large N!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Vectorization Strategies\n",
    "\n",
    "### The Golden Rule: Eliminate Python Loops\n",
    "\n",
    "Let's see how to replace common loop patterns with vectorized operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Apply a function to each element\n",
    "data = np.random.rand(100000)\n",
    "\n",
    "# BAD: Python loop\n",
    "def apply_function_loop(x):\n",
    "    result = np.empty_like(x)\n",
    "    for i in range(len(x)):\n",
    "        result[i] = np.exp(x[i]) + np.sin(x[i])\n",
    "    return result\n",
    "\n",
    "# GOOD: Vectorized\n",
    "def apply_function_vectorized(x):\n",
    "    return np.exp(x) + np.sin(x)\n",
    "\n",
    "# Performance comparison\n",
    "print(\"Performance comparison:\")\n",
    "%timeit apply_function_loop(data)\n",
    "%timeit apply_function_vectorized(data)\n",
    "\n",
    "# Verify they give the same result\n",
    "result1 = apply_function_loop(data[:100])  # Use smaller array for speed\n",
    "result2 = apply_function_vectorized(data[:100])\n",
    "print(f\"\\nResults are equal: {np.allclose(result1, result2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Vectorization Challenge\n",
    "\n",
    "Convert the following loop-based operations to vectorized NumPy operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.1: Conditional operations\n",
    "# Loop version: Set values > 0.5 to their square, others to 0\n",
    "data = np.random.rand(1000)\n",
    "\n",
    "def conditional_loop(x):\n",
    "    result = np.empty_like(x)\n",
    "    for i in range(len(x)):\n",
    "        if x[i] > 0.5:\n",
    "            result[i] = x[i] ** 2\n",
    "        else:\n",
    "            result[i] = 0.0\n",
    "    return result\n",
    "\n",
    "# TODO: Implement vectorized version\n",
    "def conditional_vectorized(x):\n",
    "    # Hint: Use np.where() or boolean indexing\n",
    "    ...\n",
    "\n",
    "# Uncomment to test:\n",
    "result_loop = conditional_loop(data)\n",
    "result_vectorized = conditional_vectorized(data)\n",
    "print(f\"Results match: {np.allclose(result_loop, result_vectorized)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.2: Cumulative operations\n",
    "# Convert nested loop to vectorized operation\n",
    "matrix = np.random.rand(500, 500)\n",
    "\n",
    "# Loop version: Compute row-wise running averages\n",
    "def running_average_loop(mat):\n",
    "    result = np.empty_like(mat)\n",
    "    for i in range(mat.shape[0]):\n",
    "        for j in range(mat.shape[1]):\n",
    "            result[i, j] = np.mean(mat[i, :j+1])\n",
    "    return result\n",
    "\n",
    "# TODO: Implement Vectorized version\n",
    "# Hint: use np.cumsum. Reference: https://numpy.org/doc/stable/reference/generated/numpy.cumsum.html\n",
    "def running_average_vectorized(mat):\n",
    "    ...\n",
    "\n",
    "# Test on smaller matrix for verification\n",
    "small_mat = matrix[:5, :5]\n",
    "result_loop = running_average_loop(small_mat)\n",
    "result_vec = running_average_vectorized(small_mat)\n",
    "\n",
    "print(f\"Results match: {np.allclose(result_loop, result_vec)}\")\n",
    "print(\"\\nPerformance on full matrix:\")\n",
    "%timeit running_average_loop(matrix)\n",
    "%timeit running_average_vectorized(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Session 3: Optimization and Real-World Applications\n",
    "\n",
    "## 3.1 Euclidean Distance Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Problem\n",
    "\n",
    "Given N vectors in D-dimensional space, compute the matrix of Euclidean distances between all pairs:\n",
    "\n",
    "$$d_{ij} = \\sqrt{\\sum_{k=1}^{D} (x_{ik} - x_{jk})^2}$$\n",
    "\n",
    "This is fundamental in:\n",
    "- Machine learning (k-NN, clustering)\n",
    "- Molecular dynamics simulations\n",
    "- Computer graphics\n",
    "- Scientific computing\n",
    "\n",
    "We'll implement and compare multiple approaches, analyzing their trade-offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test data\n",
    "np.random.seed(42)\n",
    "n_points = 1000\n",
    "n_features = 50\n",
    "X = np.random.rand(n_points, n_features) * 10.0\n",
    "\n",
    "print(f\"Dataset: {n_points} points in {n_features}D space\")\n",
    "print(f\"Input size: {X.nbytes / 1e6:.1f} MB\")\n",
    "print(f\"Output will be: {n_points}×{n_points} = {n_points**2:,} distances\")\n",
    "print(f\"Output size: {(n_points**2 * 8) / 1e6:.1f} MB (float64)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: Broadcasting (Intuitive but Memory-Intensive)\n",
    "\n",
    "The most straightforward approach uses broadcasting to compute all pairwise differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_broadcast(x, y):\n",
    "    \"\"\"\n",
    "    Euclidean distance matrix using broadcasting.\n",
    "    \n",
    "    Args:\n",
    "        x: (N, D) array of N vectors in D dimensions\n",
    "        y: (M, D) array of M vectors in D dimensions\n",
    "        \n",
    "    Returns:\n",
    "        (N, M) array of distances\n",
    "    \"\"\"\n",
    "    # Shape: (N, 1, D) - (1, M, D) → (N, M, D)\n",
    "    diff = x[:, np.newaxis, :] - y[np.newaxis, :, :]\n",
    "    \n",
    "    # Sum of squares along last dimension\n",
    "    return np.sqrt((diff * diff).sum(axis=2))\n",
    "\n",
    "# Test with small dataset first\n",
    "X_small = X[:10, :5]\n",
    "distances_broadcast = euclidean_broadcast(X_small, X_small)\n",
    "\n",
    "print(f\"Small test result shape: {distances_broadcast.shape}\")\n",
    "print(\"\\nFirst few distances:\")\n",
    "print(distances_broadcast[:3, :3])\n",
    "print(\"\\nDiagonal (should be ~0):\")\n",
    "print(np.diag(distances_broadcast)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Analysis of Broadcasting Approach\n",
    "\n",
    "Let's understand the memory requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_memory_usage(n_points, n_features):\n",
    "    input_size = n_points * n_features * 8  # float64\n",
    "    intermediate_size = n_points * n_points * n_features * 8  # diff array\n",
    "    output_size = n_points * n_points * 8  # distance matrix\n",
    "    \n",
    "    print(f\"For {n_points} points in {n_features}D:\")\n",
    "    print(f\"Input memory:        {input_size / 1e6:8.1f} MB\")\n",
    "    print(f\"Intermediate memory: {intermediate_size / 1e6:8.1f} MB (diff array)\")\n",
    "    print(f\"Output memory:       {output_size / 1e6:8.1f} MB\")\n",
    "    print(f\"Peak memory usage:   {(input_size + intermediate_size + output_size) / 1e6:8.1f} MB\")\n",
    "    print(f\"Memory amplification: {intermediate_size / input_size:.1f}x\\n\")\n",
    "\n",
    "# Analyze different problem sizes\n",
    "sizes = [(100, 10), (500, 20), (1000, 50), (2000, 100)]\n",
    "for n, d in sizes:\n",
    "    analyze_memory_usage(n, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: Mathematical Optimization (The \"Trick\")\n",
    "\n",
    "We can avoid the large intermediate array using the algebraic identity:\n",
    "\n",
    "$$\\|x_i - x_j\\|^2 = \\|x_i\\|^2 + \\|x_j\\|^2 - 2 x_i \\cdot x_j$$\n",
    "\n",
    "This reduces memory usage and can be faster due to optimized BLAS operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_trick(x, y):\n",
    "    \"\"\"\n",
    "    Euclidean distance matrix using the algebraic trick.\n",
    "    \n",
    "    Uses: ||x-y||² = ||x||² + ||y||² - 2⟨x,y⟩\n",
    "    \"\"\"\n",
    "    # Compute squared norms: ||x_i||² for each row\n",
    "    x_sqnorms = np.einsum('ij,ij->i', x, x)[:, np.newaxis]  # Shape: (N, 1)\n",
    "    y_sqnorms = np.einsum('ij,ij->i', y, y)[np.newaxis, :]  # Shape: (1, M)\n",
    "    \n",
    "    # Compute dot products: x_i · y_j\n",
    "    xy_dots = x @ y.T  # Shape: (N, M)\n",
    "    \n",
    "    # Apply the formula (with abs for numerical stability)\n",
    "    squared_distances = np.abs(x_sqnorms + y_sqnorms - 2.0 * xy_dots)\n",
    "    \n",
    "    return np.sqrt(squared_distances)\n",
    "\n",
    "# Test and verify it gives same results\n",
    "distances_trick = euclidean_trick(X_small, X_small)\n",
    "\n",
    "print(f\"Results match: {np.allclose(distances_broadcast, distances_trick, atol=1e-6)}\")\n",
    "print(f\"Max difference: {np.abs(distances_broadcast - distances_trick).max():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Dive: Einstein Summation\n",
    "\n",
    "[`np.einsum`](https://numpy.org/doc/stable/reference/generated/numpy.einsum.html) is a powerful tool for expressing tensor operations concisely and efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding np.einsum with examples\n",
    "A = np.random.rand(3, 4)\n",
    "print(f\"Matrix A shape: {A.shape}\")\n",
    "\n",
    "# Example 1: Row-wise sum of squares\n",
    "method1 = (A * A).sum(axis=1)  # Traditional way\n",
    "method2 = np.einsum('ij,ij->i', A, A)  # Einstein summation\n",
    "\n",
    "print(f\"Traditional (A*A).sum(axis=1): {method1}\")\n",
    "print(f\"Einstein np.einsum('ij,ij->i'): {method2}\")\n",
    "print(f\"Results match: {np.allclose(method1, method2)}\")\n",
    "\n",
    "# Example 2: Matrix multiplication\n",
    "B = np.random.rand(4, 5)\n",
    "method1 = A @ B\n",
    "method2 = np.einsum('ik,kj->ij', A, B)\n",
    "\n",
    "print(f\"\\nMatrix multiplication A@B vs einsum: {np.allclose(method1, method2)}\")\n",
    "print(f\"Max difference: {np.abs(method1 - method2).max():.2e}\")\n",
    "\n",
    "# Performance comparison for row-wise squared norms\n",
    "large_matrix = np.random.rand(1000, 300)\n",
    "print(\"\\nPerformance comparison for squared norms:\")\n",
    "%timeit (large_matrix * large_matrix).sum(axis=1)\n",
    "%timeit np.einsum('ij,ij->i', large_matrix, large_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Comparison: Broadcasting vs Trick\n",
    "\n",
    "Let's compare both methods across different problem sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance testing on different sizes\n",
    "test_sizes = [(100, 20), (300, 30), (500, 40)]\n",
    "\n",
    "for n_points, n_features in test_sizes:\n",
    "    print(f\"\\n=== Testing {n_points} points, {n_features} features ===\")\n",
    "    \n",
    "    # Generate test data\n",
    "    test_data = np.random.rand(n_points, n_features)\n",
    "    \n",
    "    # Memory requirements\n",
    "    input_mb = test_data.nbytes / 1e6\n",
    "    output_mb = (n_points**2 * 8) / 1e6\n",
    "    intermediate_mb = (n_points**2 * n_features * 8) / 1e6\n",
    "    \n",
    "    print(f\"Memory - Input: {input_mb:.1f} MB, Output: {output_mb:.1f} MB\")\n",
    "    print(f\"Broadcasting peak: {input_mb + intermediate_mb + output_mb:.1f} MB\")\n",
    "    print(f\"Trick peak: {input_mb + output_mb:.1f} MB\")\n",
    "    \n",
    "    # Time both methods\n",
    "    print(\"Timing broadcast method...\")\n",
    "    time_broadcast = %timeit -o euclidean_broadcast(test_data, test_data)\n",
    "    \n",
    "    print(\"Timing trick method...\")\n",
    "    time_trick = %timeit -o euclidean_trick(test_data, test_data)\n",
    "    \n",
    "    speedup = time_broadcast.best / time_trick.best\n",
    "    print(f\"Speedup: {speedup:.2f}x ({time_trick.best:.3f}s vs {time_broadcast.best:.3f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Performance Profiling and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line-by-Line Profiling\n",
    "\n",
    "Let's see exactly where time is spent in each function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install line_profiler if not available\n",
    "try:\n",
    "    %load_ext line_profiler\n",
    "except:\n",
    "    print(\"line_profiler not available. Install with: pip install line_profiler\")\n",
    "    print(\"Continuing without detailed profiling...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile the trick method\n",
    "profile_data = np.random.rand(800, 40)\n",
    "\n",
    "try:\n",
    "    %lprun -f euclidean_trick euclidean_trick(profile_data, profile_data)\n",
    "except:\n",
    "    print(\"Line profiler not available, showing manual timing breakdown:\")\n",
    "    \n",
    "    # Manual timing breakdown\n",
    "    import time\n",
    "    \n",
    "    # Time each component\n",
    "    start = time.time()\n",
    "    x_sqnorms = np.einsum('ij,ij->i', profile_data, profile_data)[:, np.newaxis]\n",
    "    time1 = time.time() - start\n",
    "    \n",
    "    start = time.time()\n",
    "    y_sqnorms = np.einsum('ij,ij->i', profile_data, profile_data)[np.newaxis, :]\n",
    "    time2 = time.time() - start\n",
    "    \n",
    "    start = time.time()\n",
    "    xy_dots = profile_data @ profile_data.T\n",
    "    time3 = time.time() - start\n",
    "    \n",
    "    start = time.time()\n",
    "    result = np.sqrt(np.abs(x_sqnorms + y_sqnorms - 2.0 * xy_dots))\n",
    "    time4 = time.time() - start\n",
    "    \n",
    "    total = time1 + time2 + time3 + time4\n",
    "    print(f\"Compute x squared norms: {time1:.4f}s ({100*time1/total:.1f}%)\")\n",
    "    print(f\"Compute y squared norms: {time2:.4f}s ({100*time2/total:.1f}%)\")\n",
    "    print(f\"Matrix multiplication:   {time3:.4f}s ({100*time3/total:.1f}%)\")\n",
    "    print(f\"Final computation:       {time4:.4f}s ({100*time4/total:.1f}%)\")\n",
    "    print(f\"Total:                   {total:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding NumPy's Multithreading\n",
    "\n",
    "NumPy uses OpenMP for parallel operations, especially in BLAS routines like matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check BLAS configuration\n",
    "print(\"NumPy configuration:\")\n",
    "np.show_config()\n",
    "\n",
    "# The matrix multiplication x @ y.T is typically the bottleneck\n",
    "# and benefits most from multiple cores\n",
    "large_data = np.random.rand(1500, 100)\n",
    "\n",
    "print(\"\\nTiming matrix multiplication (should use multiple cores):\")\n",
    "%timeit large_data @ large_data.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Profiling\n",
    "\n",
    "Let's analyze memory usage patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple memory monitoring\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "def memory_profile_function(func, data, name):\n",
    "    \"\"\"Profile memory usage of a function\"\"\"\n",
    "    initial_memory = get_memory_usage()\n",
    "    result = func(data, data)\n",
    "    peak_memory = get_memory_usage()\n",
    "    del result  # Free result memory\n",
    "    final_memory = get_memory_usage()\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Initial: {initial_memory:.1f} MB\")\n",
    "    print(f\"  Peak:    {peak_memory:.1f} MB (+{peak_memory-initial_memory:.1f} MB)\")\n",
    "    print(f\"  Final:   {final_memory:.1f} MB\")\n",
    "    print()\n",
    "\n",
    "# Test with moderately sized data\n",
    "test_data = np.random.rand(800, 50)\n",
    "print(f\"Test data: {test_data.nbytes/1e6:.1f} MB\")\n",
    "print()\n",
    "\n",
    "memory_profile_function(euclidean_trick, test_data, \"Trick method\")\n",
    "memory_profile_function(euclidean_broadcast, test_data, \"Broadcast method\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Optimization Challenge\n",
    "\n",
    "Implement an optimized version using `einsum` for the final sum in the broadcast method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_broadcast_optimized(x, y):\n",
    "    \"\"\"\n",
    "    Optimized broadcast version using einsum for the final reduction.\n",
    "    \"\"\"\n",
    "    diff = x[:, np.newaxis, :] - y[np.newaxis, :, :]\n",
    "    \n",
    "    # Use einsum instead of (diff * diff).sum(axis=2)\n",
    "    squared_distances = np.einsum('ijk,ijk->ij', diff, diff)\n",
    "    \n",
    "    return np.sqrt(squared_distances)\n",
    "\n",
    "# Test the optimized version\n",
    "test_data = np.random.rand(200, 20)\n",
    "\n",
    "result_original = euclidean_broadcast(test_data, test_data)\n",
    "result_optimized = euclidean_broadcast_optimized(test_data, test_data)\n",
    "\n",
    "print(f\"Results match: {np.allclose(result_original, result_optimized)}\")\n",
    "\n",
    "print(\"\\nPerformance comparison:\")\n",
    "%timeit euclidean_broadcast(test_data, test_data)\n",
    "%timeit euclidean_broadcast_optimized(test_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Best Practices and Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Each Approach\n",
    "\n",
    "| Method | Memory Usage | Speed | Best When |\n",
    "|--------|-------------|-------|----------|\n",
    "| Broadcasting | O(N²D) | Moderate | Small N, large D, simple to understand |\n",
    "| Algebraic Trick | O(N²) | Fast | Large N, moderate D, memory-limited |\n",
    "| Chunked Processing | O(chunk_size²D) | Variable | Very large N, limited memory |\n",
    "\n",
    "### Advanced: Chunking a problem\n",
    "In the following example we chunk the input into processable chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Chunked processing for very large datasets\n",
    "def euclidean_chunked(x, y, chunk_size=1000):\n",
    "    \"\"\"\n",
    "    Compute distance matrix in chunks to manage memory usage.\n",
    "    \n",
    "    This approach trades computation time for memory efficiency.\n",
    "    \"\"\"\n",
    "    n, m = x.shape[0], y.shape[0]\n",
    "    result = np.empty((n, m), dtype=np.float64)\n",
    "    \n",
    "    for i in range(0, n, chunk_size):\n",
    "        i_end = min(i + chunk_size, n)\n",
    "        for j in range(0, m, chunk_size):\n",
    "            j_end = min(j + chunk_size, m)\n",
    "            \n",
    "            # Compute distance for this chunk\n",
    "            chunk_result = euclidean_trick(x[i:i_end], y[j:j_end])\n",
    "            result[i:i_end, j:j_end] = chunk_result\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Demonstrate chunked processing\n",
    "large_data = np.random.rand(1200, 30)\n",
    "\n",
    "print(\"Comparing chunked vs direct computation:\")\n",
    "print(f\"Data size: {large_data.nbytes/1e6:.1f} MB\")\n",
    "print(f\"Full result would be: {(1200**2 * 8)/1e6:.1f} MB\")\n",
    "\n",
    "# Time chunked version\n",
    "%timeit euclidean_chunked(large_data, large_data, chunk_size=400)\n",
    "\n",
    "# Verify results match (on smaller subset)\n",
    "subset = large_data[:100]\n",
    "result_direct = euclidean_trick(subset, subset)\n",
    "result_chunked = euclidean_chunked(subset, subset, chunk_size=50)\n",
    "print(f\"\\nChunked results match direct: {np.allclose(result_direct, result_chunked)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Performance Pitfalls\n",
    "\n",
    "1. **Creating unnecessary copies**\n",
    "2. **Using wrong dtypes** (float64 when float32 would suffice)\n",
    "3. **Not considering memory layout** (C vs Fortran order)\n",
    "4. **Ignoring broadcasting opportunities**\n",
    "5. **Mixing NumPy with Python loops**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of dtype impact\n",
    "data_f64 = np.random.rand(1000, 100).astype(np.float64)\n",
    "data_f32 = data_f64.astype(np.float32)\n",
    "\n",
    "print(f\"Float64 memory: {data_f64.nbytes/1e6:.1f} MB\")\n",
    "print(f\"Float32 memory: {data_f32.nbytes/1e6:.1f} MB\")\n",
    "\n",
    "print(\"\\nFloat64 performance:\")\n",
    "%timeit euclidean_trick(data_f64, data_f64)\n",
    "\n",
    "print(\"\\nFloat32 performance:\")\n",
    "%timeit euclidean_trick(data_f32, data_f32)\n",
    "\n",
    "# Check if precision loss is acceptable\n",
    "result_f64 = euclidean_trick(data_f64[:100], data_f64[:100])\n",
    "result_f32 = euclidean_trick(data_f32[:100], data_f32[:100])\n",
    "print(f\"\\nMax difference: {np.abs(result_f64 - result_f32.astype(np.float64)).max():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary and Next Steps\n",
    "\n",
    "## What We've Learned\n",
    "\n",
    "### Core Concepts\n",
    "1. **NumPy Internals**: Memory layout, strides, views vs copies\n",
    "2. **Broadcasting**: Efficient operations on arrays with different shapes\n",
    "3. **Vectorization**: Replacing Python loops with NumPy operations\n",
    "4. **Performance Optimization**: Multiple approaches to the same problem\n",
    "5. **Profiling**: Understanding where time and memory are used\n",
    "\n",
    "### Key Performance Principles\n",
    "- **Avoid Python loops** at all costs in computational kernels\n",
    "- **Understand memory patterns** - views are fast, copies are expensive\n",
    "- **Consider multiple algorithms** - there's often a memory/speed trade-off\n",
    "- **Profile before optimizing** - measure to find real bottlenecks\n",
    "- **Choose appropriate data types** - precision vs performance\n",
    "\n",
    "## Optional Exercise: Design Challenge\n",
    "\n",
    "Design an algorithm for computing pairwise similarities in a recommender system:\n",
    "- 100,000 users\n",
    "- 10,000 items\n",
    "- Sparse rating matrix (1% filled)\n",
    "- Memory budget: 8GB\n",
    "\n",
    "Consider:\n",
    "1. Data structures and memory layout\n",
    "2. Algorithmic approaches (full vs approximate)\n",
    "3. Chunking strategies\n",
    "4. Opportunities for further optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final performance demonstration\n",
    "print(\"NumPy Performance Showcase\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create a substantial computation\n",
    "n = 5000\n",
    "X = np.random.rand(n, 50)\n",
    "\n",
    "print(f\"Computing {n}×{n} distance matrix...\")\n",
    "print(f\"Total operations: ~{n**2 * 50 / 1e9:.1f} billion\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "distances = euclidean_trick(X, X)\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed = end_time - start_time\n",
    "operations_per_sec = (n**2 * 50) / elapsed / 1e9\n",
    "\n",
    "print(f\"Completed in {elapsed:.2f} seconds\")\n",
    "print(f\"Performance: {operations_per_sec:.1f} billion ops/second\")\n",
    "print(f\"Result matrix: {distances.shape} ({distances.nbytes/1e6:.1f} MB)\")\n",
    "print(f\"All diagonal elements ≈ 0: {np.allclose(np.diag(distances), 0, atol=1e-6)}. Max. deviation: {round(np.max(np.diag(distances)), 10)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
