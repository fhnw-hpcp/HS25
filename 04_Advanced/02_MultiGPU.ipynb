{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb5cf6b4-09c2-44a4-b587-5dcec1bdae6d",
   "metadata": {},
   "source": [
    "![Multi_GPU_Slide](img/HPCP_MultiGPU/Folie1.PNG) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06a3f77-31c0-45f9-8310-96ebd7fa7244",
   "metadata": {},
   "source": [
    "![Multi_GPU_Slide](img/HPCP_MultiGPU/Folie2.PNG) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb57223b-0779-452c-a803-7015dc005ad3",
   "metadata": {},
   "source": [
    "![Multi_GPU_Slide](img/HPCP_MultiGPU/Folie3.PNG) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0601bdff-a5e1-434a-beca-3f37463d753d",
   "metadata": {},
   "source": [
    "![Multi_GPU_Slide](img/HPCP_MultiGPU/Folie4.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edc104d6-d8fb-4e0a-9d9d-e7366515ff70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘scripts’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d867162-1b07-4130-baf3-d6c5c1cff449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/run_script.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/run_script.sh\n",
    "#!/bin/bash\n",
    "\n",
    "module load anaconda\n",
    "cd /psi/home/${USER}/HS25/04_Advanced/scripts\n",
    "export PYTHONPATH=/psi/home/${USER}/HS25/04_Advanced/scripts:$PYTHONPATH\n",
    "conda activate summer-school-hpc-2025\n",
    "mpirun python ${SCRIPT_NAME}.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1be124bc-317d-49ad-9fbb-78fd47e3a920",
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x scripts/run_script.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e9a20d-4e09-4115-81d6-b22dfbfa3842",
   "metadata": {},
   "source": [
    "we will use this submission script for all our tests like this:\n",
    "\n",
    "```bash\n",
    "cd HS25/04_Advanced/scripts\n",
    "SCRIPT_NAME=single_cp sbatch --cluster=gmerlin6 --partition=gpu-short --gpus=2 --output=log.out --reservation=psicourse01 run_script.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546029c0-ef3d-4ac4-be51-6b91067ac999",
   "metadata": {},
   "source": [
    "Let us discuss the following simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12b22995-3200-4437-8f47-41f8ed60e53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/single_cp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/single_cp.py\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Hello from single_job. I see {cp.cuda.runtime.getDeviceCount()} devices.\")\n",
    "\n",
    "A = cp.random.random((1024,1024), dtype=cp.float32)\n",
    "A = cp.random.random((1024,1024), dtype=cp.float32)\n",
    "A = cp.random.random((1024,1024), dtype=cp.float32)\n",
    "A = cp.random.random((1024,1024), dtype=cp.float32)\n",
    "\n",
    "with cp.cuda.Device(0):   # select GPU 0\n",
    "    A = cp.random.random((1024,1024), dtype=cp.float32)\n",
    "    B = cp.random.random((1024,1024), dtype=cp.float32)\n",
    "    res1_gpu = A @ B \n",
    "\n",
    "with cp.cuda.Device(1):   # select GPU 1\n",
    "    C = cp.random.random((1024,1024), dtype=cp.float32)\n",
    "    D = cp.random.random((1024,1024), dtype=cp.float32)\n",
    "    res2_gpu = C @ D \n",
    "\n",
    "res1_host = cp.asnumpy(res1_gpu) \n",
    "res2_host = cp.asnumpy(res2_gpu) \n",
    "print(f\"Done. res1 preview: {res1_host[0:5]} \\nand res2 preview: {res2_host[0:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75e50a2d-71ff-43c8-aa2c-a92ee3488b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from single_job. I see 2 devices.\n",
      "Done. res1 preview: [[243.97412 248.05185 249.9029  ... 261.0253  261.1789  255.34895]\n",
      " [253.66034 248.2228  252.39926 ... 262.94287 266.6966  250.7923 ]\n",
      " [261.63257 256.16394 263.0752  ... 269.082   274.162   262.13193]\n",
      " [252.28265 245.14882 252.74081 ... 257.71622 262.12726 255.1165 ]\n",
      " [246.93062 240.59546 245.90204 ... 253.8186  255.9559  251.0174 ]] and res2 preview: [[254.87265 258.94232 255.34753 ... 266.2976  254.3287  262.9796 ]\n",
      " [244.70996 262.54272 254.03656 ... 260.9129  250.0023  259.76602]\n",
      " [246.9503  253.97754 251.55046 ... 258.73566 252.97365 255.2243 ]\n",
      " [255.05783 262.63297 262.20947 ... 270.6292  256.50345 262.2771 ]\n",
      " [249.85574 264.8111  260.9229  ... 256.94852 257.7815  264.13885]]\n"
     ]
    }
   ],
   "source": [
    "!cat scripts/log.out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1f3ec7-30bc-463c-956c-6baca36616ca",
   "metadata": {},
   "source": [
    "Let us move some data from GPU 0 to GPU 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05176f4c-7152-4239-9276-44bea6fa47ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/p2p_cp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/p2p_cp.py\n",
    "import cupy as cp\n",
    "from cupy.cuda import runtime, Device, Stream\n",
    "\n",
    "src_dev, dst_dev = 0, 1\n",
    "\n",
    "#Enable P2P access\n",
    "with Device(dst_dev):\n",
    "    runtime.deviceEnablePeerAccess(src_dev)\n",
    "with Device(src_dev):\n",
    "    runtime.deviceEnablePeerAccess(dst_dev)\n",
    "\n",
    "with Device(src_dev):\n",
    "    a = cp.arange(10_000_000, dtype=cp.float32)\n",
    "\n",
    "# Allocate destination on GPU 1\n",
    "with Device(dst_dev):\n",
    "    b = cp.empty_like(a)\n",
    "    runtime.memcpyPeerAsync(b.data.ptr, dst_dev, a.data.ptr, src_dev, a.nbytes, Stream.null.ptr)\n",
    "    Stream.null.synchronize() # wait until done (this is in-efficient, but OK for this example)\n",
    "\n",
    "b_host = cp.asnumpy(b) \n",
    "print(f\"Done cpying data from Device 0 to 1. b: {b_host[0:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ae845a-82ca-4f31-a906-290355c98b7e",
   "metadata": {},
   "source": [
    "```bash\n",
    "SCRIPT_NAME=p2p_cp sbatch --cluster=gmerlin6 --partition=gpu-short --gpus=2 --gpus=A5000:2 --output=log.out --reservation=psicourse01 run_script.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcc61d0-04b5-44b6-98b9-cacdbbf5a21a",
   "metadata": {},
   "source": [
    "This is pretty tedious—better to use a library that supports communication patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3bdd35-1e5a-4ce8-a0a7-08a5b4eefe4c",
   "metadata": {},
   "source": [
    "![Multi_GPU_Slide](img/HPCP_MultiGPU/Folie5.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0746e541-b125-4330-8dbc-462a3b71ae5d",
   "metadata": {},
   "source": [
    "![Multi_GPU_Slide](img/HPCP_MultiGPU/Folie6.PNG) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0849f937-0d61-4629-bed2-3ad6a23ef3cf",
   "metadata": {},
   "source": [
    "![Multi_GPU_Slide](img/HPCP_MultiGPU/Folie7.PNG) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ae7f22-7cdd-467f-af56-ae0a1d20d37f",
   "metadata": {},
   "source": [
    "![Multi_GPU_Slide](img/HPCP_MultiGPU/Folie8.PNG) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc67c708-96a3-49ac-85bb-6e5de7d3bd9f",
   "metadata": {},
   "source": [
    "#### Task 1 — Implement `allGather` with NCCL\n",
    "\n",
    "Follow the official documentation:  \n",
    "- CuPy NCCL docs: https://docs.cupy.dev/en/stable/reference/generated/cupy.cuda.nccl.NcclCommunicator.html  \n",
    "- NVIDIA NCCL user guide: https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/index.html  \n",
    "\n",
    "**Steps:**\n",
    "1. Create send and receive buffers on all GPUs.  \n",
    "2. Initialize NCCL across all GPUs with `initAll`.  \n",
    "3. Call the `allGather` operation on each GPU.  \n",
    "   - *Hint:* Wrap these calls inside `groupStart()` and `groupEnd()`.  \n",
    "4. Synchronize, then print part of the result to verify correctness.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10b5f2a-a1c1-47b5-8e55-4ae5a452a33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/nccl_allgather.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/nccl_allgather.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aff2fc6-21dc-4e78-b69e-9256066f35b8",
   "metadata": {},
   "source": [
    "```bash\n",
    "SCRIPT_NAME=nccl_allgather sbatch --cluster=gmerlin6 --partition=gpu-short --gpus=2 --output=log.out --reservation=psicourse01 run_script.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f416d26d-c8f6-4e6b-8801-96a41fadbf9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<cupy_backends.cuda.libs.nccl.NcclCommunicator object at 0x14976ab7b830>, <cupy_backends.cuda.libs.nccl.NcclCommunicator object at 0x14976ab7bd90>]\n",
      "Start allGatrher on: <cupy_backends.cuda.libs.nccl.NcclCommunicator object at 0x14976ab7b830>\n",
      "Start allGatrher on: <cupy_backends.cuda.libs.nccl.NcclCommunicator object at 0x14976ab7bd90>\n",
      "sync\n",
      "sync\n",
      "Done: [0. 1. 2. 3. 4.]\n"
     ]
    }
   ],
   "source": [
    "!cat scripts/log.out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c79c52-a56e-468e-8502-d236a121408e",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "\n",
    "Write an NCCL script where each GPU is controlled by its own process.  \n",
    "You can achieve this by either:\n",
    "- running a single task with **N cores and N GPUs**, or  \n",
    "- running **N tasks**, each bound to one GPU.  \n",
    "\n",
    "**Requirements:**\n",
    "- Broadcast an array from **rank/GPU 0** to all other ranks.  \n",
    "- No `groupStart()` needed, but **each process must initialize** its own `NcclCommunicator`.  \n",
    "- Use `comm_id = nccl.get_unique_id()` to initialize the communicator.  \n",
    "- Use `cuda.Stream.null.ptr` as the CUDA stream (default stream).  \n",
    "- Use `n_devices = int(os.environ[\"SLURM_GPUS_ON_NODE\"])` to get the number of GPUs and **not** `cp.cuda.runtime.getDeviceCount()`! Calling cuda.runtime before starting new processes will lead to an error.\n",
    "- Use `comm.destroy()` to destroy the `NcclCommunicator` within each process at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37183c5-5412-4532-a7d7-52a2f9518e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/nccl_broadcast.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/nccl_broadcast.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a026283-07b3-4f6e-8301-53afbd49d5e6",
   "metadata": {},
   "source": [
    "```bash\n",
    "SCRIPT_NAME=nccl_broadcast sbatch --cluster=gmerlin6 --partition=gpu-short --gpus=4 --output=log.out --reservation=psicourse01 run_script.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eee1541f-7ec7-44c5-ab42-1c22d084ee1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 GPUs\n",
      "Rank 3 started.\n",
      "Rank 2 started.\n",
      "Rank 1 started.\n",
      "Rank 1 finished: [1. 1. 1. 1. 1.]\n",
      "Rank 2 finished: [1. 1. 1. 1. 1.]\n",
      "Rank 3 finished: [1. 1. 1. 1. 1.]\n",
      "Rank 0 successfully finished.\n"
     ]
    }
   ],
   "source": [
    "!cat scripts/log.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dea0fd2-b352-48a8-959d-590ac16b8ae9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b6b7d9e-0c32-4641-a67c-d317a8ab79df",
   "metadata": {},
   "source": [
    "![Multi_GPU_Slide](img/HPCP_MultiGPU/Folie9.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d28b727-083b-4694-a2fd-f68b2da547d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/hello_mpi.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/hello_mpi.py\n",
    "from mpi4py import MPI\n",
    "import cupy as cp\n",
    "from cupy import cuda\n",
    "import socket\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "num_gpus = cuda.runtime.getDeviceCount()\n",
    "print(f\"Hello from rank {rank} on {socket.gethostname()}, I see {num_gpus} GPU(s).\")\n",
    "cp.cuda.Device(rank % num_gpus).use()  # Each task gets an isolated GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4f5136-8df9-46d9-9714-eb81568a36a6",
   "metadata": {},
   "source": [
    "```bash\n",
    "SCRIPT_NAME=hello_mpi sbatch --cluster=gmerlin6 --partition=gpu-short --ntasks=8 --gpus-per-task=1 --output=log.out --reservation=psicourse01 run_script.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b8aeaff-df6e-4cd0-8a7f-262384464b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from rank 1 on merlin-g-014.psi.ch, I see 8 GPUS.\n",
      "Hello from rank 3 on merlin-g-014.psi.ch, I see 8 GPUS.\n",
      "Hello from rank 6 on merlin-g-014.psi.ch, I see 8 GPUS.\n",
      "Hello from rank 7 on merlin-g-014.psi.ch, I see 8 GPUS.\n",
      "Hello from rank 0 on merlin-g-014.psi.ch, I see 8 GPUS.\n",
      "Hello from rank 5 on merlin-g-014.psi.ch, I see 8 GPUS.\n",
      "Hello from rank 4 on merlin-g-014.psi.ch, I see 8 GPUS.\n",
      "Hello from rank 2 on merlin-g-014.psi.ch, I see 8 GPUS.\n"
     ]
    }
   ],
   "source": [
    "!cat scripts/log.out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f08a1c7-1d22-4259-aeca-a28ccc3b1793",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "Understand how Slurm parameters affect resource allocation and rank→GPU mapping.\n",
    "Launch short jobs with different combinations of:\n",
    "* --ntasks\n",
    "* --gpus-per-task\n",
    "* --ntasks-per-node=4\n",
    "* --ngpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a813cf7-b0eb-4607-bd8d-04f433326fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ToDo: Play around"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53922982-a4e0-4776-9ded-78a3b29f5884",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "Use two MPI ranks (0 and 1), one GPU per rank. Create a CuPy buffer on rank 0 and send it to rank 1.\n",
    "\n",
    "* Initialize MPI and map each rank to a GPU.\n",
    "* On rank 0: create a CuPy array (e.g., cp.arange(...)) on its GPU.\n",
    "* Send that device buffer to rank 1 using comm.Send (CUDA-aware path: `comm.Send([data, MPI.FLOAT], ...)`).\n",
    "* On rank 1: Recv into a CuPy buffer on its GPU and verify contents (cp.allclose, print a small slice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa10c39-2e49-4917-9385-520599dcf119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/send_mpi.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/send_mpi.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61ddcf7-a1b0-4bc3-bc1b-6075abb7b7c7",
   "metadata": {},
   "source": [
    "```bash\n",
    "SCRIPT_NAME=send_mpi sbatch --cluster=gmerlin6 --partition=gpu-short --ntasks=2 --gpus-per-task=1 --output=log.out --reservation=psicourse01 run_script.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd17273f-6ea0-4efc-880c-8d0f8277d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from 0. I see 2 devices.\n",
      "Hello from 1. I see 2 devices.\n",
      "0 sent: [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n",
      "1 recv: [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n"
     ]
    }
   ],
   "source": [
    "!cat scripts/log.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcdbaba-9153-4889-96d4-450e4fdf4148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4711a8a-4152-4e0a-ae1e-85d39d8d65f1",
   "metadata": {},
   "source": [
    "![Multi_GPU_Slide](img/HPCP_MultiGPU/Folie10.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e80b716-8147-4e04-9054-b12be006b2b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:summer-school-hpc-2025]",
   "language": "python",
   "name": "conda-env-summer-school-hpc-2025-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
